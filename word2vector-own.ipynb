{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_tweet_text_from_json(file_path):\n",
    "    events = []\n",
    "    labels = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "            labels.append(value[\"label\"])\n",
    "        return events,labels\n",
    "    \n",
    "def get_tweet_text_from_json_unlabel(file_path):\n",
    "    events = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "        return events\n",
    "corinfo_train_events,corinfo_train_labels = get_tweet_text_from_json(\"train_negs.json\")\n",
    "misinfo_train_events,misinfo_train_labels = get_tweet_text_from_json(\"train.json\")\n",
    "train_events, train_labels = corinfo_train_events + misinfo_train_events,corinfo_train_labels + misinfo_train_labels\n",
    "dev_events,dev_labels = get_tweet_text_from_json(\"dev.json\")\n",
    "test_unlabel = get_tweet_text_from_json_unlabel(\"test-unlabelled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "#     pdb.set_trace()\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "    \n",
    "# df['post'] = df['post'].apply(clean_text)\n",
    "# print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,event in enumerate(train_events):\n",
    "    train_events[i] = clean_text(train_events[i])\n",
    "for i,event in enumerate(dev_events):\n",
    "    dev_events[i] = clean_text(dev_events[i])\n",
    "for i,event in enumerate(test_unlabel):\n",
    "    test_unlabel[i] = clean_text(test_unlabel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "train_tokenized = []\n",
    "dev_tokenized = []\n",
    "test_tokenized = []\n",
    "for event in train_events:\n",
    "    train_tokenized.append(tokenize_text(event))\n",
    "for event in dev_events:\n",
    "    dev_tokenized.append(tokenize_text(event))\n",
    "for event in test_unlabel:\n",
    "    test_tokenized.append(tokenize_text(event))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################get negtive labeled data#########################\n",
    "\n",
    "#encoding:'utf-8'\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "def get_articles(url):\n",
    "    header={'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"}\n",
    "    ret=urllib.request.Request(url=url,headers=header)\n",
    "    res=urllib.request.urlopen(ret)\n",
    "    response=BeautifulSoup(res,'html.parser')\n",
    "    datas=response.find_all('p')\n",
    "    dr = re.compile(r'<[^>]+>',re.S)\n",
    "    datas_clean = dr.sub('',str(datas))\n",
    "#     pdb.set_trace()\n",
    "    return datas_clean.replace('\\n','').replace('[','').replace(']','').replace('(','').replace(')','')\n",
    "\n",
    "def getDatas():\n",
    "    header={'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"}    \n",
    "    url = \"https://www.climatechangeinaustralia.gov.au/en/climate-campus/climate-system/\"\n",
    "    ret=urllib.request.Request(url=url,headers=header)\n",
    "    res=urllib.request.urlopen(ret)\n",
    "    response=BeautifulSoup(res,'html.parser')\n",
    "    datas=response.find_all(lambda tag: tag.name == 'ul' and tag.get('class') == ['nav_ul'])\n",
    "    \n",
    "    regex_href = re.compile('<a href=\"(.*?)\">')\n",
    "    nav_urls = re.findall(regex_href, str(datas))\n",
    "    \n",
    "    train_negs = {}\n",
    "    for i,nav_url in enumerate(nav_urls):\n",
    "        dic_train = {}\n",
    "        dic_train['text'] = get_articles('https://www.climatechangeinaustralia.gov.au' + nav_url)\n",
    "        dic_train['label'] = 0\n",
    "        train_negs['train-%s'%i] = dic_train\n",
    "    jstr = json.dumps(train_negs,ensure_ascii = False)\n",
    "    with open('train_negs.json','w') as f:\n",
    "        f.write(jstr)\n",
    "#     make a folder\n",
    "#     folder_name=\"output\"\n",
    "#     if not os.path.exists(folder_name):\n",
    "#         os.mkdir(folder_name)\n",
    "    pass\n",
    "getDatas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
