{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "def get_tweet_text_from_json(file_path):\n",
    "    events = []\n",
    "    labels = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "            labels.append(value[\"label\"])\n",
    "        return events,labels\n",
    "    \n",
    "def get_tweet_text_from_json_unlabel(file_path):\n",
    "    events = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "        return events\n",
    "    \n",
    "misinfo_train_events,misinfo_train_labels = get_tweet_text_from_json(\"train.json\")\n",
    "dev_events,dev_labels = get_tweet_text_from_json(\"dev.json\")\n",
    "test_unlabel = get_tweet_text_from_json_unlabel(\"test-unlabelled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: HTTP Error 500: Internal Server\n",
      "[nltk_data]     Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punc = punctuation + u'.,;《》？！“”‘’@#￥%…&×（）——+【】{};；●，。&～、|\\s:：'\n",
    "# exclude = set(punc)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if (treebank_tag.startswith('J'))|(treebank_tag =='ADJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif (treebank_tag.startswith('R'))|(treebank_tag=='ADV'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_drop_punc_evnets(events):\n",
    "#     preprocess_drop_punc_evnets = events\n",
    "    preprocess_drop_punc_evnets = []\n",
    "#     pdb.set_trace()\n",
    "    for event in events:\n",
    "        event_re = re.sub(r\"[{}]+\".format(punc),\" \",event)\n",
    "        preprocess_drop_punc_evnets.append(event_re)\n",
    "    return preprocess_drop_punc_evnets\n",
    "\n",
    "def preprocess_stopwords_lemma_evnets(events):\n",
    "    preprocess_BOW_events = []\n",
    "    preprocess_events = []\n",
    "    preprocess_events_tokens = []\n",
    "#     pdb.set_trace()\n",
    "    after_drop_punc_evnets = preprocess_drop_punc_evnets(events)\n",
    "    for event in after_drop_punc_evnets:\n",
    "        dic_bow = defaultdict(int)\n",
    "        tokenize_words = tt.tokenize(event)\n",
    "#         rep = [abbr_dict[x] if x in abbr_dict else x for x in tokenize_words]\n",
    "        words_pos = nltk.pos_tag(tokenize_words, tagset=\"universal\")\n",
    "#         pdb.set_trace()\n",
    "        filter_event = [(w,pos) for (w,pos) in words_pos if w.lower() not in stopwords]\n",
    "        lemma_event = []\n",
    "        for (word,pos) in filter_event:\n",
    "#             pdb.set_trace()\n",
    "            word_lemma = wnl.lemmatize(word.lower(),get_wordnet_pos(pos))\n",
    "            lemma_event.append(word_lemma)\n",
    "            dic_bow[word_lemma] += 1\n",
    "        preprocess_events_tokens.append(lemma_event,)\n",
    "        preprocess_events.append(' '.join(lemma_event))\n",
    "        preprocess_BOW_events.append(dic_bow)\n",
    "    return preprocess_events_tokens,preprocess_events,preprocess_BOW_events\n",
    "\n",
    "preprocess_train_events_tokens,preprocessed_train_events,preprocessed_train_BOW_events = preprocess_stopwords_lemma_evnets(misinfo_train_events)\n",
    "preprocess_dev_events_tokens,preprocessed_dev_events,preprocessed_dev_BOW_events = preprocess_stopwords_lemma_evnets(dev_events)\n",
    "preprocess_test_events_tokens,preprocessed_test,preprocessed_test_BOW = preprocess_stopwords_lemma_evnets(test_unlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "all_text_tokens = preprocess_train_events_tokens + preprocess_train_events_tokens + preprocess_test_events_tokens\n",
    "dictionary = corpora.Dictionary(all_text_tokens)\n",
    "dictionary.filter_extremes(no_below = 10, no_above = 0.35)\n",
    "dictionary.compactify()\n",
    "doc_term_matrix = [dictionary.doc2bow(event) for event in all_text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=50, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, '0.025*\"meat\" + 0.022*\"eat\" + 0.019*\"zealand\" + 0.016*\"carter\" + 0.016*\"farmer\" + 0.015*\"r\" + 0.012*\"house\" + 0.011*\"beef\" + 0.011*\"food\" + 0.010*\"perry\" + 0.010*\"amendment\" + 0.010*\"bob\" + 0.009*\"committee\" + 0.009*\"pentagon\" + 0.008*\"diet\" + 0.008*\"burger\" + 0.008*\"fuel\" + 0.008*\"agenda\" + 0.007*\"livestock\" + 0.006*\"alternative\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics = 1, num_words = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = ldamodel.get_document_topics(doc_term_matrix[1171],minimum_probability = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(preprocess_train_events_tokens)):\n",
    "    top_topics = ldamodel.get_document_topics(doc_term_matrix[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(3)]\n",
    "#     topic_vec.extend([preprocess_train_events_tokens[i].real_counts]) # counts of reviews for restaurant\n",
    "#     topic_vec.extend([len(preprocess_train_events_tokens[i])]) # length review\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vecs = []\n",
    "for i in range(len(preprocess_dev_events_tokens)):\n",
    "    top_topics = ldamodel.get_document_topics(doc_term_matrix[i+len(preprocess_train_events_tokens)], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(3)]\n",
    "#     topic_vec.extend([preprocess_train_events_tokens[i].real_counts]) # counts of reviews for restaurant\n",
    "#     topic_vec.extend([len(preprocess_train_events_tokens[i])]) # length review\n",
    "    dev_vecs.append(topic_vec)\n",
    "    \n",
    "test_vecs = []\n",
    "for i in range(len(preprocess_test_events_tokens)):\n",
    "    top_topics = ldamodel.get_document_topics(doc_term_matrix[i+len(preprocess_train_events_tokens)+len(preprocess_dev_events_tokens)], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(3)]\n",
    "#     topic_vec.extend([preprocess_train_events_tokens[i].real_counts]) # counts of reviews for restaurant\n",
    "#     topic_vec.extend([len(preprocess_train_events_tokens[i])]) # length review\n",
    "    test_vecs.append(topic_vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n"
     ]
    }
   ],
   "source": [
    "print(len(test_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "def evaluate_result(groundtruth, prediction):\n",
    "    prediction = prediction.tolist()\n",
    "    for i,item in enumerate(prediction):\n",
    "        if item == -1:\n",
    "            prediction[i] = 0\n",
    "    p, r, f, _ = precision_recall_fscore_support(groundtruth, prediction, pos_label=1, average=\"binary\")\n",
    "#     pdb.set_trace()\n",
    "    print(\"Performance on the positive class (documents with misinformation):\")\n",
    "    print(\"Precision =\", p)\n",
    "    print(\"Recall    =\", r)\n",
    "    print(\"F1        =\", f)\n",
    "    print(accuracy_score(dev_labels,prediction))\n",
    "    \n",
    "def transfer_output(preds):\n",
    "    test_dic = {}\n",
    "    for i,test_pre in enumerate(preds):\n",
    "        if test_pre == -1:\n",
    "            dic = {}\n",
    "            dic['label'] = 0\n",
    "    #         pdb.set_trace()\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "        else:\n",
    "            dic = {}\n",
    "    #         pdb.set_trace()\n",
    "            dic['label'] = 1\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "    jstr = json.dumps(test_dic,ensure_ascii = False)\n",
    "    # print(jstr)\n",
    "    with open('test-output.json','w') as f:\n",
    "        f.write(jstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 1.0\n",
      "F1        = 0.6666666666666666\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 1.0\n",
      "F1        = 0.6666666666666666\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 1.0\n",
      "F1        = 0.6666666666666666\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.56\n",
      "Recall    = 0.56\n",
      "F1        = 0.56\n",
      "0.56\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 1.0\n",
      "F1        = 0.6666666666666666\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 1.0\n",
      "F1        = 0.6666666666666666\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 1.0\n",
      "F1        = 0.6666666666666666\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.64\n",
      "F1        = 0.5614035087719298\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5208333333333334\n",
      "Recall    = 0.5\n",
      "F1        = 0.5102040816326531\n",
      "0.52\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.509090909090909\n",
      "Recall    = 0.56\n",
      "F1        = 0.5333333333333333\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.54\n",
      "F1        = 0.5192307692307693\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4888888888888889\n",
      "Recall    = 0.88\n",
      "F1        = 0.6285714285714286\n",
      "0.48\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5882352941176471\n",
      "Recall    = 0.2\n",
      "F1        = 0.29850746268656714\n",
      "0.53\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5882352941176471\n",
      "Recall    = 0.2\n",
      "F1        = 0.29850746268656714\n",
      "0.53\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5882352941176471\n",
      "Recall    = 0.2\n",
      "F1        = 0.29850746268656714\n",
      "0.53\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5882352941176471\n",
      "Recall    = 0.2\n",
      "F1        = 0.29850746268656714\n",
      "0.53\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5050505050505051\n",
      "Recall    = 1.0\n",
      "F1        = 0.6711409395973155\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5050505050505051\n",
      "Recall    = 1.0\n",
      "F1        = 0.6711409395973155\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5050505050505051\n",
      "Recall    = 1.0\n",
      "F1        = 0.6711409395973155\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5050505050505051\n",
      "Recall    = 1.0\n",
      "F1        = 0.6711409395973155\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4943820224719101\n",
      "Recall    = 0.88\n",
      "F1        = 0.6330935251798561\n",
      "0.49\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4943820224719101\n",
      "Recall    = 0.88\n",
      "F1        = 0.6330935251798561\n",
      "0.49\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4943820224719101\n",
      "Recall    = 0.88\n",
      "F1        = 0.6330935251798561\n",
      "0.49\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4943820224719101\n",
      "Recall    = 0.88\n",
      "F1        = 0.6330935251798561\n",
      "0.49\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "nus = [0.001, 0.01, 0.1, 1]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "kernels = ['rbf','linear']\n",
    "for kernel in kernels:\n",
    "    for nu in nus:\n",
    "        for gamma in gammas:\n",
    "            ocs = svm.OneClassSVM(gamma=gamma,kernel = kernel, nu = nu).fit(train_vecs)\n",
    "            y_pre = ocs.predict(dev_vecs)\n",
    "            print(ocs)\n",
    "            evaluate_result(dev_labels,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "ocs = svm.OneClassSVM(gamma=0.001,kernel = 'linear', nu = 0.01).fit(train_vecs)\n",
    "test_pres = ocs.predict(test_vecs)\n",
    "\n",
    "print(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4945054945054945\n",
      "Recall    = 0.9\n",
      "F1        = 0.6382978723404256\n",
      "0.49\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4675324675324675\n",
      "Recall    = 0.72\n",
      "F1        = 0.5669291338582677\n",
      "0.45\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4444444444444444\n",
      "Recall    = 0.56\n",
      "F1        = 0.4955752212389381\n",
      "0.43\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4725274725274725\n",
      "Recall    = 0.86\n",
      "F1        = 0.6099290780141844\n",
      "0.45\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4594594594594595\n",
      "Recall    = 0.68\n",
      "F1        = 0.5483870967741935\n",
      "0.44\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.49056603773584906\n",
      "Recall    = 0.52\n",
      "F1        = 0.5048543689320388\n",
      "0.49\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4838709677419355\n",
      "Recall    = 0.9\n",
      "F1        = 0.6293706293706294\n",
      "0.47\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.4861111111111111\n",
      "Recall    = 0.7\n",
      "F1        = 0.5737704918032787\n",
      "0.48\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5217391304347826\n",
      "Recall    = 0.48\n",
      "F1        = 0.4999999999999999\n",
      "0.52\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5053763440860215\n",
      "Recall    = 0.94\n",
      "F1        = 0.6573426573426574\n",
      "0.51\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.49295774647887325\n",
      "Recall    = 0.7\n",
      "F1        = 0.5785123966942148\n",
      "0.49\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.52\n",
      "Recall    = 0.52\n",
      "F1        = 0.52\n",
      "0.52\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5053763440860215\n",
      "Recall    = 0.94\n",
      "F1        = 0.6573426573426574\n",
      "0.51\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.48\n",
      "Recall    = 0.72\n",
      "F1        = 0.576\n",
      "0.47\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5555555555555556\n",
      "Recall    = 0.6\n",
      "F1        = 0.576923076923077\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "n_neighborss = [10,20,30,40,100]\n",
    "contaminations = [0.1,0.3,0.5]\n",
    "for n_neighbor in n_neighborss:\n",
    "    for contamination in contaminations:\n",
    "        clf = LocalOutlierFactor(n_neighbors = n_neighbor, novelty=True, contamination=contamination)\n",
    "        clf.fit(train_vecs)\n",
    "        y_pre = clf.predict(dev_vecs)\n",
    "        print(clf)\n",
    "        evaluate_result(dev_labels,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 ...  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors = 40, novelty=True, contamination=0.1)\n",
    "clf.fit(train_vecs)\n",
    "test_pres = clf.predict(test_vecs)\n",
    "print(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_output(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19561\n",
      "[-1 -1 -1 ... -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# 0.556\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "tfidfvector = TfidfVectorizer()\n",
    "train_tfidfvector = tfidfvector.fit_transform(preprocessed_train_events)\n",
    "dev_tfidfvector = tfidfvector.transform(preprocessed_dev_events)\n",
    "test_tfidfvector = tfidfvector.transform(preprocessed_test)\n",
    "print(len(tfidfvector.vocabulary_))\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors = 10, novelty=True, contamination=0.3)\n",
    "clf.fit(train_tfidfvector)\n",
    "test_pres = clf.predict(test_tfidfvector)\n",
    "print(test_pres)\n",
    "# transfer_output(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-948\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
