{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpfo-dcOYGWX"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def get_tweet_text_from_json(file_path):\n",
    "    events = []\n",
    "    labels = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "            labels.append(value[\"label\"])\n",
    "        return events,labels\n",
    "    \n",
    "def get_tweet_text_from_json_unlabel(file_path):\n",
    "    events = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "        return events\n",
    "# a,a_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_bbc_sport.json\")\n",
    "# b,b_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_thegurdian_ausnews.json\")\n",
    "# c,c_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_thegurdian_australian-immigration-and-asylum.json\")\n",
    "# d,d_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_thegurdian_australian-politics.json\")\n",
    "# e,e_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_thegurdian_uk-news.json\")\n",
    "# f,f_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_thegurdian_world.json\")\n",
    "# g,g_labels = get_tweet_text_from_json(\"drive/FreeGPUnlp/train_negs_thegurdian.json\")\n",
    "# cor_info_events,cor_info_labels = a + b + c + d + e + f + g, a_labels + b_labels + c_labels + d_labels + e_labels + f_labels + g_labels\n",
    "cor_info_events,cor_info_labels = get_tweet_text_from_json(\"train_negs.json\")\n",
    "misinfo_train_events,misinfo_train_labels = get_tweet_text_from_json(\"train.json\")\n",
    "dev_events,dev_labels = get_tweet_text_from_json(\"dev.json\")\n",
    "test_unlabel = get_tweet_text_from_json_unlabel(\"test-unlabelled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56054,
     "status": "error",
     "timestamp": 1589042564440,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "j6YBtbCGb5iM",
    "outputId": "40aa4919-2a60-4c72-9798-1207df23e297"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/liyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/liyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/liyi/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/liyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-73-54ac14cbea15>(189)preprocess_stopwords_lemma_evnets()\n",
      "-> after_drop_punc_evnets = preprocess_drop_punc_evnets(events_ner)\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(190)preprocess_stopwords_lemma_evnets()\n",
      "-> for event in after_drop_punc_evnets:\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(191)preprocess_stopwords_lemma_evnets()\n",
      "-> dic_bow = defaultdict(int)\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(194)preprocess_stopwords_lemma_evnets()\n",
      "-> event_not = re.sub(\"[.*]*n[‘|']t\",\" not\",event)\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(195)preprocess_stopwords_lemma_evnets()\n",
      "-> tokenize_words = tt.tokenize(event_not)\n",
      "(Pdb) event_not\n",
      "'why flooding is not a sign of climate change Distinguished climate scientist writes In the context of climate change is what we are seeing in a new level of disaster which is becoming more common The flood disaster unfolding in is certainly very unusual But so are other natural weather disasters which have always occurred and always will occur Major floods are difficult to compare throughout history because of the ways we alter the landscape For example as cities like expand over the years soil is covered up by roads parking lots and buidings with water rapidly draining off rather than soaking into the soil The population of is now ten times what is was in the s The metroplex has expanded greatly and the water drainage is basically in the direction of downtown '\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(196)preprocess_stopwords_lemma_evnets()\n",
      "-> words_pos = nltk.pos_tag(tokenize_words, tagset=\"universal\")\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(198)preprocess_stopwords_lemma_evnets()\n",
      "-> filter_event = [(w,pos) for (w,pos) in words_pos if w.lower() not in stopwords]\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(199)preprocess_stopwords_lemma_evnets()\n",
      "-> lemma_event = []\n",
      "(Pdb) filter_event\n",
      "[('flooding', 'NOUN'), ('not', 'ADV'), ('sign', 'NOUN'), ('climate', 'NOUN'), ('change', 'NOUN'), ('Distinguished', 'NOUN'), ('climate', 'NOUN'), ('scientist', 'NOUN'), ('writes', 'VERB'), ('context', 'NOUN'), ('climate', 'NOUN'), ('change', 'NOUN'), ('seeing', 'VERB'), ('new', 'ADJ'), ('level', 'NOUN'), ('disaster', 'NOUN'), ('becoming', 'VERB'), ('common', 'ADJ'), ('flood', 'ADJ'), ('disaster', 'NOUN'), ('unfolding', 'NOUN'), ('certainly', 'ADV'), ('unusual', 'ADJ'), ('But', 'CONJ'), ('so', 'ADV'), ('natural', 'ADJ'), ('weather', 'NOUN'), ('disasters', 'NOUN'), ('always', 'ADV'), ('occurred', 'VERB'), ('always', 'ADV'), ('occur', 'VERB'), ('Major', 'NOUN'), ('floods', 'NOUN'), ('difficult', 'ADJ'), ('compare', 'VERB'), ('throughout', 'ADP'), ('history', 'NOUN'), ('ways', 'NOUN'), ('alter', 'VERB'), ('landscape', 'NOUN'), ('example', 'NOUN'), ('cities', 'NOUN'), ('like', 'ADP'), ('expand', 'NOUN'), ('years', 'NOUN'), ('soil', 'NOUN'), ('covered', 'VERB'), ('roads', 'NOUN'), ('parking', 'VERB'), ('lots', 'NOUN'), ('buidings', 'NOUN'), ('water', 'NOUN'), ('rapidly', 'ADV'), ('draining', 'VERB'), ('rather', 'ADV'), ('soaking', 'VERB'), ('soil', 'NOUN'), ('population', 'NOUN'), ('ten', 'ADJ'), ('times', 'NOUN'), ('metroplex', 'NOUN'), ('expanded', 'VERB'), ('greatly', 'ADV'), ('water', 'NOUN'), ('drainage', 'NOUN'), ('basically', 'ADV'), ('direction', 'NOUN'), ('downtown', 'NOUN')]\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(200)preprocess_stopwords_lemma_evnets()\n",
      "-> for (word,pos) in filter_event:\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(202)preprocess_stopwords_lemma_evnets()\n",
      "-> word_lemma = wnl.lemmatize(word.lower(),get_wordnet_pos(pos))\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(203)preprocess_stopwords_lemma_evnets()\n",
      "-> lemma_event.append(word_lemma)\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(204)preprocess_stopwords_lemma_evnets()\n",
      "-> dic_bow[word_lemma] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(200)preprocess_stopwords_lemma_evnets()\n",
      "-> for (word,pos) in filter_event:\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(202)preprocess_stopwords_lemma_evnets()\n",
      "-> word_lemma = wnl.lemmatize(word.lower(),get_wordnet_pos(pos))\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(203)preprocess_stopwords_lemma_evnets()\n",
      "-> lemma_event.append(word_lemma)\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(204)preprocess_stopwords_lemma_evnets()\n",
      "-> dic_bow[word_lemma] += 1\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(200)preprocess_stopwords_lemma_evnets()\n",
      "-> for (word,pos) in filter_event:\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(202)preprocess_stopwords_lemma_evnets()\n",
      "-> word_lemma = wnl.lemmatize(word.lower(),get_wordnet_pos(pos))\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(203)preprocess_stopwords_lemma_evnets()\n",
      "-> lemma_event.append(word_lemma)\n",
      "(Pdb) n\n",
      "> <ipython-input-73-54ac14cbea15>(204)preprocess_stopwords_lemma_evnets()\n",
      "-> dic_bow[word_lemma] += 1\n",
      "(Pdb) continue\n",
      "> <ipython-input-73-54ac14cbea15>(189)preprocess_stopwords_lemma_evnets()\n",
      "-> after_drop_punc_evnets = preprocess_drop_punc_evnets(events_ner)\n",
      "(Pdb) continue\n",
      "> <ipython-input-73-54ac14cbea15>(189)preprocess_stopwords_lemma_evnets()\n",
      "-> after_drop_punc_evnets = preprocess_drop_punc_evnets(events_ner)\n",
      "(Pdb) continue\n",
      "> <ipython-input-73-54ac14cbea15>(189)preprocess_stopwords_lemma_evnets()\n",
      "-> after_drop_punc_evnets = preprocess_drop_punc_evnets(events_ner)\n",
      "(Pdb) continue\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "punc = punctuation + u'.,;《》？！@#￥%…&×（）——+【】{};；●，。&～、|\\s:：'\n",
    "\n",
    "stopwords = ['a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'after',\n",
    " 'again',\n",
    " 'ain',\n",
    " 'all',\n",
    " 'am',\n",
    " 'an',\n",
    " 'and',\n",
    " 'any',\n",
    " 'are',\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'because',\n",
    " 'been',\n",
    " 'before',\n",
    " 'being',\n",
    " 'below',\n",
    " 'between',\n",
    " 'both',\n",
    " 'by',\n",
    " 'can',\n",
    " 'could',\n",
    " 'd',\n",
    " 'did',\n",
    " 'do',\n",
    " 'does',\n",
    " 'doing',\n",
    " 'down',\n",
    " 'during',\n",
    " 'each',\n",
    " 'for',\n",
    " 'from',\n",
    " 'further',\n",
    " 'had',\n",
    " 'has',\n",
    " 'have',\n",
    " 'having',\n",
    " 'he',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'i',\n",
    " 'in',\n",
    " 'into',\n",
    " 'is',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'll',\n",
    " 'm',\n",
    " 'ma',\n",
    " 'me',\n",
    " 'might',\n",
    " 'more',\n",
    " 'most',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'now',\n",
    " 'o',\n",
    " 'of',\n",
    " 'off',\n",
    " 'on',\n",
    " 'other',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 're',\n",
    " 's',\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'some',\n",
    " 't',\n",
    " 'than',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'the',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'there',\n",
    " 'these',\n",
    " 'they',\n",
    " 'this',\n",
    " 'those',\n",
    " 'through',\n",
    " 'to',\n",
    " 'too',\n",
    " 'under',\n",
    " 'until',\n",
    " 'up',\n",
    " 've',\n",
    " 'very',\n",
    " 'was',\n",
    " 'we',\n",
    " 'were',\n",
    " 'what',\n",
    " 'when',\n",
    " 'where',\n",
    " 'which',\n",
    " 'while','who','whom','why','will','with','won',\"won't\",'wouldn',\"wouldn't\",'y','you',\"you'd\",\"you'll\",\"you're\",\"you've\",'your','yours','yourself','yourselves']\n",
    "\n",
    "from string import digits\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if (treebank_tag.startswith('J'))|(treebank_tag =='ADJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif (treebank_tag.startswith('R'))|(treebank_tag=='ADV'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_drop_punc_evnets(events):\n",
    "#     preprocess_drop_punc_evnets = events\n",
    "    preprocess_drop_punc_evnets = []\n",
    "#     pdb.set_trace()\n",
    "    for event in events:\n",
    "        event_re = re.sub(r\"[{}]+\".format(punc),\" \",event)\n",
    "        remove_digits = str.maketrans('', '', digits)\n",
    "        preprocess_drop_punc_evnets.append(event_re.translate(remove_digits))\n",
    "    return preprocess_drop_punc_evnets\n",
    "  \n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.label_ == \"PERSON\":\n",
    "        return \"YYYYYYYY\"\n",
    "    elif token.ent_iob != 0 and token.label_ == \"GPE\":\n",
    "        return \"XXXXXXXX\"\n",
    "    else:\n",
    "        return token.string\n",
    "    \n",
    "def scrub(text):\n",
    "    event = []\n",
    "    doc = nlp(text)\n",
    "    ents = [e.merge() for e in doc.ents if e.label_.lower() in [\"person\",\"gpe\"]]\n",
    "    for item in doc:\n",
    "        if item in ents:\n",
    "            pass\n",
    "        else:\n",
    "            event.append(item.text)\n",
    "    return \" \".join(event)\n",
    "#     for ent in doc.ents:\n",
    "#         ent.merge()\n",
    "#     pdb.set_trace()\n",
    "#     tokens = map(replace_name_with_placeholder, doc)\n",
    "#     return \"\".join(tokens)\n",
    "\n",
    "def ner(events):\n",
    "    for i,event in enumerate(events):\n",
    "        events[i] = scrub(event)\n",
    "#         pdb.set_trace()\n",
    "    return events\n",
    "\n",
    "def preprocess_stopwords_lemma_evnets(events):\n",
    "    preprocess_BOW_events = []\n",
    "    preprocess_events = []\n",
    "    preprocess_events_tokens = []\n",
    "    events_ner = ner(events)\n",
    "    pdb.set_trace()\n",
    "    after_drop_punc_evnets = preprocess_drop_punc_evnets(events_ner)\n",
    "    for event in after_drop_punc_evnets:\n",
    "        dic_bow = defaultdict(int)\n",
    "#         pdb.set_trace()\n",
    "#         event_ner = scrub(event)\n",
    "        event_not = re.sub(\"[.*]*n[‘|']t\",\" not\",event)\n",
    "        tokenize_words = tt.tokenize(event_not)\n",
    "        words_pos = nltk.pos_tag(tokenize_words, tagset=\"universal\")\n",
    "#         pdb.set_trace()\n",
    "        filter_event = [(w,pos) for (w,pos) in words_pos if w.lower() not in stopwords]\n",
    "        lemma_event = []\n",
    "        for (word,pos) in filter_event:\n",
    "#             pdb.set_trace()\n",
    "            word_lemma = wnl.lemmatize(word.lower(),get_wordnet_pos(pos))\n",
    "            lemma_event.append(word_lemma)\n",
    "            dic_bow[word_lemma] += 1\n",
    "        preprocess_events_tokens.append(lemma_event,)\n",
    "        preprocess_events.append(' '.join(lemma_event))\n",
    "        preprocess_BOW_events.append(dic_bow)\n",
    "    return preprocess_events_tokens,preprocess_events,preprocess_BOW_events\n",
    "\n",
    "preprocess_train_events_tokens,preprocessed_train_events,preprocessed_train_BOW_events = preprocess_stopwords_lemma_evnets(misinfo_train_events)\n",
    "preprocess_cor_events_tokens,preprocessed_cor_events,preprocessed_cor_BOW_events = preprocess_stopwords_lemma_evnets(cor_info_events)\n",
    "preprocess_dev_events_tokens,preprocessed_dev_events,preprocessed_dev_BOW_events = preprocess_stopwords_lemma_evnets(dev_events)\n",
    "preprocess_test_events_tokens,preprocessed_test,preprocessed_test_BOW = preprocess_stopwords_lemma_evnets(test_unlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJhqtR1GhFi2"
   },
   "outputs": [],
   "source": [
    "train_labels = misinfo_train_labels + cor_info_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHIKJQAihrLL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_result(groundtruth, prediction):\n",
    "    prediction = prediction.tolist()\n",
    "    for i,item in enumerate(prediction):\n",
    "        if item == -1:\n",
    "            prediction[i] = 0\n",
    "    p, r, f, _ = precision_recall_fscore_support(groundtruth, prediction, pos_label=1, average=\"binary\")\n",
    "#     pdb.set_trace()\n",
    "    # print(\"Performance on the positive class (documents with misinformation):\")\n",
    "    # print(\"Precision =\", p)\n",
    "    # print(\"Recall    =\", r)\n",
    "    # print(\"F1        =\", f)\n",
    "    # print(accuracy_score(groundtruth,prediction))\n",
    "    return p,r,f,accuracy_score(groundtruth,prediction)\n",
    "    \n",
    "def transfer_output(preds):\n",
    "    test_dic = {}\n",
    "    for i,test_pre in enumerate(preds):\n",
    "\n",
    "        if test_pre == -1:\n",
    "            dic = {}\n",
    "            dic['label'] = 0\n",
    "    #         pdb.set_trace()\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "        elif test_pre == 0:\n",
    "            dic = {}\n",
    "    #         pdb.set_trace()\n",
    "            dic['label'] = 0\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "        else:\n",
    "            dic = {}\n",
    "    #         pdb.set_trace()\n",
    "            dic['label'] = 1\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "    jstr = json.dumps(test_dic,ensure_ascii = False)\n",
    "    # print(jstr)\n",
    "    with open('test-output.json','w') as f:\n",
    "        f.write(jstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTxbV7oMqnOQ"
   },
   "outputs": [],
   "source": [
    "################################################ DictVectorizer ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3PYtNrDdkfr"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "vector_train = vectorizer.fit_transform(preprocessed_train_BOW_events + preprocessed_cor_BOW_events)\n",
    "vector_development = vectorizer.transform(preprocessed_dev_BOW_events)\n",
    "vector_test = vectorizer.transform(preprocessed_test_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSbRbcybqsnW"
   },
   "outputs": [],
   "source": [
    "################################################ w2v_own ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzSZcKPBqxC7"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "all_texts = preprocess_cor_events_tokens + preprocess_train_events_tokens + preprocess_dev_events_tokens + preprocess_test_events_tokens\n",
    "word_model_skip_gram = Word2Vec(all_texts, min_count=1, size=60, window=8, iter=50, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sllLCFWFuvM5"
   },
   "outputs": [],
   "source": [
    "################################################ w2v_google ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131637,
     "status": "ok",
     "timestamp": 1588956804611,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "HStJR6YUuyWR",
    "outputId": "e53eea66-fbf6-474e-83ac-ab6ec389c2c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,preprocess_train_events_tokens + preprocess_cor_events_tokens)\n",
    "X_dev_word_average = word_averaging_list(wv,preprocess_dev_events_tokens)\n",
    "X_test_word_average = word_averaging_list(wv,preprocess_test_events_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPgSngMZrM4w"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "        self.word_model = word_model\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self):  # comply with scikit-learn transformer requirement\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "\n",
    "\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "            mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "            array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "\n",
    "        self.word_model = word_model\n",
    "        self.word_idf_weight = None\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        \"\"\"\n",
    "        Fit in a list of docs, which had been preprocessed and tokenized,\n",
    "        such as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
    "        Then build up a tfidf model to compute each word's idf as its weight.\n",
    "        Noted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
    "        :param\n",
    "            pre_processed_docs: list of docs, which are tokenized\n",
    "        :return:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        text_docs = []\n",
    "        for doc in docs:\n",
    "            text_docs.append(\" \".join(doc))\n",
    "\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(text_docs)  # must be list of text string\n",
    "\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
    "        self.word_idf_weight = defaultdict(lambda: max_idf,[(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "            mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "            array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcOE946brOXh"
   },
   "outputs": [],
   "source": [
    "mean_vec_tr = MeanEmbeddingVectorizer(word_model_skip_gram)\n",
    "train_vec = mean_vec_tr.transform(preprocess_train_events_tokens + preprocess_cor_events_tokens)\n",
    "dev_vec = mean_vec_tr.transform(preprocess_dev_events_tokens)\n",
    "test_vec = mean_vec_tr.transform(preprocess_test_events_tokens)\n",
    "\n",
    "tfidf_vec_tr_skip_gram = TfidfEmbeddingVectorizer(word_model_skip_gram)\n",
    "tfidf_vec_tr_skip_gram.fit(preprocess_train_events_tokens + preprocess_cor_events_tokens)  # fit tfidf model first\n",
    "tfidf_train_vec_skip_gram = tfidf_vec_tr_skip_gram.transform(preprocess_train_events_tokens + preprocess_cor_events_tokens)\n",
    "tfidf_dev_vec_skip_gram = tfidf_vec_tr_skip_gram.transform(preprocess_dev_events_tokens)\n",
    "tfidf_test_vec_skip_gram = tfidf_vec_tr_skip_gram.transform(preprocess_test_events_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3IYCwHStbwv"
   },
   "outputs": [],
   "source": [
    "################################################ tfidf ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wrOZCuft4EY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvector = TfidfVectorizer(min_df=3,  \n",
    "                              max_df=0.5,\n",
    "                              max_features=None,                 \n",
    "                              ngram_range=(1, 2), \n",
    "                              use_idf=True,\n",
    "                              smooth_idf=True)\n",
    "train_tfidfvector = tfidfvector.fit_transform(preprocessed_train_events + preprocessed_cor_events)\n",
    "dev_tfidfvector = tfidfvector.transform(preprocessed_dev_events)\n",
    "test_tfidfvector = tfidfvector.transform(preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7sRV9vkqRuE"
   },
   "outputs": [],
   "source": [
    "################################################ classic binary algrithom ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9fnIsKNfcub"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def tune_log(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "    solvers = [\"liblinear\",\"lbfgs\",\"newton-cg\",\"sag\"]\n",
    "    penalties = [\"l1\",\"l2\"]\n",
    "    cs = [0.001,0.01,0.1,0.5,0.53,1,100, 1000]\n",
    "    f_score = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    parameter_save = []\n",
    "    for solver in solvers:\n",
    "        for penalty in penalties:\n",
    "            for c in cs:\n",
    "                if ((solver == \"lbfgs\")|(solver == \"newton-cg\")|(solver == \"sag\")) & (penalty == \"l1\"):\n",
    "                    continue\n",
    "                clf = LogisticRegression(C=c, penalty = penalty, solver = solver)\n",
    "                clf.fit(train_feature,train_label)\n",
    "                preds = clf.predict(dev_feature)\n",
    "                p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "                f_score.append(f)\n",
    "                precision.append(p)\n",
    "                recall.append(r)\n",
    "                parameter_save.append((c,penalty,solver))\n",
    "    i = f_score.index(max(f_score))\n",
    "    print(precision[i])\n",
    "    print(recall[i])\n",
    "    print(f_score[i])\n",
    "    (c,penalty,solver) = parameter_save[i]\n",
    "    clf = LogisticRegression(C=c, penalty = penalty, solver = solver)\n",
    "    clf.fit(train_feature,train_label)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def tune_nb(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "    alphas = [0.001,0.01,0.1,0.5,0.53,0.55,0.6,1,1.2,1.3,2,10,50,100]\n",
    "    f_score = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    parameter_save = []\n",
    "    for alpha in alphas:\n",
    "        clf = MultinomialNB(alpha = alpha)\n",
    "        clf.fit(train_feature,train_label)\n",
    "        preds = clf.predict(dev_feature)\n",
    "        p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "        f_score.append(f)\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        parameter_save.append(alpha)\n",
    "    i = f_score.index(max(f_score))\n",
    "    print(precision[i])\n",
    "    print(recall[i])\n",
    "    print(f_score[i])\n",
    "    alpha = parameter_save[i]\n",
    "    clf = MultinomialNB(alpha = alpha)\n",
    "    clf.fit(train_feature,train_label)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "def tune_svm(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "  kernelList = ['linear','rbf','sigmoid']\n",
    "  gammaList=np.logspace(-10,1,10)\n",
    "  f_score = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  parameter_save = []\n",
    "  for kernel in kernelList:\n",
    "    for gamma in gammaList:\n",
    "      svc = SVC(kernel=kernel,gamma=gamma).fit(train_feature,train_label)\n",
    "      preds = svc.predict(dev_feature)\n",
    "      p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "      print(p)\n",
    "      print(r)\n",
    "      print(f)\n",
    "      f_score.append(f)\n",
    "      precision.append(p)\n",
    "      recall.append(r)\n",
    "      parameter_save.append((kernel,gamma))\n",
    "  i = f_score.index(max(f_score))\n",
    "  print(precision[i])\n",
    "  print(recall[i])\n",
    "  print(f_score[i])\n",
    "  (kernel,gamma) = parameter_save[i]\n",
    "  clf = SVC(kernel=kernel,gamma=gamma)\n",
    "  clf.fit(train_feature,train_label)\n",
    "  test_pres = clf.predict(test_feature)\n",
    "  return test_pres\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def tune_knn(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "  list=['euclidean','manhattan','minkowski']\n",
    "  KList=range(2,13)\n",
    "  f_score = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  parameter_save = []\n",
    "  for i in list:\n",
    "    for k in KList:\n",
    "      knn = KNeighborsClassifier(n_neighbors=k,metric=i).fit(train_feature,train_label)\n",
    "      preds = knn.predict(dev_feature)\n",
    "      p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "      print(p)\n",
    "      print(r)\n",
    "      print(f)\n",
    "      f_score.append(f)\n",
    "      precision.append(p)\n",
    "      recall.append(r)\n",
    "      parameter_save.append((k,i))\n",
    "  i = f_score.index(max(f_score))\n",
    "  print(precision[i])\n",
    "  print(recall[i])\n",
    "  print(f_score[i])\n",
    "  (k,distance) = parameter_save[i]\n",
    "  clf = KNeighborsClassifier(n_neighbors=k,metric=distance)\n",
    "  clf.fit(train_feature,train_label)\n",
    "  test_pres = clf.predict(test_feature)\n",
    "  return test_pres\n",
    "\n",
    "from sklearn import tree\n",
    "def tune_decisiontree(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "  depthList=np.arange(3,10)\n",
    "  f_score = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  parameter_save = []\n",
    "  for i in depthList:\n",
    "    dtc = tree.DecisionTreeClassifier(max_depth=i).fit(train_feature,train_label)\n",
    "    preds = dtc.predict(dev_feature)\n",
    "    p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "    print(p)\n",
    "    print(r)\n",
    "    print(f)\n",
    "    f_score.append(f)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "    parameter_save.append(i)\n",
    "  i = f_score.index(max(f_score))\n",
    "  print(precision[i])\n",
    "  print(recall[i])\n",
    "  print(f_score[i])\n",
    "  depth = parameter_save[i]\n",
    "  clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "  clf.fit(train_feature,train_label)\n",
    "  test_pres = clf.predict(test_feature)\n",
    "  return test_pres\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def tune_randomforest(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "  f_score = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  parameter_save = []\n",
    "  for i in range(10,101):\n",
    "    rfc = RandomForestClassifier(n_estimators=i,random_state=13)\n",
    "    rfc.fit(train_feature,train_label)\n",
    "    preds = rfc.predict(dev_feature)\n",
    "    p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "    print(p)\n",
    "    print(r)\n",
    "    print(f)\n",
    "    f_score.append(f)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "    parameter_save.append(i)\n",
    "  i = f_score.index(max(f_score))\n",
    "  print(precision[i])\n",
    "  print(recall[i])\n",
    "  print(f_score[i])\n",
    "  n = parameter_save[i]\n",
    "  clf = RandomForestClassifier(n_estimators=n,random_state=13)\n",
    "  clf.fit(train_feature,train_label)\n",
    "  test_pres = clf.predict(test_feature)\n",
    "  return test_pres\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def tune_mlp(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "  f_score = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  parameter_save = []\n",
    "  activations = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
    "  solvers = [\"lbfgs\",\"sgd\",\"adam\"]\n",
    "  alphas = [1e-4,1e-3,1e-2,0.1,1]\n",
    "  for activation in activations:\n",
    "    for solver in solvers:\n",
    "      for alpha in alphas:\n",
    "        mlp = MLPClassifier(solver=solver, activation=activation,alpha=alpha,hidden_layer_sizes=(100,), random_state=1,max_iter=100,verbose=10,learning_rate_init=.1)\n",
    "        mlp.fit(train_feature,train_label)\n",
    "        preds = mlp.predict(dev_feature)\n",
    "        p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "        print(p)\n",
    "        print(r)\n",
    "        print(f)\n",
    "        f_score.append(f)\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        parameter_save.append((activation,solver,alpha))\n",
    "      \n",
    "  i = f_score.index(max(f_score))\n",
    "  print(precision[i])\n",
    "  print(recall[i])\n",
    "  print(f_score[i])\n",
    "  (activation,solver,alpha) = parameter_save[i]\n",
    "  clf = MLPClassifier(solver=solver, activation=activation,alpha=alpha,hidden_layer_sizes=(100,), random_state=1,max_iter=100,verbose=10,learning_rate_init=.1)\n",
    "  print(clf)\n",
    "  clf.fit(train_feature,train_label)\n",
    "  test_pres = clf.predict(test_feature)\n",
    "  return test_pres\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "# import numpy as np\n",
    "# # fit model no training data\n",
    "# #to be tuned:max_depth、n_estimate、学习率\n",
    "# def tune_xgb(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "#   depthList=np.arange(3,10)\n",
    "#   learn_rates = [0.1,0.2,0.3,0.4,0.5]\n",
    "#   f_score = []\n",
    "#   precision = []\n",
    "#   recall = []\n",
    "#   parameter_save = []\n",
    "#   for n_estimate in range(10,101):\n",
    "#     for depth in depthList:\n",
    "#       for learningrate in learn_rates:\n",
    "#         model = XGBClassifier(n_estimatores = n_estimate,max_depth = depth,learning_rate = learningrate)\n",
    "#         model.fit(train_feature, train_label)\n",
    "#         preds = model.predict(dev_feature)\n",
    "#         p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "#         print(p)\n",
    "#         print(r)\n",
    "#         print(f)\n",
    "#         f_score.append(f)\n",
    "#         precision.append(p)\n",
    "#         recall.append(r)\n",
    "#         parameter_save.append((n_estimate,depth,learningrate))\n",
    "#   i = f_score.index(max(f_score))\n",
    "#   print(precision[i])\n",
    "#   print(recall[i])\n",
    "#   print(f_score[i])\n",
    "#   (n,d,l) = parameter_save[i]\n",
    "#   clf = XGBClassifier(n_estimatores = n,max_depth = d,learning_rate = l)\n",
    "#   clf.fit(train_feature,train_label)\n",
    "#   test_pres = clf.predict(test_feature)\n",
    "#   return test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22384,
     "status": "ok",
     "timestamp": 1588956380972,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "RK6tkczMg0wb",
    "outputId": "6df970a2-811a-43f0-ddbe-5f7708c7915a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n",
      "0.9\n",
      "0.8571428571428572\n",
      "0.7543859649122807\n",
      "0.86\n",
      "0.8037383177570094\n"
     ]
    }
   ],
   "source": [
    "pred_log = tune_log(vector_train,train_labels,vector_development,dev_labels,vector_test) # 0.66\n",
    "pred_nb = tune_nb(vector_train,train_labels,vector_development,dev_labels,vector_test) #0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6902,
     "status": "ok",
     "timestamp": 1587926068292,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "I5c2x9CHibZU",
    "outputId": "f28fb8d1-4850-43a0-c3f6-fc75b98ee1c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "pred_log_w2v = tune_log(tfidf_train_vec_skip_gram,train_labels,tfidf_dev_vec_skip_gram,dev_labels,tfidf_test_vec_skip_gram) #to be submitted runned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19146,
     "status": "ok",
     "timestamp": 1588955989387,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "bRDYvWwEuYBL",
    "outputId": "9c17b593-88f4-4a4b-d772-57eee52a4946"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n",
      "0.7547169811320755\n",
      "0.8\n",
      "0.7766990291262137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_log_tfidf = tune_log(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector) #0.68\n",
    "pred_nb_tfidf = tune_nb(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector) #to be submitted runned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_output(pred_log_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15361,
     "status": "ok",
     "timestamp": 1587926385539,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "_PYSVu_UxpTH",
    "outputId": "8c634d0a-6337-4923-d310-c185a6ffcd55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "pred_log_gw2v = tune_log(X_train_word_average,train_labels,X_dev_word_average,dev_labels,X_test_word_average) #to be submitted runned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 531963,
     "status": "ok",
     "timestamp": 1588945586235,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "vGjxzIhtumgU",
    "outputId": "ed9b59d8-6368-4649-dd7c-174c5682113d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.803921568627451\n",
      "0.82\n",
      "0.8118811881188118\n",
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.85\n",
      "0.68\n",
      "0.7555555555555556\n",
      "0.8653846153846154\n",
      "0.9\n",
      "0.8823529411764707\n",
      "0.7796610169491526\n",
      "0.92\n",
      "0.8440366972477064\n",
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n"
     ]
    }
   ],
   "source": [
    "pred_svm_tfidf = tune_svm(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14081,
     "status": "ok",
     "timestamp": 1588945618693,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "mCnnkQHQxsaG",
    "outputId": "64ac3d92-0bb2-44da-cc8c-217edaee2939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157894736842105\n",
      "0.62\n",
      "0.7045454545454546\n",
      "0.7647058823529411\n",
      "0.78\n",
      "0.7722772277227723\n",
      "0.7954545454545454\n",
      "0.7\n",
      "0.7446808510638298\n",
      "0.7708333333333334\n",
      "0.74\n",
      "0.7551020408163266\n",
      "0.7872340425531915\n",
      "0.74\n",
      "0.7628865979381443\n",
      "0.7755102040816326\n",
      "0.76\n",
      "0.7676767676767676\n",
      "0.782608695652174\n",
      "0.72\n",
      "0.7499999999999999\n",
      "0.7708333333333334\n",
      "0.74\n",
      "0.7551020408163266\n",
      "0.7872340425531915\n",
      "0.74\n",
      "0.7628865979381443\n",
      "0.76\n",
      "0.76\n",
      "0.76\n",
      "0.7551020408163265\n",
      "0.74\n",
      "0.7474747474747474\n",
      "0.5319148936170213\n",
      "1.0\n",
      "0.6944444444444444\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.5\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.8157894736842105\n",
      "0.62\n",
      "0.7045454545454546\n",
      "0.7647058823529411\n",
      "0.78\n",
      "0.7722772277227723\n",
      "0.7954545454545454\n",
      "0.7\n",
      "0.7446808510638298\n",
      "0.7708333333333334\n",
      "0.74\n",
      "0.7551020408163266\n",
      "0.7872340425531915\n",
      "0.74\n",
      "0.7628865979381443\n",
      "0.7755102040816326\n",
      "0.76\n",
      "0.7676767676767676\n",
      "0.782608695652174\n",
      "0.72\n",
      "0.7499999999999999\n",
      "0.7708333333333334\n",
      "0.74\n",
      "0.7551020408163266\n",
      "0.7872340425531915\n",
      "0.74\n",
      "0.7628865979381443\n",
      "0.76\n",
      "0.76\n",
      "0.76\n",
      "0.7551020408163265\n",
      "0.74\n",
      "0.7474747474747474\n",
      "0.7647058823529411\n",
      "0.78\n",
      "0.7722772277227723\n"
     ]
    }
   ],
   "source": [
    "pred_knn_tfidf = tune_knn(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4677,
     "status": "ok",
     "timestamp": 1588945633311,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "htLJfUCLyEX_",
    "outputId": "05c66a5b-8ca8-412c-f630-c844305b39f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5208333333333334\n",
      "1.0\n",
      "0.684931506849315\n",
      "0.6024096385542169\n",
      "1.0\n",
      "0.7518796992481204\n",
      "0.6024096385542169\n",
      "1.0\n",
      "0.7518796992481204\n",
      "0.625\n",
      "1.0\n",
      "0.7692307692307693\n",
      "0.6329113924050633\n",
      "1.0\n",
      "0.7751937984496124\n",
      "0.6410256410256411\n",
      "1.0\n",
      "0.7812500000000001\n",
      "0.6363636363636364\n",
      "0.98\n",
      "0.7716535433070867\n",
      "0.6410256410256411\n",
      "1.0\n",
      "0.7812500000000001\n"
     ]
    }
   ],
   "source": [
    "pred_decisiontree_tfidf = tune_decisiontree(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 113663,
     "status": "ok",
     "timestamp": 1588945752634,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "t9fv47t1yRJQ",
    "outputId": "b7982963-b706-46f0-d876-4a1f6d1aa298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7090909090909091\n",
      "0.78\n",
      "0.7428571428571428\n",
      "0.7213114754098361\n",
      "0.88\n",
      "0.7927927927927928\n",
      "0.7241379310344828\n",
      "0.84\n",
      "0.7777777777777777\n",
      "0.7049180327868853\n",
      "0.86\n",
      "0.7747747747747747\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.6949152542372882\n",
      "0.82\n",
      "0.7522935779816514\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.6949152542372882\n",
      "0.82\n",
      "0.7522935779816514\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7068965517241379\n",
      "0.82\n",
      "0.7592592592592593\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7068965517241379\n",
      "0.82\n",
      "0.7592592592592593\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7068965517241379\n",
      "0.82\n",
      "0.7592592592592593\n",
      "0.7592592592592593\n",
      "0.82\n",
      "0.7884615384615384\n",
      "0.7454545454545455\n",
      "0.82\n",
      "0.780952380952381\n",
      "0.7592592592592593\n",
      "0.82\n",
      "0.7884615384615384\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7454545454545455\n",
      "0.82\n",
      "0.780952380952381\n",
      "0.7241379310344828\n",
      "0.84\n",
      "0.7777777777777777\n",
      "0.7592592592592593\n",
      "0.82\n",
      "0.7884615384615384\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7454545454545455\n",
      "0.82\n",
      "0.780952380952381\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.7368421052631579\n",
      "0.84\n",
      "0.7850467289719626\n",
      "0.7413793103448276\n",
      "0.86\n",
      "0.7962962962962963\n",
      "0.7413793103448276\n",
      "0.86\n",
      "0.7962962962962963\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7413793103448276\n",
      "0.86\n",
      "0.7962962962962963\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7413793103448276\n",
      "0.86\n",
      "0.7962962962962963\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7368421052631579\n",
      "0.84\n",
      "0.7850467289719626\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7192982456140351\n",
      "0.82\n",
      "0.7663551401869159\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7321428571428571\n",
      "0.82\n",
      "0.7735849056603773\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7333333333333333\n",
      "0.88\n",
      "0.8\n",
      "0.7288135593220338\n",
      "0.86\n",
      "0.7889908256880733\n",
      "0.7333333333333333\n",
      "0.88\n",
      "0.8\n",
      "0.7333333333333333\n",
      "0.88\n",
      "0.8\n",
      "0.7333333333333333\n",
      "0.88\n",
      "0.8\n",
      "0.7333333333333333\n",
      "0.88\n",
      "0.8\n",
      "0.7213114754098361\n",
      "0.88\n",
      "0.7927927927927928\n",
      "0.7213114754098361\n",
      "0.88\n",
      "0.7927927927927928\n",
      "0.7213114754098361\n",
      "0.88\n",
      "0.7927927927927928\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7213114754098361\n",
      "0.88\n",
      "0.7927927927927928\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7213114754098361\n",
      "0.88\n",
      "0.7927927927927928\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.7166666666666667\n",
      "0.86\n",
      "0.7818181818181817\n",
      "0.711864406779661\n",
      "0.84\n",
      "0.7706422018348624\n",
      "0.7333333333333333\n",
      "0.88\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "pred_randomforest_tfidf = tune_randomforest(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8436281,
     "status": "ok",
     "timestamp": 1588954198400,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "2Q8yfgky-bqS",
    "outputId": "c1e73498-862f-4617-a549-fbb1dfe64b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8936170212765957\n",
      "0.84\n",
      "0.8659793814432989\n",
      "0.9130434782608695\n",
      "0.84\n",
      "0.8749999999999999\n",
      "0.8979591836734694\n",
      "0.88\n",
      "0.888888888888889\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "0.8679245283018868\n",
      "0.92\n",
      "0.8932038834951457\n",
      "Iteration 1, loss = 0.64914262\n",
      "Iteration 2, loss = 0.57880381\n",
      "Iteration 3, loss = 0.46835045\n",
      "Iteration 4, loss = 0.33324788\n",
      "Iteration 5, loss = 0.22961332\n",
      "Iteration 6, loss = 0.16901454\n",
      "Iteration 7, loss = 0.13203234\n",
      "Iteration 8, loss = 0.10648177\n",
      "Iteration 9, loss = 0.08807138\n",
      "Iteration 10, loss = 0.07446773\n",
      "Iteration 11, loss = 0.06310193\n",
      "Iteration 12, loss = 0.05513173\n",
      "Iteration 13, loss = 0.04780408\n",
      "Iteration 14, loss = 0.04162781\n",
      "Iteration 15, loss = 0.03684286\n",
      "Iteration 16, loss = 0.03299014\n",
      "Iteration 17, loss = 0.02964879\n",
      "Iteration 18, loss = 0.02694449\n",
      "Iteration 19, loss = 0.02428382\n",
      "Iteration 20, loss = 0.02207882\n",
      "Iteration 21, loss = 0.02026603\n",
      "Iteration 22, loss = 0.01851880\n",
      "Iteration 23, loss = 0.01715021\n",
      "Iteration 24, loss = 0.01608571\n",
      "Iteration 25, loss = 0.01480275\n",
      "Iteration 26, loss = 0.01381174\n",
      "Iteration 27, loss = 0.01295392\n",
      "Iteration 28, loss = 0.01218257\n",
      "Iteration 29, loss = 0.01148986\n",
      "Iteration 30, loss = 0.01085886\n",
      "Iteration 31, loss = 0.01026524\n",
      "Iteration 32, loss = 0.00975498\n",
      "Iteration 33, loss = 0.00922724\n",
      "Iteration 34, loss = 0.00884403\n",
      "Iteration 35, loss = 0.00840101\n",
      "Iteration 36, loss = 0.00802059\n",
      "Iteration 37, loss = 0.00766572\n",
      "Iteration 38, loss = 0.00736244\n",
      "Iteration 39, loss = 0.00706961\n",
      "Iteration 40, loss = 0.00680984\n",
      "Iteration 41, loss = 0.00654807\n",
      "Iteration 42, loss = 0.00630701\n",
      "Iteration 43, loss = 0.00607202\n",
      "Iteration 44, loss = 0.00585911\n",
      "Iteration 45, loss = 0.00565220\n",
      "Iteration 46, loss = 0.00547558\n",
      "Iteration 47, loss = 0.00530469\n",
      "Iteration 48, loss = 0.00513113\n",
      "Iteration 49, loss = 0.00497256\n",
      "Iteration 50, loss = 0.00482065\n",
      "Iteration 51, loss = 0.00468976\n",
      "Iteration 52, loss = 0.00455519\n",
      "Iteration 53, loss = 0.00443678\n",
      "Iteration 54, loss = 0.00430656\n",
      "Iteration 55, loss = 0.00419815\n",
      "Iteration 56, loss = 0.00408553\n",
      "Iteration 57, loss = 0.00398304\n",
      "Iteration 58, loss = 0.00387941\n",
      "Iteration 59, loss = 0.00378940\n",
      "Iteration 60, loss = 0.00370093\n",
      "Iteration 61, loss = 0.00361007\n",
      "Iteration 62, loss = 0.00353621\n",
      "Iteration 63, loss = 0.00345337\n",
      "Iteration 64, loss = 0.00337412\n",
      "Iteration 65, loss = 0.00331496\n",
      "Iteration 66, loss = 0.00323956\n",
      "Iteration 67, loss = 0.00316662\n",
      "Iteration 68, loss = 0.00310619\n",
      "Iteration 69, loss = 0.00303811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.64960315\n",
      "Iteration 2, loss = 0.57927631\n",
      "Iteration 3, loss = 0.46885746\n",
      "Iteration 4, loss = 0.33379720\n",
      "Iteration 5, loss = 0.23019550\n",
      "Iteration 6, loss = 0.16962369\n",
      "Iteration 7, loss = 0.13266575\n",
      "Iteration 8, loss = 0.10713681\n",
      "Iteration 9, loss = 0.08874624\n",
      "Iteration 10, loss = 0.07516078\n",
      "Iteration 11, loss = 0.06381154\n",
      "Iteration 12, loss = 0.05585620\n",
      "Iteration 13, loss = 0.04854265\n",
      "Iteration 14, loss = 0.04237966\n",
      "Iteration 15, loss = 0.03760655\n",
      "Iteration 16, loss = 0.03376514\n",
      "Iteration 17, loss = 0.03043368\n",
      "Iteration 18, loss = 0.02773998\n",
      "Iteration 19, loss = 0.02508765\n",
      "Iteration 20, loss = 0.02289139\n",
      "Iteration 21, loss = 0.02108674\n",
      "Iteration 22, loss = 0.01934677\n",
      "Iteration 23, loss = 0.01798515\n",
      "Iteration 24, loss = 0.01692769\n",
      "Iteration 25, loss = 0.01565124\n",
      "Iteration 26, loss = 0.01466592\n",
      "Iteration 27, loss = 0.01381399\n",
      "Iteration 28, loss = 0.01304826\n",
      "Iteration 29, loss = 0.01236058\n",
      "Iteration 30, loss = 0.01173495\n",
      "Iteration 31, loss = 0.01114595\n",
      "Iteration 32, loss = 0.01064085\n",
      "Iteration 33, loss = 0.01011702\n",
      "Iteration 34, loss = 0.00973822\n",
      "Iteration 35, loss = 0.00929952\n",
      "Iteration 36, loss = 0.00892304\n",
      "Iteration 37, loss = 0.00857201\n",
      "Iteration 38, loss = 0.00827262\n",
      "Iteration 39, loss = 0.00798363\n",
      "Iteration 40, loss = 0.00772749\n",
      "Iteration 41, loss = 0.00746909\n",
      "Iteration 42, loss = 0.00723123\n",
      "Iteration 43, loss = 0.00699963\n",
      "Iteration 44, loss = 0.00678981\n",
      "Iteration 45, loss = 0.00658586\n",
      "Iteration 46, loss = 0.00641233\n",
      "Iteration 47, loss = 0.00624439\n",
      "Iteration 48, loss = 0.00607355\n",
      "Iteration 49, loss = 0.00591775\n",
      "Iteration 50, loss = 0.00576850\n",
      "Iteration 51, loss = 0.00564039\n",
      "Iteration 52, loss = 0.00550840\n",
      "Iteration 53, loss = 0.00539268\n",
      "Iteration 54, loss = 0.00526461\n",
      "Iteration 55, loss = 0.00515884\n",
      "Iteration 56, loss = 0.00504846\n",
      "Iteration 57, loss = 0.00494833\n",
      "Iteration 58, loss = 0.00484690\n",
      "Iteration 59, loss = 0.00475908\n",
      "Iteration 60, loss = 0.00467299\n",
      "Iteration 61, loss = 0.00458407\n",
      "Iteration 62, loss = 0.00451242\n",
      "Iteration 63, loss = 0.00443154\n",
      "Iteration 64, loss = 0.00435450\n",
      "Iteration 65, loss = 0.00429738\n",
      "Iteration 66, loss = 0.00422385\n",
      "Iteration 67, loss = 0.00415263\n",
      "Iteration 68, loss = 0.00409427\n",
      "Iteration 69, loss = 0.00402797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.65420682\n",
      "Iteration 2, loss = 0.58399382\n",
      "Iteration 3, loss = 0.47391268\n",
      "Iteration 4, loss = 0.33926774\n",
      "Iteration 5, loss = 0.23598664\n",
      "Iteration 6, loss = 0.17567635\n",
      "Iteration 7, loss = 0.13895274\n",
      "Iteration 8, loss = 0.11363145\n",
      "Iteration 9, loss = 0.09543043\n",
      "Iteration 10, loss = 0.08201807\n",
      "Iteration 11, loss = 0.07082548\n",
      "Iteration 12, loss = 0.06300979\n",
      "Iteration 13, loss = 0.05582812\n",
      "Iteration 14, loss = 0.04978870\n",
      "Iteration 15, loss = 0.04512475\n",
      "Iteration 16, loss = 0.04138711\n",
      "Iteration 17, loss = 0.03814514\n",
      "Iteration 18, loss = 0.03554811\n",
      "Iteration 19, loss = 0.03296979\n",
      "Iteration 20, loss = 0.03085127\n",
      "Iteration 21, loss = 0.02911851\n",
      "Iteration 22, loss = 0.02744146\n",
      "Iteration 23, loss = 0.02614008\n",
      "Iteration 24, loss = 0.02514318\n",
      "Iteration 25, loss = 0.02392268\n",
      "Iteration 26, loss = 0.02298416\n",
      "Iteration 27, loss = 0.02218151\n",
      "Iteration 28, loss = 0.02146230\n",
      "Iteration 29, loss = 0.02081514\n",
      "Iteration 30, loss = 0.02023340\n",
      "Iteration 31, loss = 0.01968067\n",
      "Iteration 32, loss = 0.01921762\n",
      "Iteration 33, loss = 0.01872300\n",
      "Iteration 34, loss = 0.01837814\n",
      "Iteration 35, loss = 0.01797309\n",
      "Iteration 36, loss = 0.01762600\n",
      "Iteration 37, loss = 0.01730356\n",
      "Iteration 38, loss = 0.01703329\n",
      "Iteration 39, loss = 0.01677285\n",
      "Iteration 40, loss = 0.01654309\n",
      "Iteration 41, loss = 0.01630846\n",
      "Iteration 42, loss = 0.01609241\n",
      "Iteration 43, loss = 0.01588511\n",
      "Iteration 44, loss = 0.01569620\n",
      "Iteration 45, loss = 0.01551195\n",
      "Iteration 46, loss = 0.01535930\n",
      "Iteration 47, loss = 0.01521094\n",
      "Iteration 48, loss = 0.01505747\n",
      "Iteration 49, loss = 0.01491940\n",
      "Iteration 50, loss = 0.01478661\n",
      "Iteration 51, loss = 0.01467648\n",
      "Iteration 52, loss = 0.01456037\n",
      "Iteration 53, loss = 0.01446177\n",
      "Iteration 54, loss = 0.01434504\n",
      "Iteration 55, loss = 0.01425579\n",
      "Iteration 56, loss = 0.01415799\n",
      "Iteration 57, loss = 0.01407131\n",
      "Iteration 58, loss = 0.01398204\n",
      "Iteration 59, loss = 0.01390619\n",
      "Iteration 60, loss = 0.01383414\n",
      "Iteration 61, loss = 0.01375456\n",
      "Iteration 62, loss = 0.01369524\n",
      "Iteration 63, loss = 0.01362382\n",
      "Iteration 64, loss = 0.01355947\n",
      "Iteration 65, loss = 0.01351275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.70008330\n",
      "Iteration 2, loss = 0.63042560\n",
      "Iteration 3, loss = 0.52299415\n",
      "Iteration 4, loss = 0.39175272\n",
      "Iteration 5, loss = 0.29092160\n",
      "Iteration 6, loss = 0.23245694\n",
      "Iteration 7, loss = 0.19730055\n",
      "Iteration 8, loss = 0.17327795\n",
      "Iteration 9, loss = 0.15619160\n",
      "Iteration 10, loss = 0.14372984\n",
      "Iteration 11, loss = 0.13332614\n",
      "Iteration 12, loss = 0.12613036\n",
      "Iteration 13, loss = 0.11949709\n",
      "Iteration 14, loss = 0.11392510\n",
      "Iteration 15, loss = 0.10958952\n",
      "Iteration 16, loss = 0.10613809\n",
      "Iteration 17, loss = 0.10304200\n",
      "Iteration 18, loss = 0.10067648\n",
      "Iteration 19, loss = 0.09810995\n",
      "Iteration 20, loss = 0.09604277\n",
      "Iteration 21, loss = 0.09431614\n",
      "Iteration 22, loss = 0.09255909\n",
      "Iteration 23, loss = 0.09116885\n",
      "Iteration 24, loss = 0.09007425\n",
      "Iteration 25, loss = 0.08877390\n",
      "Iteration 26, loss = 0.08761358\n",
      "Iteration 27, loss = 0.08664483\n",
      "Iteration 28, loss = 0.08575997\n",
      "Iteration 29, loss = 0.08486445\n",
      "Iteration 30, loss = 0.08408724\n",
      "Iteration 31, loss = 0.08326765\n",
      "Iteration 32, loss = 0.08262687\n",
      "Iteration 33, loss = 0.08182977\n",
      "Iteration 34, loss = 0.08120243\n",
      "Iteration 35, loss = 0.08056287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.07994069\n",
      "Iteration 37, loss = 0.07933808\n",
      "Iteration 38, loss = 0.07880061\n",
      "Iteration 39, loss = 0.07829529\n",
      "Iteration 40, loss = 0.07779623\n",
      "Iteration 41, loss = 0.07725850\n",
      "Iteration 42, loss = 0.07672049\n",
      "Iteration 43, loss = 0.07626240\n",
      "Iteration 44, loss = 0.07577707\n",
      "Iteration 45, loss = 0.07529935\n",
      "Iteration 46, loss = 0.07487066\n",
      "Iteration 47, loss = 0.07444101\n",
      "Iteration 48, loss = 0.07399367\n",
      "Iteration 49, loss = 0.07356892\n",
      "Iteration 50, loss = 0.07313713\n",
      "Iteration 51, loss = 0.07276498\n",
      "Iteration 52, loss = 0.07237567\n",
      "Iteration 53, loss = 0.07203216\n",
      "Iteration 54, loss = 0.07159137\n",
      "Iteration 55, loss = 0.07125066\n",
      "Iteration 56, loss = 0.07089189\n",
      "Iteration 57, loss = 0.07053617\n",
      "Iteration 58, loss = 0.07017278\n",
      "Iteration 59, loss = 0.06982410\n",
      "Iteration 60, loss = 0.06952353\n",
      "Iteration 61, loss = 0.06916720\n",
      "Iteration 62, loss = 0.06887084\n",
      "Iteration 63, loss = 0.06854192\n",
      "Iteration 64, loss = 0.06824595\n",
      "Iteration 65, loss = 0.06798325\n",
      "Iteration 66, loss = 0.06766757\n",
      "Iteration 67, loss = 0.06730971\n",
      "Iteration 68, loss = 0.06705782\n",
      "Iteration 69, loss = 0.06674478\n",
      "Iteration 70, loss = 0.06646341\n",
      "Iteration 71, loss = 0.06619607\n",
      "Iteration 72, loss = 0.06591488\n",
      "Iteration 73, loss = 0.06566765\n",
      "Iteration 74, loss = 0.06541914\n",
      "Iteration 75, loss = 0.06514770\n",
      "Iteration 76, loss = 0.06489856\n",
      "Iteration 77, loss = 0.06464846\n",
      "Iteration 78, loss = 0.06439143\n",
      "Iteration 79, loss = 0.06415028\n",
      "Iteration 80, loss = 0.06393972\n",
      "Iteration 81, loss = 0.06369795\n",
      "Iteration 82, loss = 0.06346773\n",
      "Iteration 83, loss = 0.06324846\n",
      "Iteration 84, loss = 0.06301183\n",
      "Iteration 85, loss = 0.06279963\n",
      "Iteration 86, loss = 0.06258749\n",
      "Iteration 87, loss = 0.06238997\n",
      "Iteration 88, loss = 0.06217240\n",
      "Iteration 89, loss = 0.06196071\n",
      "Iteration 90, loss = 0.06177024\n",
      "Iteration 91, loss = 0.06158623\n",
      "Iteration 92, loss = 0.06140594\n",
      "Iteration 93, loss = 0.06118176\n",
      "Iteration 94, loss = 0.06101455\n",
      "Iteration 95, loss = 0.06082547\n",
      "Iteration 96, loss = 0.06063882\n",
      "Iteration 97, loss = 0.06046748\n",
      "Iteration 98, loss = 0.06028547\n",
      "Iteration 99, loss = 0.06012600\n",
      "Iteration 100, loss = 0.05996133\n",
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14316695\n",
      "Iteration 2, loss = 1.02540158\n",
      "Iteration 3, loss = 0.88503880\n",
      "Iteration 4, loss = 0.73504901\n",
      "Iteration 5, loss = 0.61351986\n",
      "Iteration 6, loss = 0.53392030\n",
      "Iteration 7, loss = 0.48018366\n",
      "Iteration 8, loss = 0.43989797\n",
      "Iteration 9, loss = 0.40885534\n",
      "Iteration 10, loss = 0.38453698\n",
      "Iteration 11, loss = 0.36399385\n",
      "Iteration 12, loss = 0.34782239\n",
      "Iteration 13, loss = 0.33397763\n",
      "Iteration 14, loss = 0.32242213\n",
      "Iteration 15, loss = 0.31282948\n",
      "Iteration 16, loss = 0.30521440\n",
      "Iteration 17, loss = 0.29831066\n",
      "Iteration 18, loss = 0.29355486\n",
      "Iteration 19, loss = 0.28828826\n",
      "Iteration 20, loss = 0.28435300\n",
      "Iteration 21, loss = 0.28137484\n",
      "Iteration 22, loss = 0.27804413\n",
      "Iteration 23, loss = 0.27592349\n",
      "Iteration 24, loss = 0.27404370\n",
      "Iteration 25, loss = 0.27284239\n",
      "Iteration 26, loss = 0.27077028\n",
      "Iteration 27, loss = 0.26963928\n",
      "Iteration 28, loss = 0.26933916\n",
      "Iteration 29, loss = 0.26791118\n",
      "Iteration 30, loss = 0.26732003\n",
      "Iteration 31, loss = 0.26652830\n",
      "Iteration 32, loss = 0.26602826\n",
      "Iteration 33, loss = 0.26567583\n",
      "Iteration 34, loss = 0.26518847\n",
      "Iteration 35, loss = 0.26500054\n",
      "Iteration 36, loss = 0.26501311\n",
      "Iteration 37, loss = 0.26449747\n",
      "Iteration 38, loss = 0.26440362\n",
      "Iteration 39, loss = 0.26438489\n",
      "Iteration 40, loss = 0.26420559\n",
      "Iteration 41, loss = 0.26458608\n",
      "Iteration 42, loss = 0.26400038\n",
      "Iteration 43, loss = 0.26393125\n",
      "Iteration 44, loss = 0.26411584\n",
      "Iteration 45, loss = 0.26375163\n",
      "Iteration 46, loss = 0.26399918\n",
      "Iteration 47, loss = 0.26378856\n",
      "Iteration 48, loss = 0.26359314\n",
      "Iteration 49, loss = 0.26375558\n",
      "Iteration 50, loss = 0.26334261\n",
      "Iteration 51, loss = 0.26352274\n",
      "Iteration 52, loss = 0.26326611\n",
      "Iteration 53, loss = 0.26343740\n",
      "Iteration 54, loss = 0.26367773\n",
      "Iteration 55, loss = 0.26338649\n",
      "Iteration 56, loss = 0.26353148\n",
      "Iteration 57, loss = 0.26406294\n",
      "Iteration 58, loss = 0.26345121\n",
      "Iteration 59, loss = 0.26342844\n",
      "Iteration 60, loss = 0.26361293\n",
      "Iteration 61, loss = 0.26334999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8627450980392157\n",
      "0.88\n",
      "0.8712871287128714\n",
      "Iteration 1, loss = 0.24684875\n",
      "Iteration 2, loss = 0.17757635\n",
      "Iteration 3, loss = 0.14573203\n",
      "Iteration 4, loss = 0.10098523\n",
      "Iteration 5, loss = 0.06927806\n",
      "Iteration 6, loss = 0.04933838\n",
      "Iteration 7, loss = 0.03611092\n",
      "Iteration 8, loss = 0.02769028\n",
      "Iteration 9, loss = 0.02155205\n",
      "Iteration 10, loss = 0.01687076\n",
      "Iteration 11, loss = 0.01722885\n",
      "Iteration 12, loss = 0.01948621\n",
      "Iteration 13, loss = 0.02102448\n",
      "Iteration 14, loss = 0.02076098\n",
      "Iteration 15, loss = 0.02144742\n",
      "Iteration 16, loss = 0.03558617\n",
      "Iteration 17, loss = 0.07713198\n",
      "Iteration 18, loss = 0.08506559\n",
      "Iteration 19, loss = 0.08929649\n",
      "Iteration 20, loss = 0.08378936\n",
      "Iteration 21, loss = 0.06273623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7894736842105263\n",
      "0.9\n",
      "0.8411214953271027\n",
      "Iteration 1, loss = 0.63499674\n",
      "Iteration 2, loss = 0.43466977\n",
      "Iteration 3, loss = 0.19257124\n",
      "Iteration 4, loss = 0.08576411\n",
      "Iteration 5, loss = 0.07582544\n",
      "Iteration 6, loss = 0.09562409\n",
      "Iteration 7, loss = 0.12364346\n",
      "Iteration 8, loss = 0.16785463\n",
      "Iteration 9, loss = 0.17708611\n",
      "Iteration 10, loss = 0.17383760\n",
      "Iteration 11, loss = 0.14359168\n",
      "Iteration 12, loss = 0.13490506\n",
      "Iteration 13, loss = 0.10947244\n",
      "Iteration 14, loss = 0.11218305\n",
      "Iteration 15, loss = 0.19757686\n",
      "Iteration 16, loss = 0.19737479\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8653846153846154\n",
      "0.9\n",
      "0.8823529411764707\n",
      "Iteration 1, loss = 1.19526465\n",
      "Iteration 2, loss = 0.63153894\n",
      "Iteration 3, loss = 0.53794791\n",
      "Iteration 4, loss = 0.44456930\n",
      "Iteration 5, loss = 0.44640867\n",
      "Iteration 6, loss = 0.44921731\n",
      "Iteration 7, loss = 0.29041674\n",
      "Iteration 8, loss = 0.17458455\n",
      "Iteration 9, loss = 0.21531244\n",
      "Iteration 10, loss = 0.21942234\n",
      "Iteration 11, loss = 0.16128442\n",
      "Iteration 12, loss = 0.17128199\n",
      "Iteration 13, loss = 0.11724837\n",
      "Iteration 14, loss = 0.07597791\n",
      "Iteration 15, loss = 0.06140643\n",
      "Iteration 16, loss = 0.05077025\n",
      "Iteration 17, loss = 0.04774968\n",
      "Iteration 18, loss = 0.06308872\n",
      "Iteration 19, loss = 0.12573793\n",
      "Iteration 20, loss = 0.11383141\n",
      "Iteration 21, loss = 0.10462483\n",
      "Iteration 22, loss = 0.08563649\n",
      "Iteration 23, loss = 0.08672541\n",
      "Iteration 24, loss = 0.09179194\n",
      "Iteration 25, loss = 0.09001723\n",
      "Iteration 26, loss = 0.06830748\n",
      "Iteration 27, loss = 0.05792173\n",
      "Iteration 28, loss = 0.05422673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8035714285714286\n",
      "0.9\n",
      "0.8490566037735849\n",
      "Iteration 1, loss = 4.01448145\n",
      "Iteration 2, loss = 1.81862025\n",
      "Iteration 3, loss = 1.41343545\n",
      "Iteration 4, loss = 0.90599518\n",
      "Iteration 5, loss = 0.88320664\n",
      "Iteration 6, loss = 1.13201211\n",
      "Iteration 7, loss = 0.61093364\n",
      "Iteration 8, loss = 0.46229504\n",
      "Iteration 9, loss = 0.43797901\n",
      "Iteration 10, loss = 0.39525259\n",
      "Iteration 11, loss = 0.31668891\n",
      "Iteration 12, loss = 0.30499971\n",
      "Iteration 13, loss = 0.26068224\n",
      "Iteration 14, loss = 0.23247797\n",
      "Iteration 15, loss = 0.21370274\n",
      "Iteration 16, loss = 0.22459295\n",
      "Iteration 17, loss = 0.21324353\n",
      "Iteration 18, loss = 0.21641883\n",
      "Iteration 19, loss = 0.23243809\n",
      "Iteration 20, loss = 0.22675849\n",
      "Iteration 21, loss = 0.23422445\n",
      "Iteration 22, loss = 0.20944671\n",
      "Iteration 23, loss = 0.21666969\n",
      "Iteration 24, loss = 0.18824383\n",
      "Iteration 25, loss = 0.18550151\n",
      "Iteration 26, loss = 0.20148143\n",
      "Iteration 27, loss = 0.19278354\n",
      "Iteration 28, loss = 0.18901608\n",
      "Iteration 29, loss = 0.19534099\n",
      "Iteration 30, loss = 0.20124091\n",
      "Iteration 31, loss = 0.21357793\n",
      "Iteration 32, loss = 0.21131230\n",
      "Iteration 33, loss = 0.20332895\n",
      "Iteration 34, loss = 0.18072889\n",
      "Iteration 35, loss = 0.18179550\n",
      "Iteration 36, loss = 0.18937439\n",
      "Iteration 37, loss = 0.19933327\n",
      "Iteration 38, loss = 0.20695851\n",
      "Iteration 39, loss = 0.20199661\n",
      "Iteration 40, loss = 0.21177361\n",
      "Iteration 41, loss = 0.20860343\n",
      "Iteration 42, loss = 0.22445504\n",
      "Iteration 43, loss = 0.20960961\n",
      "Iteration 44, loss = 0.21363345\n",
      "Iteration 45, loss = 0.20221778\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8490566037735849\n",
      "0.9\n",
      "0.8737864077669903\n",
      "Iteration 1, loss = 39.22878616\n",
      "Iteration 2, loss = 6.92875082\n",
      "Iteration 3, loss = 2.25781735\n",
      "Iteration 4, loss = 1.11223343\n",
      "Iteration 5, loss = 0.92892866\n",
      "Iteration 6, loss = 0.87352875\n",
      "Iteration 7, loss = 0.85271328\n",
      "Iteration 8, loss = 0.87184882\n",
      "Iteration 9, loss = 0.84506816\n",
      "Iteration 10, loss = 0.79389607\n",
      "Iteration 11, loss = 0.75653187\n",
      "Iteration 12, loss = 0.76892911\n",
      "Iteration 13, loss = 0.75515531\n",
      "Iteration 14, loss = 0.73219837\n",
      "Iteration 15, loss = 0.69783495\n",
      "Iteration 16, loss = 0.74140490\n",
      "Iteration 17, loss = 0.70650923\n",
      "Iteration 18, loss = 0.65419070\n",
      "Iteration 19, loss = 0.66357190\n",
      "Iteration 20, loss = 0.63542163\n",
      "Iteration 21, loss = 0.63432762\n",
      "Iteration 22, loss = 0.65232071\n",
      "Iteration 23, loss = 0.62279581\n",
      "Iteration 24, loss = 0.60066585\n",
      "Iteration 25, loss = 0.60935493\n",
      "Iteration 26, loss = 0.61869191\n",
      "Iteration 27, loss = 0.58803678\n",
      "Iteration 28, loss = 0.56947431\n",
      "Iteration 29, loss = 0.56802965\n",
      "Iteration 30, loss = 0.57012082\n",
      "Iteration 31, loss = 0.53864037\n",
      "Iteration 32, loss = 0.53917832\n",
      "Iteration 33, loss = 0.54730181\n",
      "Iteration 34, loss = 0.53798744\n",
      "Iteration 35, loss = 0.54795763\n",
      "Iteration 36, loss = 0.54394021\n",
      "Iteration 37, loss = 0.54463859\n",
      "Iteration 38, loss = 0.55170977\n",
      "Iteration 39, loss = 0.57076963\n",
      "Iteration 40, loss = 0.55398533\n",
      "Iteration 41, loss = 0.53890875\n",
      "Iteration 42, loss = 0.53504965\n",
      "Iteration 43, loss = 0.55142350\n",
      "Iteration 44, loss = 0.56561035\n",
      "Iteration 45, loss = 0.51614545\n",
      "Iteration 46, loss = 0.52686176\n",
      "Iteration 47, loss = 0.51013645\n",
      "Iteration 48, loss = 0.50027064\n",
      "Iteration 49, loss = 0.50218892\n",
      "Iteration 50, loss = 0.50999987\n",
      "Iteration 51, loss = 0.51042759\n",
      "Iteration 52, loss = 0.51124096\n",
      "Iteration 53, loss = 0.51617311\n",
      "Iteration 54, loss = 0.52922616\n",
      "Iteration 55, loss = 0.52965745\n",
      "Iteration 56, loss = 0.52318014\n",
      "Iteration 57, loss = 0.52744731\n",
      "Iteration 58, loss = 0.52892556\n",
      "Iteration 59, loss = 0.51608788\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.86\n",
      "0.86\n",
      "0.8599999999999999\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "0.9183673469387755\n",
      "0.9\n",
      "0.9090909090909091\n",
      "0.9166666666666666\n",
      "0.88\n",
      "0.8979591836734694\n",
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n",
      "Iteration 1, loss = 0.66204324\n",
      "Iteration 2, loss = 0.66064167\n",
      "Iteration 3, loss = 0.65539236\n",
      "Iteration 4, loss = 0.65921821\n",
      "Iteration 5, loss = 0.65412320\n",
      "Iteration 6, loss = 0.64931997\n",
      "Iteration 7, loss = 0.64707181\n",
      "Iteration 8, loss = 0.64074350\n",
      "Iteration 9, loss = 0.63095856\n",
      "Iteration 10, loss = 0.62234904\n",
      "Iteration 11, loss = 0.60733455\n",
      "Iteration 12, loss = 0.59093276\n",
      "Iteration 13, loss = 0.56538243\n",
      "Iteration 14, loss = 0.53800106\n",
      "Iteration 15, loss = 0.50924033\n",
      "Iteration 16, loss = 0.46675880\n",
      "Iteration 17, loss = 0.43351235\n",
      "Iteration 18, loss = 0.39347614\n",
      "Iteration 19, loss = 0.35717183\n",
      "Iteration 20, loss = 0.32331476\n",
      "Iteration 21, loss = 0.29525667\n",
      "Iteration 22, loss = 0.26807394\n",
      "Iteration 23, loss = 0.24592776\n",
      "Iteration 24, loss = 0.22666324\n",
      "Iteration 25, loss = 0.21014585\n",
      "Iteration 26, loss = 0.19455183\n",
      "Iteration 27, loss = 0.18091009\n",
      "Iteration 28, loss = 0.17035808\n",
      "Iteration 29, loss = 0.15912389\n",
      "Iteration 30, loss = 0.14960483\n",
      "Iteration 31, loss = 0.14086791\n",
      "Iteration 32, loss = 0.13300510\n",
      "Iteration 33, loss = 0.12624246\n",
      "Iteration 34, loss = 0.11932286\n",
      "Iteration 35, loss = 0.11371694\n",
      "Iteration 36, loss = 0.10838806\n",
      "Iteration 37, loss = 0.10297011\n",
      "Iteration 38, loss = 0.09824556\n",
      "Iteration 39, loss = 0.09394081\n",
      "Iteration 40, loss = 0.08987289\n",
      "Iteration 41, loss = 0.08639680\n",
      "Iteration 42, loss = 0.08231193\n",
      "Iteration 43, loss = 0.07897832\n",
      "Iteration 44, loss = 0.07588069\n",
      "Iteration 45, loss = 0.07256088\n",
      "Iteration 46, loss = 0.07017115\n",
      "Iteration 47, loss = 0.06736486\n",
      "Iteration 48, loss = 0.06473664\n",
      "Iteration 49, loss = 0.06231501\n",
      "Iteration 50, loss = 0.05984941\n",
      "Iteration 51, loss = 0.05792107\n",
      "Iteration 52, loss = 0.05578004\n",
      "Iteration 53, loss = 0.05412821\n",
      "Iteration 54, loss = 0.05214421\n",
      "Iteration 55, loss = 0.05051786\n",
      "Iteration 56, loss = 0.04896341\n",
      "Iteration 57, loss = 0.04736906\n",
      "Iteration 58, loss = 0.04575669\n",
      "Iteration 59, loss = 0.04423934\n",
      "Iteration 60, loss = 0.04316703\n",
      "Iteration 61, loss = 0.04160440\n",
      "Iteration 62, loss = 0.04046962\n",
      "Iteration 63, loss = 0.03927206\n",
      "Iteration 64, loss = 0.03819518\n",
      "Iteration 65, loss = 0.03719975\n",
      "Iteration 66, loss = 0.03613661\n",
      "Iteration 67, loss = 0.03489963\n",
      "Iteration 68, loss = 0.03416048\n",
      "Iteration 69, loss = 0.03303068\n",
      "Iteration 70, loss = 0.03227128\n",
      "Iteration 71, loss = 0.03142892\n",
      "Iteration 72, loss = 0.03052673\n",
      "Iteration 73, loss = 0.02979201\n",
      "Iteration 74, loss = 0.02908692\n",
      "Iteration 75, loss = 0.02835255\n",
      "Iteration 76, loss = 0.02776137\n",
      "Iteration 77, loss = 0.02695080\n",
      "Iteration 78, loss = 0.02629802\n",
      "Iteration 79, loss = 0.02567615\n",
      "Iteration 80, loss = 0.02515726\n",
      "Iteration 81, loss = 0.02455018\n",
      "Iteration 82, loss = 0.02397468\n",
      "Iteration 83, loss = 0.02349123\n",
      "Iteration 84, loss = 0.02288506\n",
      "Iteration 85, loss = 0.02242387\n",
      "Iteration 86, loss = 0.02193767\n",
      "Iteration 87, loss = 0.02152799\n",
      "Iteration 88, loss = 0.02099390\n",
      "Iteration 89, loss = 0.02052976\n",
      "Iteration 90, loss = 0.02014660\n",
      "Iteration 91, loss = 0.01976396\n",
      "Iteration 92, loss = 0.01946339\n",
      "Iteration 93, loss = 0.01893183\n",
      "Iteration 94, loss = 0.01862646\n",
      "Iteration 95, loss = 0.01822725\n",
      "Iteration 96, loss = 0.01789049\n",
      "Iteration 97, loss = 0.01754995\n",
      "Iteration 98, loss = 0.01722129\n",
      "Iteration 99, loss = 0.01691668\n",
      "Iteration 100, loss = 0.01663292\n",
      "0.88\n",
      "0.88\n",
      "0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66219651\n",
      "Iteration 2, loss = 0.66079520\n",
      "Iteration 3, loss = 0.65554676\n",
      "Iteration 4, loss = 0.65937431\n",
      "Iteration 5, loss = 0.65428209\n",
      "Iteration 6, loss = 0.64948314\n",
      "Iteration 7, loss = 0.64724140\n",
      "Iteration 8, loss = 0.64092251\n",
      "Iteration 9, loss = 0.63115111\n",
      "Iteration 10, loss = 0.62256070\n",
      "Iteration 11, loss = 0.60757229\n",
      "Iteration 12, loss = 0.59120510\n",
      "Iteration 13, loss = 0.56569716\n",
      "Iteration 14, loss = 0.53836676\n",
      "Iteration 15, loss = 0.50966445\n",
      "Iteration 16, loss = 0.46723462\n",
      "Iteration 17, loss = 0.43404535\n",
      "Iteration 18, loss = 0.39404849\n",
      "Iteration 19, loss = 0.35778070\n",
      "Iteration 20, loss = 0.32395012\n",
      "Iteration 21, loss = 0.29591613\n",
      "Iteration 22, loss = 0.26875488\n",
      "Iteration 23, loss = 0.24662934\n",
      "Iteration 24, loss = 0.22738578\n",
      "Iteration 25, loss = 0.21088783\n",
      "Iteration 26, loss = 0.19531612\n",
      "Iteration 27, loss = 0.18169263\n",
      "Iteration 28, loss = 0.17116182\n",
      "Iteration 29, loss = 0.15994916\n",
      "Iteration 30, loss = 0.15045091\n",
      "Iteration 31, loss = 0.14173404\n",
      "Iteration 32, loss = 0.13388992\n",
      "Iteration 33, loss = 0.12714873\n",
      "Iteration 34, loss = 0.12024998\n",
      "Iteration 35, loss = 0.11466338\n",
      "Iteration 36, loss = 0.10935462\n",
      "Iteration 37, loss = 0.10395427\n",
      "Iteration 38, loss = 0.09924831\n",
      "Iteration 39, loss = 0.09496106\n",
      "Iteration 40, loss = 0.09091081\n",
      "Iteration 41, loss = 0.08745386\n",
      "Iteration 42, loss = 0.08338553\n",
      "Iteration 43, loss = 0.08006788\n",
      "Iteration 44, loss = 0.07698796\n",
      "Iteration 45, loss = 0.07368390\n",
      "Iteration 46, loss = 0.07131061\n",
      "Iteration 47, loss = 0.06851899\n",
      "Iteration 48, loss = 0.06590545\n",
      "Iteration 49, loss = 0.06349984\n",
      "Iteration 50, loss = 0.06104793\n",
      "Iteration 51, loss = 0.05913437\n",
      "Iteration 52, loss = 0.05700658\n",
      "Iteration 53, loss = 0.05536631\n",
      "Iteration 54, loss = 0.05340141\n",
      "Iteration 55, loss = 0.05178534\n",
      "Iteration 56, loss = 0.05024645\n",
      "Iteration 57, loss = 0.04866394\n",
      "Iteration 58, loss = 0.04706362\n",
      "Iteration 59, loss = 0.04555906\n",
      "Iteration 60, loss = 0.04449933\n",
      "Iteration 61, loss = 0.04294779\n",
      "Iteration 62, loss = 0.04182500\n",
      "Iteration 63, loss = 0.04063907\n",
      "Iteration 64, loss = 0.03957462\n",
      "Iteration 65, loss = 0.03858794\n",
      "Iteration 66, loss = 0.03753836\n",
      "Iteration 67, loss = 0.03630984\n",
      "Iteration 68, loss = 0.03558303\n",
      "Iteration 69, loss = 0.03446231\n",
      "Iteration 70, loss = 0.03371280\n",
      "Iteration 71, loss = 0.03288216\n",
      "Iteration 72, loss = 0.03198881\n",
      "Iteration 73, loss = 0.03126519\n",
      "Iteration 74, loss = 0.03056982\n",
      "Iteration 75, loss = 0.02984369\n",
      "Iteration 76, loss = 0.02926226\n",
      "Iteration 77, loss = 0.02846118\n",
      "Iteration 78, loss = 0.02781658\n",
      "Iteration 79, loss = 0.02720290\n",
      "Iteration 80, loss = 0.02669371\n",
      "Iteration 81, loss = 0.02609452\n",
      "Iteration 82, loss = 0.02552679\n",
      "Iteration 83, loss = 0.02505334\n",
      "Iteration 84, loss = 0.02445452\n",
      "Iteration 85, loss = 0.02400124\n",
      "Iteration 86, loss = 0.02352233\n",
      "Iteration 87, loss = 0.02312183\n",
      "Iteration 88, loss = 0.02259503\n",
      "Iteration 89, loss = 0.02213826\n",
      "Iteration 90, loss = 0.02176296\n",
      "Iteration 91, loss = 0.02138799\n",
      "Iteration 92, loss = 0.02109405\n",
      "Iteration 93, loss = 0.02056960\n",
      "Iteration 94, loss = 0.02027190\n",
      "Iteration 95, loss = 0.01987958\n",
      "Iteration 96, loss = 0.01954941\n",
      "Iteration 97, loss = 0.01921578\n",
      "Iteration 98, loss = 0.01889370\n",
      "Iteration 99, loss = 0.01859600\n",
      "Iteration 100, loss = 0.01831866\n",
      "0.88\n",
      "0.88\n",
      "0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66372873\n",
      "Iteration 2, loss = 0.66232810\n",
      "Iteration 3, loss = 0.65708591\n",
      "Iteration 4, loss = 0.66092801\n",
      "Iteration 5, loss = 0.65586118\n",
      "Iteration 6, loss = 0.65110226\n",
      "Iteration 7, loss = 0.64892200\n",
      "Iteration 8, loss = 0.64269423\n",
      "Iteration 9, loss = 0.63305467\n",
      "Iteration 10, loss = 0.62465138\n",
      "Iteration 11, loss = 0.60991918\n",
      "Iteration 12, loss = 0.59389267\n",
      "Iteration 13, loss = 0.56880290\n",
      "Iteration 14, loss = 0.54197613\n",
      "Iteration 15, loss = 0.51385225\n",
      "Iteration 16, loss = 0.47193496\n",
      "Iteration 17, loss = 0.43931393\n",
      "Iteration 18, loss = 0.39970791\n",
      "Iteration 19, loss = 0.36380253\n",
      "Iteration 20, loss = 0.33023341\n",
      "Iteration 21, loss = 0.30243600\n",
      "Iteration 22, loss = 0.27548486\n",
      "Iteration 23, loss = 0.25356042\n",
      "Iteration 24, loss = 0.23452057\n",
      "Iteration 25, loss = 0.21821103\n",
      "Iteration 26, loss = 0.20285644\n",
      "Iteration 27, loss = 0.18940895\n",
      "Iteration 28, loss = 0.17908379\n",
      "Iteration 29, loss = 0.16807966\n",
      "Iteration 30, loss = 0.15878268\n",
      "Iteration 31, loss = 0.15025950\n",
      "Iteration 32, loss = 0.14259555\n",
      "Iteration 33, loss = 0.13606149\n",
      "Iteration 34, loss = 0.12936424\n",
      "Iteration 35, loss = 0.12396334\n",
      "Iteration 36, loss = 0.11884851\n",
      "Iteration 37, loss = 0.11361590\n",
      "Iteration 38, loss = 0.10908864\n",
      "Iteration 39, loss = 0.10496842\n",
      "Iteration 40, loss = 0.10108696\n",
      "Iteration 41, loss = 0.09781335\n",
      "Iteration 42, loss = 0.09390164\n",
      "Iteration 43, loss = 0.09073536\n",
      "Iteration 44, loss = 0.08782439\n",
      "Iteration 45, loss = 0.08466920\n",
      "Iteration 46, loss = 0.08245151\n",
      "Iteration 47, loss = 0.07979788\n",
      "Iteration 48, loss = 0.07732232\n",
      "Iteration 49, loss = 0.07506829\n",
      "Iteration 50, loss = 0.07274379\n",
      "Iteration 51, loss = 0.07096895\n",
      "Iteration 52, loss = 0.06896436\n",
      "Iteration 53, loss = 0.06743122\n",
      "Iteration 54, loss = 0.06564648\n",
      "Iteration 55, loss = 0.06412402\n",
      "Iteration 56, loss = 0.06273082\n",
      "Iteration 57, loss = 0.06125881\n",
      "Iteration 58, loss = 0.05976803\n",
      "Iteration 59, loss = 0.05838193\n",
      "Iteration 60, loss = 0.05743817\n",
      "Iteration 61, loss = 0.05598685\n",
      "Iteration 62, loss = 0.05497432\n",
      "Iteration 63, loss = 0.05389397\n",
      "Iteration 64, loss = 0.05294400\n",
      "Iteration 65, loss = 0.05203383\n",
      "Iteration 66, loss = 0.05111022\n",
      "Iteration 67, loss = 0.04995618\n",
      "Iteration 68, loss = 0.04934316\n",
      "Iteration 69, loss = 0.04830464\n",
      "Iteration 70, loss = 0.04764172\n",
      "Iteration 71, loss = 0.04691631\n",
      "Iteration 72, loss = 0.04610329\n",
      "Iteration 73, loss = 0.04547849\n",
      "Iteration 74, loss = 0.04487185\n",
      "Iteration 75, loss = 0.04421622\n",
      "Iteration 76, loss = 0.04372026\n",
      "Iteration 77, loss = 0.04300581\n",
      "Iteration 78, loss = 0.04243144\n",
      "Iteration 79, loss = 0.04188797\n",
      "Iteration 80, loss = 0.04146327\n",
      "Iteration 81, loss = 0.04093342\n",
      "Iteration 82, loss = 0.04043206\n",
      "Iteration 83, loss = 0.04004774\n",
      "Iteration 84, loss = 0.03951167\n",
      "Iteration 85, loss = 0.03912683\n",
      "Iteration 86, loss = 0.03870866\n",
      "Iteration 87, loss = 0.03839007\n",
      "Iteration 88, loss = 0.03792616\n",
      "Iteration 89, loss = 0.03753233\n",
      "Iteration 90, loss = 0.03722401\n",
      "Iteration 91, loss = 0.03691466\n",
      "Iteration 92, loss = 0.03667318\n",
      "Iteration 93, loss = 0.03621224\n",
      "Iteration 94, loss = 0.03597823\n",
      "Iteration 95, loss = 0.03564503\n",
      "Iteration 96, loss = 0.03536729\n",
      "Iteration 97, loss = 0.03509160\n",
      "Iteration 98, loss = 0.03482512\n",
      "Iteration 99, loss = 0.03458574\n",
      "Iteration 100, loss = 0.03435907\n",
      "0.88\n",
      "0.88\n",
      "0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67899756\n",
      "Iteration 2, loss = 0.67741186\n",
      "Iteration 3, loss = 0.67200003\n",
      "Iteration 4, loss = 0.67574822\n",
      "Iteration 5, loss = 0.67069225\n",
      "Iteration 6, loss = 0.66608415\n",
      "Iteration 7, loss = 0.66425658\n",
      "Iteration 8, loss = 0.65865729\n",
      "Iteration 9, loss = 0.65002168\n",
      "Iteration 10, loss = 0.64313028\n",
      "Iteration 11, loss = 0.63054350\n",
      "Iteration 12, loss = 0.61744193\n",
      "Iteration 13, loss = 0.59600415\n",
      "Iteration 14, loss = 0.57365890\n",
      "Iteration 15, loss = 0.55078652\n",
      "Iteration 16, loss = 0.51359370\n",
      "Iteration 17, loss = 0.48633167\n",
      "Iteration 18, loss = 0.45042826\n",
      "Iteration 19, loss = 0.41794675\n",
      "Iteration 20, loss = 0.38674777\n",
      "Iteration 21, loss = 0.36096838\n",
      "Iteration 22, loss = 0.33575284\n",
      "Iteration 23, loss = 0.31542249\n",
      "Iteration 24, loss = 0.29794350\n",
      "Iteration 25, loss = 0.28303551\n",
      "Iteration 26, loss = 0.26935954\n",
      "Iteration 27, loss = 0.25715574\n",
      "Iteration 28, loss = 0.24838865\n",
      "Iteration 29, loss = 0.23891941\n",
      "Iteration 30, loss = 0.23111474\n",
      "Iteration 31, loss = 0.22400222\n",
      "Iteration 32, loss = 0.21763230\n",
      "Iteration 33, loss = 0.21259321\n",
      "Iteration 34, loss = 0.20738887\n",
      "Iteration 35, loss = 0.20329331\n",
      "Iteration 36, loss = 0.19957857\n",
      "Iteration 37, loss = 0.19541884\n",
      "Iteration 38, loss = 0.19215399\n",
      "Iteration 39, loss = 0.18914144\n",
      "Iteration 40, loss = 0.18637451\n",
      "Iteration 41, loss = 0.18438661\n",
      "Iteration 42, loss = 0.18141080\n",
      "Iteration 43, loss = 0.17919032\n",
      "Iteration 44, loss = 0.17741440\n",
      "Iteration 45, loss = 0.17515569\n",
      "Iteration 46, loss = 0.17390316\n",
      "Iteration 47, loss = 0.17205920\n",
      "Iteration 48, loss = 0.17039323\n",
      "Iteration 49, loss = 0.16909078\n",
      "Iteration 50, loss = 0.16742214\n",
      "Iteration 51, loss = 0.16646272\n",
      "Iteration 52, loss = 0.16510799\n",
      "Iteration 53, loss = 0.16417456\n",
      "Iteration 54, loss = 0.16343258\n",
      "Iteration 55, loss = 0.16232759\n",
      "Iteration 56, loss = 0.16171304\n",
      "Iteration 57, loss = 0.16100244\n",
      "Iteration 58, loss = 0.15990005\n",
      "Iteration 59, loss = 0.15913858\n",
      "Iteration 60, loss = 0.15875502\n",
      "Iteration 61, loss = 0.15773310\n",
      "Iteration 62, loss = 0.15729215\n",
      "Iteration 63, loss = 0.15667564\n",
      "Iteration 64, loss = 0.15619628\n",
      "Iteration 65, loss = 0.15560924\n",
      "Iteration 66, loss = 0.15531483\n",
      "Iteration 67, loss = 0.15450318\n",
      "Iteration 68, loss = 0.15439387\n",
      "Iteration 69, loss = 0.15377412\n",
      "Iteration 70, loss = 0.15338754\n",
      "Iteration 71, loss = 0.15306057\n",
      "Iteration 72, loss = 0.15264587\n",
      "Iteration 73, loss = 0.15239960\n",
      "Iteration 74, loss = 0.15232986\n",
      "Iteration 75, loss = 0.15172416\n",
      "Iteration 76, loss = 0.15146530\n",
      "Iteration 77, loss = 0.15131923\n",
      "Iteration 78, loss = 0.15091206\n",
      "Iteration 79, loss = 0.15049556\n",
      "Iteration 80, loss = 0.15036405\n",
      "Iteration 81, loss = 0.15014995\n",
      "Iteration 82, loss = 0.14980574\n",
      "Iteration 83, loss = 0.14973860\n",
      "Iteration 84, loss = 0.14939978\n",
      "Iteration 85, loss = 0.14921779\n",
      "Iteration 86, loss = 0.14897514\n",
      "Iteration 87, loss = 0.14896563\n",
      "Iteration 88, loss = 0.14874470\n",
      "Iteration 89, loss = 0.14849136\n",
      "Iteration 90, loss = 0.14840952\n",
      "Iteration 91, loss = 0.14830861\n",
      "Iteration 92, loss = 0.14813810\n",
      "Iteration 93, loss = 0.14790642\n",
      "Iteration 94, loss = 0.14783969\n",
      "Iteration 95, loss = 0.14776623\n",
      "Iteration 96, loss = 0.14745664\n",
      "Iteration 97, loss = 0.14732929\n",
      "Iteration 98, loss = 0.14725049\n",
      "Iteration 99, loss = 0.14726702\n",
      "Iteration 100, loss = 0.14702079\n",
      "0.8775510204081632\n",
      "0.86\n",
      "0.8686868686868686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.82646522\n",
      "Iteration 2, loss = 0.80538279\n",
      "Iteration 3, loss = 0.77947717\n",
      "Iteration 4, loss = 0.76573617\n",
      "Iteration 5, loss = 0.74652474\n",
      "Iteration 6, loss = 0.73087771\n",
      "Iteration 7, loss = 0.72101513\n",
      "Iteration 8, loss = 0.71032187\n",
      "Iteration 9, loss = 0.69961858\n",
      "Iteration 10, loss = 0.69398064\n",
      "Iteration 11, loss = 0.68621240\n",
      "Iteration 12, loss = 0.68199221\n",
      "Iteration 13, loss = 0.67323887\n",
      "Iteration 14, loss = 0.66839696\n",
      "Iteration 15, loss = 0.66842393\n",
      "Iteration 16, loss = 0.65417662\n",
      "Iteration 17, loss = 0.65717062\n",
      "Iteration 18, loss = 0.64686601\n",
      "Iteration 19, loss = 0.64239031\n",
      "Iteration 20, loss = 0.63415853\n",
      "Iteration 21, loss = 0.62829007\n",
      "Iteration 22, loss = 0.62135575\n",
      "Iteration 23, loss = 0.61721666\n",
      "Iteration 24, loss = 0.61087895\n",
      "Iteration 25, loss = 0.60397168\n",
      "Iteration 26, loss = 0.59955872\n",
      "Iteration 27, loss = 0.58969574\n",
      "Iteration 28, loss = 0.58687041\n",
      "Iteration 29, loss = 0.57793331\n",
      "Iteration 30, loss = 0.57304023\n",
      "Iteration 31, loss = 0.56891290\n",
      "Iteration 32, loss = 0.56335450\n",
      "Iteration 33, loss = 0.56003629\n",
      "Iteration 34, loss = 0.55839464\n",
      "Iteration 35, loss = 0.55714498\n",
      "Iteration 36, loss = 0.55681888\n",
      "Iteration 37, loss = 0.55253878\n",
      "Iteration 38, loss = 0.55113487\n",
      "Iteration 39, loss = 0.55114426\n",
      "Iteration 40, loss = 0.54787370\n",
      "Iteration 41, loss = 0.55148768\n",
      "Iteration 42, loss = 0.54820432\n",
      "Iteration 43, loss = 0.54656116\n",
      "Iteration 44, loss = 0.54681361\n",
      "Iteration 45, loss = 0.54608781\n",
      "Iteration 46, loss = 0.54730872\n",
      "Iteration 47, loss = 0.54648718\n",
      "Iteration 48, loss = 0.54514672\n",
      "Iteration 49, loss = 0.54657880\n",
      "Iteration 50, loss = 0.54521160\n",
      "Iteration 51, loss = 0.54516851\n",
      "Iteration 52, loss = 0.54414702\n",
      "Iteration 53, loss = 0.54513883\n",
      "Iteration 54, loss = 0.54567481\n",
      "Iteration 55, loss = 0.54484262\n",
      "Iteration 56, loss = 0.54434924\n",
      "Iteration 57, loss = 0.54560850\n",
      "Iteration 58, loss = 0.54451736\n",
      "Iteration 59, loss = 0.54480969\n",
      "Iteration 60, loss = 0.54541314\n",
      "Iteration 61, loss = 0.54517210\n",
      "Iteration 62, loss = 0.54541205\n",
      "Iteration 63, loss = 0.54556056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8\n",
      "0.8\n",
      "0.8000000000000002\n",
      "Iteration 1, loss = 0.43101450\n",
      "Iteration 2, loss = 0.14977597\n",
      "Iteration 3, loss = 0.08259152\n",
      "Iteration 4, loss = 0.04361157\n",
      "Iteration 5, loss = 0.02566700\n",
      "Iteration 6, loss = 0.01755034\n",
      "Iteration 7, loss = 0.01304243\n",
      "Iteration 8, loss = 0.01048024\n",
      "Iteration 9, loss = 0.00917085\n",
      "Iteration 10, loss = 0.00854795\n",
      "Iteration 11, loss = 0.00898215\n",
      "Iteration 12, loss = 0.01458290\n",
      "Iteration 13, loss = 0.01414321\n",
      "Iteration 14, loss = 0.01377824\n",
      "Iteration 15, loss = 0.01960217\n",
      "Iteration 16, loss = 0.02577607\n",
      "Iteration 17, loss = 0.03005534\n",
      "Iteration 18, loss = 0.04412294\n",
      "Iteration 19, loss = 0.10027415\n",
      "Iteration 20, loss = 0.15156454\n",
      "Iteration 21, loss = 0.13117247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8571428571428571\n",
      "0.84\n",
      "0.8484848484848485\n",
      "Iteration 1, loss = 0.64571639\n",
      "Iteration 2, loss = 0.23318035\n",
      "Iteration 3, loss = 0.11573758\n",
      "Iteration 4, loss = 0.09897594\n",
      "Iteration 5, loss = 0.14382518\n",
      "Iteration 6, loss = 0.15846062\n",
      "Iteration 7, loss = 0.16480918\n",
      "Iteration 8, loss = 0.16335561\n",
      "Iteration 9, loss = 0.15706117\n",
      "Iteration 10, loss = 0.17056430\n",
      "Iteration 11, loss = 0.17759869\n",
      "Iteration 12, loss = 0.24224542\n",
      "Iteration 13, loss = 0.27380723\n",
      "Iteration 14, loss = 0.22711331\n",
      "Iteration 15, loss = 0.18224196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8666666666666667\n",
      "0.78\n",
      "0.8210526315789474\n",
      "Iteration 1, loss = 0.97231389\n",
      "Iteration 2, loss = 0.42899081\n",
      "Iteration 3, loss = 0.35927090\n",
      "Iteration 4, loss = 0.37326827\n",
      "Iteration 5, loss = 0.43797740\n",
      "Iteration 6, loss = 0.48451031\n",
      "Iteration 7, loss = 0.52328587\n",
      "Iteration 8, loss = 0.53427570\n",
      "Iteration 9, loss = 0.52954568\n",
      "Iteration 10, loss = 0.54880741\n",
      "Iteration 11, loss = 0.46880734\n",
      "Iteration 12, loss = 0.50169269\n",
      "Iteration 13, loss = 0.51695004\n",
      "Iteration 14, loss = 0.50910945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7384615384615385\n",
      "0.96\n",
      "0.8347826086956522\n",
      "Iteration 1, loss = 3.71994871\n",
      "Iteration 2, loss = 1.11511173\n",
      "Iteration 3, loss = 0.64600744\n",
      "Iteration 4, loss = 0.58070037\n",
      "Iteration 5, loss = 0.63563198\n",
      "Iteration 6, loss = 0.69650308\n",
      "Iteration 7, loss = 0.75555499\n",
      "Iteration 8, loss = 0.83224929\n",
      "Iteration 9, loss = 0.82271095\n",
      "Iteration 10, loss = 0.77223254\n",
      "Iteration 11, loss = 0.76169058\n",
      "Iteration 12, loss = 0.76866882\n",
      "Iteration 13, loss = 0.82083439\n",
      "Iteration 14, loss = 0.83289225\n",
      "Iteration 15, loss = 0.84903656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9473684210526315\n",
      "0.72\n",
      "0.8181818181818181\n",
      "Iteration 1, loss = 39.71552778\n",
      "Iteration 2, loss = 6.86387882\n",
      "Iteration 3, loss = 1.95945956\n",
      "Iteration 4, loss = 0.90374068\n",
      "Iteration 5, loss = 0.71114839\n",
      "Iteration 6, loss = 0.67334255\n",
      "Iteration 7, loss = 0.66917897\n",
      "Iteration 8, loss = 0.66798559\n",
      "Iteration 9, loss = 0.67049611\n",
      "Iteration 10, loss = 0.67560153\n",
      "Iteration 11, loss = 0.68729071\n",
      "Iteration 12, loss = 0.70004152\n",
      "Iteration 13, loss = 0.72018147\n",
      "Iteration 14, loss = 0.76421864\n",
      "Iteration 15, loss = 0.81601492\n",
      "Iteration 16, loss = 0.84398920\n",
      "Iteration 17, loss = 0.88761656\n",
      "Iteration 18, loss = 0.90560977\n",
      "Iteration 19, loss = 0.93847166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.6447368421052632\n",
      "0.98\n",
      "0.7777777777777777\n",
      "0.9148936170212766\n",
      "0.86\n",
      "0.8865979381443299\n",
      "0.9148936170212766\n",
      "0.86\n",
      "0.8865979381443299\n",
      "0.9166666666666666\n",
      "0.88\n",
      "0.8979591836734694\n",
      "0.9361702127659575\n",
      "0.88\n",
      "0.9072164948453608\n",
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n",
      "Iteration 1, loss = 0.64916483\n",
      "Iteration 2, loss = 0.57890692\n",
      "Iteration 3, loss = 0.46859104\n",
      "Iteration 4, loss = 0.33360853\n",
      "Iteration 5, loss = 0.22993883\n",
      "Iteration 6, loss = 0.16924921\n",
      "Iteration 7, loss = 0.13220238\n",
      "Iteration 8, loss = 0.10661196\n",
      "Iteration 9, loss = 0.08817617\n",
      "Iteration 10, loss = 0.07455344\n",
      "Iteration 11, loss = 0.06317376\n",
      "Iteration 12, loss = 0.05519359\n",
      "Iteration 13, loss = 0.04785730\n",
      "Iteration 14, loss = 0.04167384\n",
      "Iteration 15, loss = 0.03688360\n",
      "Iteration 16, loss = 0.03302614\n",
      "Iteration 17, loss = 0.02968145\n",
      "Iteration 18, loss = 0.02697413\n",
      "Iteration 19, loss = 0.02431094\n",
      "Iteration 20, loss = 0.02210375\n",
      "Iteration 21, loss = 0.02028927\n",
      "Iteration 22, loss = 0.01854064\n",
      "Iteration 23, loss = 0.01717084\n",
      "Iteration 24, loss = 0.01610516\n",
      "Iteration 25, loss = 0.01482140\n",
      "Iteration 26, loss = 0.01382968\n",
      "Iteration 27, loss = 0.01297113\n",
      "Iteration 28, loss = 0.01219914\n",
      "Iteration 29, loss = 0.01150583\n",
      "Iteration 30, loss = 0.01087432\n",
      "Iteration 31, loss = 0.01028028\n",
      "Iteration 32, loss = 0.00976951\n",
      "Iteration 33, loss = 0.00924147\n",
      "Iteration 34, loss = 0.00885779\n",
      "Iteration 35, loss = 0.00841444\n",
      "Iteration 36, loss = 0.00803370\n",
      "Iteration 37, loss = 0.00767854\n",
      "Iteration 38, loss = 0.00737494\n",
      "Iteration 39, loss = 0.00708182\n",
      "Iteration 40, loss = 0.00682175\n",
      "Iteration 41, loss = 0.00655972\n",
      "Iteration 42, loss = 0.00631844\n",
      "Iteration 43, loss = 0.00608321\n",
      "Iteration 44, loss = 0.00587007\n",
      "Iteration 45, loss = 0.00566295\n",
      "Iteration 46, loss = 0.00548613\n",
      "Iteration 47, loss = 0.00531503\n",
      "Iteration 48, loss = 0.00514127\n",
      "Iteration 49, loss = 0.00498252\n",
      "Iteration 50, loss = 0.00483044\n",
      "Iteration 51, loss = 0.00469936\n",
      "Iteration 52, loss = 0.00456461\n",
      "Iteration 53, loss = 0.00444605\n",
      "Iteration 54, loss = 0.00431569\n",
      "Iteration 55, loss = 0.00420712\n",
      "Iteration 56, loss = 0.00409436\n",
      "Iteration 57, loss = 0.00399173\n",
      "Iteration 58, loss = 0.00388796\n",
      "Iteration 59, loss = 0.00379781\n",
      "Iteration 60, loss = 0.00370921\n",
      "Iteration 61, loss = 0.00361823\n",
      "Iteration 62, loss = 0.00354425\n",
      "Iteration 63, loss = 0.00346130\n",
      "Iteration 64, loss = 0.00338191\n",
      "Iteration 65, loss = 0.00332265\n",
      "Iteration 66, loss = 0.00324713\n",
      "Iteration 67, loss = 0.00317411\n",
      "Iteration 68, loss = 0.00311357\n",
      "Iteration 69, loss = 0.00304539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.64962536\n",
      "Iteration 2, loss = 0.57937941\n",
      "Iteration 3, loss = 0.46909801\n",
      "Iteration 4, loss = 0.33415781\n",
      "Iteration 5, loss = 0.23052100\n",
      "Iteration 6, loss = 0.16985838\n",
      "Iteration 7, loss = 0.13283584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.10726707\n",
      "Iteration 9, loss = 0.08885111\n",
      "Iteration 10, loss = 0.07524660\n",
      "Iteration 11, loss = 0.06388349\n",
      "Iteration 12, loss = 0.05591820\n",
      "Iteration 13, loss = 0.04859603\n",
      "Iteration 14, loss = 0.04242585\n",
      "Iteration 15, loss = 0.03764747\n",
      "Iteration 16, loss = 0.03380134\n",
      "Iteration 17, loss = 0.03046654\n",
      "Iteration 18, loss = 0.02776984\n",
      "Iteration 19, loss = 0.02511500\n",
      "Iteration 20, loss = 0.02291655\n",
      "Iteration 21, loss = 0.02111023\n",
      "Iteration 22, loss = 0.01936886\n",
      "Iteration 23, loss = 0.01800605\n",
      "Iteration 24, loss = 0.01694741\n",
      "Iteration 25, loss = 0.01567017\n",
      "Iteration 26, loss = 0.01468414\n",
      "Iteration 27, loss = 0.01383150\n",
      "Iteration 28, loss = 0.01306513\n",
      "Iteration 29, loss = 0.01237686\n",
      "Iteration 30, loss = 0.01175073\n",
      "Iteration 31, loss = 0.01116131\n",
      "Iteration 32, loss = 0.01065571\n",
      "Iteration 33, loss = 0.01013158\n",
      "Iteration 34, loss = 0.00975232\n",
      "Iteration 35, loss = 0.00931330\n",
      "Iteration 36, loss = 0.00893650\n",
      "Iteration 37, loss = 0.00858518\n",
      "Iteration 38, loss = 0.00828549\n",
      "Iteration 39, loss = 0.00799621\n",
      "Iteration 40, loss = 0.00773977\n",
      "Iteration 41, loss = 0.00748113\n",
      "Iteration 42, loss = 0.00724305\n",
      "Iteration 43, loss = 0.00701121\n",
      "Iteration 44, loss = 0.00680117\n",
      "Iteration 45, loss = 0.00659702\n",
      "Iteration 46, loss = 0.00642328\n",
      "Iteration 47, loss = 0.00625514\n",
      "Iteration 48, loss = 0.00608412\n",
      "Iteration 49, loss = 0.00592813\n",
      "Iteration 50, loss = 0.00577871\n",
      "Iteration 51, loss = 0.00565042\n",
      "Iteration 52, loss = 0.00551826\n",
      "Iteration 53, loss = 0.00540239\n",
      "Iteration 54, loss = 0.00527418\n",
      "Iteration 55, loss = 0.00516825\n",
      "Iteration 56, loss = 0.00505774\n",
      "Iteration 57, loss = 0.00495746\n",
      "Iteration 58, loss = 0.00485590\n",
      "Iteration 59, loss = 0.00476795\n",
      "Iteration 60, loss = 0.00468173\n",
      "Iteration 61, loss = 0.00459270\n",
      "Iteration 62, loss = 0.00452093\n",
      "Iteration 63, loss = 0.00443994\n",
      "Iteration 64, loss = 0.00436277\n",
      "Iteration 65, loss = 0.00430556\n",
      "Iteration 66, loss = 0.00423192\n",
      "Iteration 67, loss = 0.00416061\n",
      "Iteration 68, loss = 0.00410214\n",
      "Iteration 69, loss = 0.00403574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.65422902\n",
      "Iteration 2, loss = 0.58409682\n",
      "Iteration 3, loss = 0.47415282\n",
      "Iteration 4, loss = 0.33962791\n",
      "Iteration 5, loss = 0.23631211\n",
      "Iteration 6, loss = 0.17591128\n",
      "Iteration 7, loss = 0.13912328\n",
      "Iteration 8, loss = 0.11376236\n",
      "Iteration 9, loss = 0.09553614\n",
      "Iteration 10, loss = 0.08210492\n",
      "Iteration 11, loss = 0.07089864\n",
      "Iteration 12, loss = 0.06307315\n",
      "Iteration 13, loss = 0.05588302\n",
      "Iteration 14, loss = 0.04983656\n",
      "Iteration 15, loss = 0.04516747\n",
      "Iteration 16, loss = 0.04142523\n",
      "Iteration 17, loss = 0.03818005\n",
      "Iteration 18, loss = 0.03558012\n",
      "Iteration 19, loss = 0.03299940\n",
      "Iteration 20, loss = 0.03087880\n",
      "Iteration 21, loss = 0.02914445\n",
      "Iteration 22, loss = 0.02746609\n",
      "Iteration 23, loss = 0.02616359\n",
      "Iteration 24, loss = 0.02516561\n",
      "Iteration 25, loss = 0.02394437\n",
      "Iteration 26, loss = 0.02300522\n",
      "Iteration 27, loss = 0.02220193\n",
      "Iteration 28, loss = 0.02148215\n",
      "Iteration 29, loss = 0.02083447\n",
      "Iteration 30, loss = 0.02025229\n",
      "Iteration 31, loss = 0.01969920\n",
      "Iteration 32, loss = 0.01923570\n",
      "Iteration 33, loss = 0.01874086\n",
      "Iteration 34, loss = 0.01839560\n",
      "Iteration 35, loss = 0.01799027\n",
      "Iteration 36, loss = 0.01764293\n",
      "Iteration 37, loss = 0.01732025\n",
      "Iteration 38, loss = 0.01704972\n",
      "Iteration 39, loss = 0.01678905\n",
      "Iteration 40, loss = 0.01655905\n",
      "Iteration 41, loss = 0.01632421\n",
      "Iteration 42, loss = 0.01610800\n",
      "Iteration 43, loss = 0.01590051\n",
      "Iteration 44, loss = 0.01571141\n",
      "Iteration 45, loss = 0.01552701\n",
      "Iteration 46, loss = 0.01537420\n",
      "Iteration 47, loss = 0.01522569\n",
      "Iteration 48, loss = 0.01507207\n",
      "Iteration 49, loss = 0.01493386\n",
      "Iteration 50, loss = 0.01480094\n",
      "Iteration 51, loss = 0.01469067\n",
      "Iteration 52, loss = 0.01457443\n",
      "Iteration 53, loss = 0.01447572\n",
      "Iteration 54, loss = 0.01435890\n",
      "Iteration 55, loss = 0.01426951\n",
      "Iteration 56, loss = 0.01417162\n",
      "Iteration 57, loss = 0.01408484\n",
      "Iteration 58, loss = 0.01399547\n",
      "Iteration 59, loss = 0.01391953\n",
      "Iteration 60, loss = 0.01384738\n",
      "Iteration 61, loss = 0.01376772\n",
      "Iteration 62, loss = 0.01370832\n",
      "Iteration 63, loss = 0.01363682\n",
      "Iteration 64, loss = 0.01357237\n",
      "Iteration 65, loss = 0.01352558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.70010544\n",
      "Iteration 2, loss = 0.63052757\n",
      "Iteration 3, loss = 0.52323019\n",
      "Iteration 4, loss = 0.39210840\n",
      "Iteration 5, loss = 0.29124667\n",
      "Iteration 6, loss = 0.23269424\n",
      "Iteration 7, loss = 0.19747550\n",
      "Iteration 8, loss = 0.17341526\n",
      "Iteration 9, loss = 0.15630555\n",
      "Iteration 10, loss = 0.14382671\n",
      "Iteration 11, loss = 0.13341097\n",
      "Iteration 12, loss = 0.12620691\n",
      "Iteration 13, loss = 0.11956661\n",
      "Iteration 14, loss = 0.11398890\n",
      "Iteration 15, loss = 0.10964941\n",
      "Iteration 16, loss = 0.10619445\n",
      "Iteration 17, loss = 0.10309622\n",
      "Iteration 18, loss = 0.10072864\n",
      "Iteration 19, loss = 0.09816064\n",
      "Iteration 20, loss = 0.09609218\n",
      "Iteration 21, loss = 0.09436467\n",
      "Iteration 22, loss = 0.09260700\n",
      "Iteration 23, loss = 0.09121627\n",
      "Iteration 24, loss = 0.09012127\n",
      "Iteration 25, loss = 0.08882052\n",
      "Iteration 26, loss = 0.08766022\n",
      "Iteration 27, loss = 0.08669134\n",
      "Iteration 28, loss = 0.08580630\n",
      "Iteration 29, loss = 0.08491077\n",
      "Iteration 30, loss = 0.08413353\n",
      "Iteration 31, loss = 0.08331404\n",
      "Iteration 32, loss = 0.08267310\n",
      "Iteration 33, loss = 0.08187618\n",
      "Iteration 34, loss = 0.08124889\n",
      "Iteration 35, loss = 0.08060933\n",
      "Iteration 36, loss = 0.07998719\n",
      "Iteration 37, loss = 0.07938467\n",
      "Iteration 38, loss = 0.07884723\n",
      "Iteration 39, loss = 0.07834191\n",
      "Iteration 40, loss = 0.07784284\n",
      "Iteration 41, loss = 0.07730521\n",
      "Iteration 42, loss = 0.07676735\n",
      "Iteration 43, loss = 0.07630929\n",
      "Iteration 44, loss = 0.07582397\n",
      "Iteration 45, loss = 0.07534631\n",
      "Iteration 46, loss = 0.07491767\n",
      "Iteration 47, loss = 0.07448809\n",
      "Iteration 48, loss = 0.07404078\n",
      "Iteration 49, loss = 0.07361608\n",
      "Iteration 50, loss = 0.07318437\n",
      "Iteration 51, loss = 0.07281226\n",
      "Iteration 52, loss = 0.07242296\n",
      "Iteration 53, loss = 0.07207950\n",
      "Iteration 54, loss = 0.07163877\n",
      "Iteration 55, loss = 0.07129810\n",
      "Iteration 56, loss = 0.07093932\n",
      "Iteration 57, loss = 0.07058366\n",
      "Iteration 58, loss = 0.07022031\n",
      "Iteration 59, loss = 0.06987167\n",
      "Iteration 60, loss = 0.06957112\n",
      "Iteration 61, loss = 0.06921481\n",
      "Iteration 62, loss = 0.06891846\n",
      "Iteration 63, loss = 0.06858958\n",
      "Iteration 64, loss = 0.06829362\n",
      "Iteration 65, loss = 0.06803099\n",
      "Iteration 66, loss = 0.06771522\n",
      "Iteration 67, loss = 0.06735750\n",
      "Iteration 68, loss = 0.06710562\n",
      "Iteration 69, loss = 0.06679264\n",
      "Iteration 70, loss = 0.06651127\n",
      "Iteration 71, loss = 0.06624395\n",
      "Iteration 72, loss = 0.06596280\n",
      "Iteration 73, loss = 0.06571554\n",
      "Iteration 74, loss = 0.06546703\n",
      "Iteration 75, loss = 0.06519567\n",
      "Iteration 76, loss = 0.06494651\n",
      "Iteration 77, loss = 0.06469642\n",
      "Iteration 78, loss = 0.06443944\n",
      "Iteration 79, loss = 0.06419834\n",
      "Iteration 80, loss = 0.06398779\n",
      "Iteration 81, loss = 0.06374601\n",
      "Iteration 82, loss = 0.06351584\n",
      "Iteration 83, loss = 0.06329651\n",
      "Iteration 84, loss = 0.06305993\n",
      "Iteration 85, loss = 0.06284774\n",
      "Iteration 86, loss = 0.06263565\n",
      "Iteration 87, loss = 0.06243809\n",
      "Iteration 88, loss = 0.06222053\n",
      "Iteration 89, loss = 0.06200887\n",
      "Iteration 90, loss = 0.06181842\n",
      "Iteration 91, loss = 0.06163441\n",
      "Iteration 92, loss = 0.06145415\n",
      "Iteration 93, loss = 0.06122995\n",
      "Iteration 94, loss = 0.06106277\n",
      "Iteration 95, loss = 0.06087368\n",
      "Iteration 96, loss = 0.06068706\n",
      "Iteration 97, loss = 0.06051572\n",
      "Iteration 98, loss = 0.06033372\n",
      "Iteration 99, loss = 0.06017424\n",
      "Iteration 100, loss = 0.06000962\n",
      "0.8846153846153846\n",
      "0.92\n",
      "0.9019607843137256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14318850\n",
      "Iteration 2, loss = 1.02549390\n",
      "Iteration 3, loss = 0.88523634\n",
      "Iteration 4, loss = 0.73535552\n",
      "Iteration 5, loss = 0.61383281\n",
      "Iteration 6, loss = 0.53417311\n",
      "Iteration 7, loss = 0.48039142\n",
      "Iteration 8, loss = 0.44008287\n",
      "Iteration 9, loss = 0.40902912\n",
      "Iteration 10, loss = 0.38470454\n",
      "Iteration 11, loss = 0.36415810\n",
      "Iteration 12, loss = 0.34798514\n",
      "Iteration 13, loss = 0.33413936\n",
      "Iteration 14, loss = 0.32258297\n",
      "Iteration 15, loss = 0.31299003\n",
      "Iteration 16, loss = 0.30537474\n",
      "Iteration 17, loss = 0.29847108\n",
      "Iteration 18, loss = 0.29371489\n",
      "Iteration 19, loss = 0.28844871\n",
      "Iteration 20, loss = 0.28451363\n",
      "Iteration 21, loss = 0.28153506\n",
      "Iteration 22, loss = 0.27820447\n",
      "Iteration 23, loss = 0.27608376\n",
      "Iteration 24, loss = 0.27420391\n",
      "Iteration 25, loss = 0.27300261\n",
      "Iteration 26, loss = 0.27093073\n",
      "Iteration 27, loss = 0.26979986\n",
      "Iteration 28, loss = 0.26949914\n",
      "Iteration 29, loss = 0.26807151\n",
      "Iteration 30, loss = 0.26748041\n",
      "Iteration 31, loss = 0.26668866\n",
      "Iteration 32, loss = 0.26618855\n",
      "Iteration 33, loss = 0.26583610\n",
      "Iteration 34, loss = 0.26534866\n",
      "Iteration 35, loss = 0.26516088\n",
      "Iteration 36, loss = 0.26517318\n",
      "Iteration 37, loss = 0.26465757\n",
      "Iteration 38, loss = 0.26456389\n",
      "Iteration 39, loss = 0.26454478\n",
      "Iteration 40, loss = 0.26436568\n",
      "Iteration 41, loss = 0.26474606\n",
      "Iteration 42, loss = 0.26416048\n",
      "Iteration 43, loss = 0.26409127\n",
      "Iteration 44, loss = 0.26427576\n",
      "Iteration 45, loss = 0.26391172\n",
      "Iteration 46, loss = 0.26415919\n",
      "Iteration 47, loss = 0.26394852\n",
      "Iteration 48, loss = 0.26375316\n",
      "Iteration 49, loss = 0.26391556\n",
      "Iteration 50, loss = 0.26350263\n",
      "Iteration 51, loss = 0.26368261\n",
      "Iteration 52, loss = 0.26342617\n",
      "Iteration 53, loss = 0.26359735\n",
      "Iteration 54, loss = 0.26383760\n",
      "Iteration 55, loss = 0.26354642\n",
      "Iteration 56, loss = 0.26369132\n",
      "Iteration 57, loss = 0.26422246\n",
      "Iteration 58, loss = 0.26361104\n",
      "Iteration 59, loss = 0.26358833\n",
      "Iteration 60, loss = 0.26377271\n",
      "Iteration 61, loss = 0.26350964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8627450980392157\n",
      "0.88\n",
      "0.8712871287128714\n",
      "Iteration 1, loss = 0.23299787\n",
      "Iteration 2, loss = 0.17403780\n",
      "Iteration 3, loss = 0.13869148\n",
      "Iteration 4, loss = 0.09375584\n",
      "Iteration 5, loss = 0.06240283\n",
      "Iteration 6, loss = 0.04308878\n",
      "Iteration 7, loss = 0.03121307\n",
      "Iteration 8, loss = 0.02378400\n",
      "Iteration 9, loss = 0.01815875\n",
      "Iteration 10, loss = 0.01417094\n",
      "Iteration 11, loss = 0.01233265\n",
      "Iteration 12, loss = 0.01335816\n",
      "Iteration 13, loss = 0.01030742\n",
      "Iteration 14, loss = 0.00761407\n",
      "Iteration 15, loss = 0.00599633\n",
      "Iteration 16, loss = 0.00625002\n",
      "Iteration 17, loss = 0.00578644\n",
      "Iteration 18, loss = 0.00408923\n",
      "Iteration 19, loss = 0.00337046\n",
      "Iteration 20, loss = 0.00338203\n",
      "Iteration 21, loss = 0.01160339\n",
      "Iteration 22, loss = 0.08015736\n",
      "Iteration 23, loss = 0.25444214\n",
      "Iteration 24, loss = 0.29585127\n",
      "Iteration 25, loss = 0.26341574\n",
      "Iteration 26, loss = 0.22001224\n",
      "Iteration 27, loss = 0.17995405\n",
      "Iteration 28, loss = 0.15461699\n",
      "Iteration 29, loss = 0.12842983\n",
      "Iteration 30, loss = 0.10463626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8214285714285714\n",
      "0.92\n",
      "0.8679245283018867\n",
      "Iteration 1, loss = 0.61330334\n",
      "Iteration 2, loss = 0.40478201\n",
      "Iteration 3, loss = 0.16459842\n",
      "Iteration 4, loss = 0.08131314\n",
      "Iteration 5, loss = 0.10406580\n",
      "Iteration 6, loss = 0.16329620\n",
      "Iteration 7, loss = 0.23192838\n",
      "Iteration 8, loss = 0.21065778\n",
      "Iteration 9, loss = 0.16343109\n",
      "Iteration 10, loss = 0.12859826\n",
      "Iteration 11, loss = 0.10590274\n",
      "Iteration 12, loss = 0.09683914\n",
      "Iteration 13, loss = 0.07682301\n",
      "Iteration 14, loss = 0.06878154\n",
      "Iteration 15, loss = 0.12875949\n",
      "Iteration 16, loss = 0.15961033\n",
      "Iteration 17, loss = 0.13778821\n",
      "Iteration 18, loss = 0.10116111\n",
      "Iteration 19, loss = 0.08777024\n",
      "Iteration 20, loss = 0.10069187\n",
      "Iteration 21, loss = 0.08519811\n",
      "Iteration 22, loss = 0.08913759\n",
      "Iteration 23, loss = 0.08074647\n",
      "Iteration 24, loss = 0.11818806\n",
      "Iteration 25, loss = 0.13187578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7538461538461538\n",
      "0.98\n",
      "0.8521739130434782\n",
      "Iteration 1, loss = 1.15447297\n",
      "Iteration 2, loss = 0.58807560\n",
      "Iteration 3, loss = 0.45464345\n",
      "Iteration 4, loss = 0.39878193\n",
      "Iteration 5, loss = 0.50272651\n",
      "Iteration 6, loss = 0.39658308\n",
      "Iteration 7, loss = 0.30405271\n",
      "Iteration 8, loss = 0.25379196\n",
      "Iteration 9, loss = 0.20529284\n",
      "Iteration 10, loss = 0.20948273\n",
      "Iteration 11, loss = 0.18440489\n",
      "Iteration 12, loss = 0.14022500\n",
      "Iteration 13, loss = 0.11990840\n",
      "Iteration 14, loss = 0.11915502\n",
      "Iteration 15, loss = 0.12871934\n",
      "Iteration 16, loss = 0.12121667\n",
      "Iteration 17, loss = 0.10613619\n",
      "Iteration 18, loss = 0.08379170\n",
      "Iteration 19, loss = 0.07097933\n",
      "Iteration 20, loss = 0.08284503\n",
      "Iteration 21, loss = 0.10506344\n",
      "Iteration 22, loss = 0.10008265\n",
      "Iteration 23, loss = 0.09139586\n",
      "Iteration 24, loss = 0.09626146\n",
      "Iteration 25, loss = 0.06763028\n",
      "Iteration 26, loss = 0.04355954\n",
      "Iteration 27, loss = 0.04200596\n",
      "Iteration 28, loss = 0.04534131\n",
      "Iteration 29, loss = 0.05256889\n",
      "Iteration 30, loss = 0.05427692\n",
      "Iteration 31, loss = 0.04779465\n",
      "Iteration 32, loss = 0.04707996\n",
      "Iteration 33, loss = 0.08287873\n",
      "Iteration 34, loss = 0.11445701\n",
      "Iteration 35, loss = 0.11548392\n",
      "Iteration 36, loss = 0.12885161\n",
      "Iteration 37, loss = 0.09376221\n",
      "Iteration 38, loss = 0.07773606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.875\n",
      "0.84\n",
      "0.8571428571428572\n",
      "Iteration 1, loss = 4.00609114\n",
      "Iteration 2, loss = 1.86080909\n",
      "Iteration 3, loss = 1.36237781\n",
      "Iteration 4, loss = 0.91186773\n",
      "Iteration 5, loss = 0.87480727\n",
      "Iteration 6, loss = 1.03824533\n",
      "Iteration 7, loss = 0.61577022\n",
      "Iteration 8, loss = 0.47622111\n",
      "Iteration 9, loss = 0.44020066\n",
      "Iteration 10, loss = 0.40496393\n",
      "Iteration 11, loss = 0.37007978\n",
      "Iteration 12, loss = 0.34340811\n",
      "Iteration 13, loss = 0.26969309\n",
      "Iteration 14, loss = 0.26848374\n",
      "Iteration 15, loss = 0.25828319\n",
      "Iteration 16, loss = 0.26642127\n",
      "Iteration 17, loss = 0.24436845\n",
      "Iteration 18, loss = 0.23492667\n",
      "Iteration 19, loss = 0.25251591\n",
      "Iteration 20, loss = 0.26069766\n",
      "Iteration 21, loss = 0.26075822\n",
      "Iteration 22, loss = 0.22758665\n",
      "Iteration 23, loss = 0.24059509\n",
      "Iteration 24, loss = 0.20832319\n",
      "Iteration 25, loss = 0.20911486\n",
      "Iteration 26, loss = 0.24147308\n",
      "Iteration 27, loss = 0.23151976\n",
      "Iteration 28, loss = 0.23473818\n",
      "Iteration 29, loss = 0.25232670\n",
      "Iteration 30, loss = 0.22131364\n",
      "Iteration 31, loss = 0.22373554\n",
      "Iteration 32, loss = 0.20518932\n",
      "Iteration 33, loss = 0.18165835\n",
      "Iteration 34, loss = 0.21015415\n",
      "Iteration 35, loss = 0.18223424\n",
      "Iteration 36, loss = 0.20750746\n",
      "Iteration 37, loss = 0.25104248\n",
      "Iteration 38, loss = 0.22136021\n",
      "Iteration 39, loss = 0.20630359\n",
      "Iteration 40, loss = 0.21311486\n",
      "Iteration 41, loss = 0.19395045\n",
      "Iteration 42, loss = 0.19468193\n",
      "Iteration 43, loss = 0.20866790\n",
      "Iteration 44, loss = 0.25133907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7796610169491526\n",
      "0.92\n",
      "0.8440366972477064\n",
      "Iteration 1, loss = 39.22278702\n",
      "Iteration 2, loss = 6.93514202\n",
      "Iteration 3, loss = 2.30095488\n",
      "Iteration 4, loss = 1.11376265\n",
      "Iteration 5, loss = 0.91392081\n",
      "Iteration 6, loss = 0.88848085\n",
      "Iteration 7, loss = 0.86491185\n",
      "Iteration 8, loss = 0.89460289\n",
      "Iteration 9, loss = 0.85745015\n",
      "Iteration 10, loss = 0.80490416\n",
      "Iteration 11, loss = 0.77202418\n",
      "Iteration 12, loss = 0.78187145\n",
      "Iteration 13, loss = 0.78785954\n",
      "Iteration 14, loss = 0.76961747\n",
      "Iteration 15, loss = 0.73843945\n",
      "Iteration 16, loss = 0.80979203\n",
      "Iteration 17, loss = 0.76655106\n",
      "Iteration 18, loss = 0.72155354\n",
      "Iteration 19, loss = 0.75117544\n",
      "Iteration 20, loss = 0.73280162\n",
      "Iteration 21, loss = 0.72734576\n",
      "Iteration 22, loss = 0.74667412\n",
      "Iteration 23, loss = 0.77018224\n",
      "Iteration 24, loss = 0.74400276\n",
      "Iteration 25, loss = 0.79588336\n",
      "Iteration 26, loss = 0.74439228\n",
      "Iteration 27, loss = 0.72429880\n",
      "Iteration 28, loss = 0.73238471\n",
      "Iteration 29, loss = 0.79542760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7014925373134329\n",
      "0.94\n",
      "0.8034188034188033\n",
      "0.8979591836734694\n",
      "0.88\n",
      "0.888888888888889\n",
      "0.8979591836734694\n",
      "0.88\n",
      "0.888888888888889\n",
      "0.9166666666666666\n",
      "0.88\n",
      "0.8979591836734694\n",
      "0.8679245283018868\n",
      "0.92\n",
      "0.8932038834951457\n",
      "0.8679245283018868\n",
      "0.92\n",
      "0.8932038834951457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65979674\n",
      "Iteration 2, loss = 0.62127767\n",
      "Iteration 3, loss = 0.55621997\n",
      "Iteration 4, loss = 0.44390416\n",
      "Iteration 5, loss = 0.31294343\n",
      "Iteration 6, loss = 0.21727634\n",
      "Iteration 7, loss = 0.16054652\n",
      "Iteration 8, loss = 0.12491550\n",
      "Iteration 9, loss = 0.10103546\n",
      "Iteration 10, loss = 0.08408106\n",
      "Iteration 11, loss = 0.07046553\n",
      "Iteration 12, loss = 0.06089496\n",
      "Iteration 13, loss = 0.05239448\n",
      "Iteration 14, loss = 0.04535601\n",
      "Iteration 15, loss = 0.03988692\n",
      "Iteration 16, loss = 0.03553368\n",
      "Iteration 17, loss = 0.03177358\n",
      "Iteration 18, loss = 0.02879665\n",
      "Iteration 19, loss = 0.02581877\n",
      "Iteration 20, loss = 0.02340205\n",
      "Iteration 21, loss = 0.02141179\n",
      "Iteration 22, loss = 0.01950007\n",
      "Iteration 23, loss = 0.01800273\n",
      "Iteration 24, loss = 0.01684625\n",
      "Iteration 25, loss = 0.01547657\n",
      "Iteration 26, loss = 0.01439900\n",
      "Iteration 27, loss = 0.01347822\n",
      "Iteration 28, loss = 0.01265576\n",
      "Iteration 29, loss = 0.01191274\n",
      "Iteration 30, loss = 0.01124418\n",
      "Iteration 31, loss = 0.01060911\n",
      "Iteration 32, loss = 0.01007469\n",
      "Iteration 33, loss = 0.00951200\n",
      "Iteration 34, loss = 0.00910605\n",
      "Iteration 35, loss = 0.00864229\n",
      "Iteration 36, loss = 0.00824144\n",
      "Iteration 37, loss = 0.00786873\n",
      "Iteration 38, loss = 0.00755113\n",
      "Iteration 39, loss = 0.00724541\n",
      "Iteration 40, loss = 0.00697385\n",
      "Iteration 41, loss = 0.00669978\n",
      "Iteration 42, loss = 0.00644716\n",
      "Iteration 43, loss = 0.00620426\n",
      "Iteration 44, loss = 0.00598282\n",
      "Iteration 45, loss = 0.00576732\n",
      "Iteration 46, loss = 0.00558450\n",
      "Iteration 47, loss = 0.00540731\n",
      "Iteration 48, loss = 0.00522716\n",
      "Iteration 49, loss = 0.00506306\n",
      "Iteration 50, loss = 0.00490601\n",
      "Iteration 51, loss = 0.00477097\n",
      "Iteration 52, loss = 0.00463224\n",
      "Iteration 53, loss = 0.00451018\n",
      "Iteration 54, loss = 0.00437496\n",
      "Iteration 55, loss = 0.00426431\n",
      "Iteration 56, loss = 0.00414777\n",
      "Iteration 57, loss = 0.00404256\n",
      "Iteration 58, loss = 0.00393598\n",
      "Iteration 59, loss = 0.00384308\n",
      "Iteration 60, loss = 0.00375292\n",
      "Iteration 61, loss = 0.00365921\n",
      "Iteration 62, loss = 0.00358367\n",
      "Iteration 63, loss = 0.00349833\n",
      "Iteration 64, loss = 0.00341805\n",
      "Iteration 65, loss = 0.00335678\n",
      "Iteration 66, loss = 0.00327995\n",
      "Iteration 67, loss = 0.00320477\n",
      "Iteration 68, loss = 0.00314315\n",
      "Iteration 69, loss = 0.00307372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.66025678\n",
      "Iteration 2, loss = 0.62174338\n",
      "Iteration 3, loss = 0.55670691\n",
      "Iteration 4, loss = 0.44443183\n",
      "Iteration 5, loss = 0.31351272\n",
      "Iteration 6, loss = 0.21787349\n",
      "Iteration 7, loss = 0.16116743\n",
      "Iteration 8, loss = 0.12555863\n",
      "Iteration 9, loss = 0.10169938\n",
      "Iteration 10, loss = 0.08476410\n",
      "Iteration 11, loss = 0.07116608\n",
      "Iteration 12, loss = 0.06161115\n",
      "Iteration 13, loss = 0.05312563\n",
      "Iteration 14, loss = 0.04610112\n",
      "Iteration 15, loss = 0.04064446\n",
      "Iteration 16, loss = 0.03630310\n",
      "Iteration 17, loss = 0.03255335\n",
      "Iteration 18, loss = 0.02958756\n",
      "Iteration 19, loss = 0.02661842\n",
      "Iteration 20, loss = 0.02421076\n",
      "Iteration 21, loss = 0.02222899\n",
      "Iteration 22, loss = 0.02032476\n",
      "Iteration 23, loss = 0.01883468\n",
      "Iteration 24, loss = 0.01768540\n",
      "Iteration 25, loss = 0.01632265\n",
      "Iteration 26, loss = 0.01525085\n",
      "Iteration 27, loss = 0.01433612\n",
      "Iteration 28, loss = 0.01351949\n",
      "Iteration 29, loss = 0.01278164\n",
      "Iteration 30, loss = 0.01211856\n",
      "Iteration 31, loss = 0.01148822\n",
      "Iteration 32, loss = 0.01095916\n",
      "Iteration 33, loss = 0.01040046\n",
      "Iteration 34, loss = 0.00999897\n",
      "Iteration 35, loss = 0.00953968\n",
      "Iteration 36, loss = 0.00914285\n",
      "Iteration 37, loss = 0.00877407\n",
      "Iteration 38, loss = 0.00846045\n",
      "Iteration 39, loss = 0.00815866\n",
      "Iteration 40, loss = 0.00789080\n",
      "Iteration 41, loss = 0.00762016\n",
      "Iteration 42, loss = 0.00737077\n",
      "Iteration 43, loss = 0.00713136\n",
      "Iteration 44, loss = 0.00691305\n",
      "Iteration 45, loss = 0.00670057\n",
      "Iteration 46, loss = 0.00652089\n",
      "Iteration 47, loss = 0.00634670\n",
      "Iteration 48, loss = 0.00616933\n",
      "Iteration 49, loss = 0.00600803\n",
      "Iteration 50, loss = 0.00585367\n",
      "Iteration 51, loss = 0.00572146\n",
      "Iteration 52, loss = 0.00558536\n",
      "Iteration 53, loss = 0.00546603\n",
      "Iteration 54, loss = 0.00533297\n",
      "Iteration 55, loss = 0.00522501\n",
      "Iteration 56, loss = 0.00511074\n",
      "Iteration 57, loss = 0.00500791\n",
      "Iteration 58, loss = 0.00490356\n",
      "Iteration 59, loss = 0.00481289\n",
      "Iteration 60, loss = 0.00472514\n",
      "Iteration 61, loss = 0.00463339\n",
      "Iteration 62, loss = 0.00456009\n",
      "Iteration 63, loss = 0.00447672\n",
      "Iteration 64, loss = 0.00439871\n",
      "Iteration 65, loss = 0.00433949\n",
      "Iteration 66, loss = 0.00426455\n",
      "Iteration 67, loss = 0.00419110\n",
      "Iteration 68, loss = 0.00413158\n",
      "Iteration 69, loss = 0.00406395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.66485557\n",
      "Iteration 2, loss = 0.62639305\n",
      "Iteration 3, loss = 0.56155306\n",
      "Iteration 4, loss = 0.44966726\n",
      "Iteration 5, loss = 0.31915227\n",
      "Iteration 6, loss = 0.22379455\n",
      "Iteration 7, loss = 0.16732241\n",
      "Iteration 8, loss = 0.13193170\n",
      "Iteration 9, loss = 0.10827877\n",
      "Iteration 10, loss = 0.09153448\n",
      "Iteration 11, loss = 0.07810852\n",
      "Iteration 12, loss = 0.06870247\n",
      "Iteration 13, loss = 0.06035755\n",
      "Iteration 14, loss = 0.05346181\n",
      "Iteration 15, loss = 0.04811848\n",
      "Iteration 16, loss = 0.04388513\n",
      "Iteration 17, loss = 0.04022765\n",
      "Iteration 18, loss = 0.03736193\n",
      "Iteration 19, loss = 0.03446956\n",
      "Iteration 20, loss = 0.03214186\n",
      "Iteration 21, loss = 0.03023401\n",
      "Iteration 22, loss = 0.02839390\n",
      "Iteration 23, loss = 0.02696597\n",
      "Iteration 24, loss = 0.02587821\n",
      "Iteration 25, loss = 0.02457485\n",
      "Iteration 26, loss = 0.02355007\n",
      "Iteration 27, loss = 0.02268590\n",
      "Iteration 28, loss = 0.02191759\n",
      "Iteration 29, loss = 0.02122115\n",
      "Iteration 30, loss = 0.02060291\n",
      "Iteration 31, loss = 0.02000972\n",
      "Iteration 32, loss = 0.01952438\n",
      "Iteration 33, loss = 0.01899572\n",
      "Iteration 34, loss = 0.01862837\n",
      "Iteration 35, loss = 0.01820410\n",
      "Iteration 36, loss = 0.01783740\n",
      "Iteration 37, loss = 0.01749809\n",
      "Iteration 38, loss = 0.01721435\n",
      "Iteration 39, loss = 0.01694192\n",
      "Iteration 40, loss = 0.01670119\n",
      "Iteration 41, loss = 0.01645476\n",
      "Iteration 42, loss = 0.01622743\n",
      "Iteration 43, loss = 0.01601323\n",
      "Iteration 44, loss = 0.01581627\n",
      "Iteration 45, loss = 0.01562404\n",
      "Iteration 46, loss = 0.01546573\n",
      "Iteration 47, loss = 0.01531159\n",
      "Iteration 48, loss = 0.01515198\n",
      "Iteration 49, loss = 0.01500880\n",
      "Iteration 50, loss = 0.01487119\n",
      "Iteration 51, loss = 0.01475744\n",
      "Iteration 52, loss = 0.01463765\n",
      "Iteration 53, loss = 0.01453590\n",
      "Iteration 54, loss = 0.01441425\n",
      "Iteration 55, loss = 0.01432323\n",
      "Iteration 56, loss = 0.01422187\n",
      "Iteration 57, loss = 0.01413269\n",
      "Iteration 58, loss = 0.01404082\n",
      "Iteration 59, loss = 0.01396230\n",
      "Iteration 60, loss = 0.01388899\n",
      "Iteration 61, loss = 0.01380673\n",
      "Iteration 62, loss = 0.01374608\n",
      "Iteration 63, loss = 0.01367228\n",
      "Iteration 64, loss = 0.01360741\n",
      "Iteration 65, loss = 0.01355870\n",
      "Iteration 66, loss = 0.01349253\n",
      "Iteration 67, loss = 0.01342625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "Iteration 1, loss = 0.71068413\n",
      "Iteration 2, loss = 0.67216437\n",
      "Iteration 3, loss = 0.60871775\n",
      "Iteration 4, loss = 0.50016589\n",
      "Iteration 5, loss = 0.37292886\n",
      "Iteration 6, loss = 0.27954193\n",
      "Iteration 7, loss = 0.22454031\n",
      "Iteration 8, loss = 0.19049952\n",
      "Iteration 9, loss = 0.16805106\n",
      "Iteration 10, loss = 0.15235156\n",
      "Iteration 11, loss = 0.13980944\n",
      "Iteration 12, loss = 0.13110131\n",
      "Iteration 13, loss = 0.12338985\n",
      "Iteration 14, loss = 0.11702833\n",
      "Iteration 15, loss = 0.11207223\n",
      "Iteration 16, loss = 0.10818202\n",
      "Iteration 17, loss = 0.10471678\n",
      "Iteration 18, loss = 0.10213161\n",
      "Iteration 19, loss = 0.09929425\n",
      "Iteration 20, loss = 0.09705092\n",
      "Iteration 21, loss = 0.09518292\n",
      "Iteration 22, loss = 0.09328711\n",
      "Iteration 23, loss = 0.09179889\n",
      "Iteration 24, loss = 0.09062925\n",
      "Iteration 25, loss = 0.08928140\n",
      "Iteration 26, loss = 0.08804443\n",
      "Iteration 27, loss = 0.08702961\n",
      "Iteration 28, loss = 0.08611558\n",
      "Iteration 29, loss = 0.08518132\n",
      "Iteration 30, loss = 0.08437648\n",
      "Iteration 31, loss = 0.08352547\n",
      "Iteration 32, loss = 0.08287397\n",
      "Iteration 33, loss = 0.08205483\n",
      "Iteration 34, loss = 0.08140909\n",
      "Iteration 35, loss = 0.08075793\n",
      "Iteration 36, loss = 0.08012494\n",
      "Iteration 37, loss = 0.07951074\n",
      "Iteration 38, loss = 0.07896474\n",
      "Iteration 39, loss = 0.07845442\n",
      "Iteration 40, loss = 0.07795026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41, loss = 0.07740381\n",
      "Iteration 42, loss = 0.07685558\n",
      "Iteration 43, loss = 0.07639387\n",
      "Iteration 44, loss = 0.07590525\n",
      "Iteration 45, loss = 0.07542196\n",
      "Iteration 46, loss = 0.07499064\n",
      "Iteration 47, loss = 0.07455682\n",
      "Iteration 48, loss = 0.07410629\n",
      "Iteration 49, loss = 0.07367772\n",
      "Iteration 50, loss = 0.07324182\n",
      "Iteration 51, loss = 0.07286713\n",
      "Iteration 52, loss = 0.07247594\n",
      "Iteration 53, loss = 0.07213140\n",
      "Iteration 54, loss = 0.07168625\n",
      "Iteration 55, loss = 0.07134397\n",
      "Iteration 56, loss = 0.07098388\n",
      "Iteration 57, loss = 0.07062637\n",
      "Iteration 58, loss = 0.07026082\n",
      "Iteration 59, loss = 0.06990932\n",
      "Iteration 60, loss = 0.06960789\n",
      "Iteration 61, loss = 0.06924986\n",
      "Iteration 62, loss = 0.06895265\n",
      "Iteration 63, loss = 0.06862234\n",
      "Iteration 64, loss = 0.06832466\n",
      "Iteration 65, loss = 0.06805955\n",
      "Iteration 66, loss = 0.06774646\n",
      "Iteration 67, loss = 0.06738362\n",
      "Iteration 68, loss = 0.06713127\n",
      "Iteration 69, loss = 0.06681555\n",
      "Iteration 70, loss = 0.06653388\n",
      "Iteration 71, loss = 0.06626594\n",
      "Iteration 72, loss = 0.06598256\n",
      "Iteration 73, loss = 0.06573523\n",
      "Iteration 74, loss = 0.06548648\n",
      "Iteration 75, loss = 0.06521235\n",
      "Iteration 76, loss = 0.06496358\n",
      "Iteration 77, loss = 0.06471183\n",
      "Iteration 78, loss = 0.06445370\n",
      "Iteration 79, loss = 0.06421039\n",
      "Iteration 80, loss = 0.06399915\n",
      "Iteration 81, loss = 0.06375688\n",
      "Iteration 82, loss = 0.06352505\n",
      "Iteration 83, loss = 0.06330680\n",
      "Iteration 84, loss = 0.06306807\n",
      "Iteration 85, loss = 0.06285508\n",
      "Iteration 86, loss = 0.06264113\n",
      "Iteration 87, loss = 0.06244440\n",
      "Iteration 88, loss = 0.06222569\n",
      "Iteration 89, loss = 0.06201296\n",
      "Iteration 90, loss = 0.06182169\n",
      "Iteration 91, loss = 0.06163732\n",
      "Iteration 92, loss = 0.06145559\n",
      "Iteration 93, loss = 0.06123132\n",
      "Iteration 94, loss = 0.06106306\n",
      "Iteration 95, loss = 0.06087349\n",
      "Iteration 96, loss = 0.06068557\n",
      "Iteration 97, loss = 0.06051345\n",
      "Iteration 98, loss = 0.06033129\n",
      "Iteration 99, loss = 0.06017161\n",
      "Iteration 100, loss = 0.06000502\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15329159\n",
      "Iteration 2, loss = 1.06093504\n",
      "Iteration 3, loss = 0.95259391\n",
      "Iteration 4, loss = 0.82302767\n",
      "Iteration 5, loss = 0.68471248\n",
      "Iteration 6, loss = 0.57352344\n",
      "Iteration 7, loss = 0.49988020\n",
      "Iteration 8, loss = 0.45033503\n",
      "Iteration 9, loss = 0.41505552\n",
      "Iteration 10, loss = 0.38852220\n",
      "Iteration 11, loss = 0.36671587\n",
      "Iteration 12, loss = 0.34971330\n",
      "Iteration 13, loss = 0.33534973\n",
      "Iteration 14, loss = 0.32344824\n",
      "Iteration 15, loss = 0.31361627\n",
      "Iteration 16, loss = 0.30583059\n",
      "Iteration 17, loss = 0.29880346\n",
      "Iteration 18, loss = 0.29396611\n",
      "Iteration 19, loss = 0.28861928\n",
      "Iteration 20, loss = 0.28463802\n",
      "Iteration 21, loss = 0.28163557\n",
      "Iteration 22, loss = 0.27823110\n",
      "Iteration 23, loss = 0.27609359\n",
      "Iteration 24, loss = 0.27418330\n",
      "Iteration 25, loss = 0.27296586\n",
      "Iteration 26, loss = 0.27089127\n",
      "Iteration 27, loss = 0.26972607\n",
      "Iteration 28, loss = 0.26941707\n",
      "Iteration 29, loss = 0.26800187\n",
      "Iteration 30, loss = 0.26739827\n",
      "Iteration 31, loss = 0.26658560\n",
      "Iteration 32, loss = 0.26606103\n",
      "Iteration 33, loss = 0.26572711\n",
      "Iteration 34, loss = 0.26524445\n",
      "Iteration 35, loss = 0.26505640\n",
      "Iteration 36, loss = 0.26508919\n",
      "Iteration 37, loss = 0.26453605\n",
      "Iteration 38, loss = 0.26446678\n",
      "Iteration 39, loss = 0.26440498\n",
      "Iteration 40, loss = 0.26423146\n",
      "Iteration 41, loss = 0.26466424\n",
      "Iteration 42, loss = 0.26404342\n",
      "Iteration 43, loss = 0.26395383\n",
      "Iteration 44, loss = 0.26417250\n",
      "Iteration 45, loss = 0.26378860\n",
      "Iteration 46, loss = 0.26405269\n",
      "Iteration 47, loss = 0.26381970\n",
      "Iteration 48, loss = 0.26363127\n",
      "Iteration 49, loss = 0.26380909\n",
      "Iteration 50, loss = 0.26336955\n",
      "Iteration 51, loss = 0.26355206\n",
      "Iteration 52, loss = 0.26328512\n",
      "Iteration 53, loss = 0.26346562\n",
      "Iteration 54, loss = 0.26372838\n",
      "Iteration 55, loss = 0.26340733\n",
      "Iteration 56, loss = 0.26352190\n",
      "Iteration 57, loss = 0.26413809\n",
      "Iteration 58, loss = 0.26348013\n",
      "Iteration 59, loss = 0.26346764\n",
      "Iteration 60, loss = 0.26363515\n",
      "Iteration 61, loss = 0.26337616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8627450980392157\n",
      "0.88\n",
      "0.8712871287128714\n",
      "Iteration 1, loss = 0.20152015\n",
      "Iteration 2, loss = 0.12212634\n",
      "Iteration 3, loss = 0.09950614\n",
      "Iteration 4, loss = 0.06942643\n",
      "Iteration 5, loss = 0.04880735\n",
      "Iteration 6, loss = 0.03539284\n",
      "Iteration 7, loss = 0.02655095\n",
      "Iteration 8, loss = 0.02062360\n",
      "Iteration 9, loss = 0.01637959\n",
      "Iteration 10, loss = 0.01310317\n",
      "Iteration 11, loss = 0.01074787\n",
      "Iteration 12, loss = 0.00892879\n",
      "Iteration 13, loss = 0.00768372\n",
      "Iteration 14, loss = 0.00629267\n",
      "Iteration 15, loss = 0.00547791\n",
      "Iteration 16, loss = 0.00479679\n",
      "Iteration 17, loss = 0.00452462\n",
      "Iteration 18, loss = 0.01313955\n",
      "Iteration 19, loss = 0.03874984\n",
      "Iteration 20, loss = 0.06637697\n",
      "Iteration 21, loss = 0.07853266\n",
      "Iteration 22, loss = 0.08164308\n",
      "Iteration 23, loss = 0.07368095\n",
      "Iteration 24, loss = 0.06401748\n",
      "Iteration 25, loss = 0.05328829\n",
      "Iteration 26, loss = 0.04357555\n",
      "Iteration 27, loss = 0.03630989\n",
      "Iteration 28, loss = 0.03080712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8461538461538461\n",
      "0.88\n",
      "0.8627450980392156\n",
      "Iteration 1, loss = 0.45757144\n",
      "Iteration 2, loss = 0.29453507\n",
      "Iteration 3, loss = 0.14222786\n",
      "Iteration 4, loss = 0.06469320\n",
      "Iteration 5, loss = 0.05032977\n",
      "Iteration 6, loss = 0.09453070\n",
      "Iteration 7, loss = 0.15161574\n",
      "Iteration 8, loss = 0.16957167\n",
      "Iteration 9, loss = 0.11677982\n",
      "Iteration 10, loss = 0.07532657\n",
      "Iteration 11, loss = 0.04771708\n",
      "Iteration 12, loss = 0.03762008\n",
      "Iteration 13, loss = 0.04806227\n",
      "Iteration 14, loss = 0.03751141\n",
      "Iteration 15, loss = 0.07805943\n",
      "Iteration 16, loss = 0.14001720\n",
      "Iteration 17, loss = 0.14009841\n",
      "Iteration 18, loss = 0.11173960\n",
      "Iteration 19, loss = 0.08371939\n",
      "Iteration 20, loss = 0.06444628\n",
      "Iteration 21, loss = 0.05485991\n",
      "Iteration 22, loss = 0.04381375\n",
      "Iteration 23, loss = 0.03138191\n",
      "Iteration 24, loss = 0.02290461\n",
      "Iteration 25, loss = 0.01634700\n",
      "Iteration 26, loss = 0.01323957\n",
      "Iteration 27, loss = 0.00964118\n",
      "Iteration 28, loss = 0.00797098\n",
      "Iteration 29, loss = 0.01449213\n",
      "Iteration 30, loss = 0.01550805\n",
      "Iteration 31, loss = 0.01139357\n",
      "Iteration 32, loss = 0.01301395\n",
      "Iteration 33, loss = 0.03765064\n",
      "Iteration 34, loss = 0.08230647\n",
      "Iteration 35, loss = 0.11732938\n",
      "Iteration 36, loss = 0.10326400\n",
      "Iteration 37, loss = 0.07908355\n",
      "Iteration 38, loss = 0.05631911\n",
      "Iteration 39, loss = 0.04039992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8333333333333334\n",
      "0.9\n",
      "0.8653846153846153\n",
      "Iteration 1, loss = 0.85351999\n",
      "Iteration 2, loss = 0.42130845\n",
      "Iteration 3, loss = 0.44907168\n",
      "Iteration 4, loss = 0.35578117\n",
      "Iteration 5, loss = 0.28983566\n",
      "Iteration 6, loss = 0.24876326\n",
      "Iteration 7, loss = 0.23344609\n",
      "Iteration 8, loss = 0.26059131\n",
      "Iteration 9, loss = 0.22813011\n",
      "Iteration 10, loss = 0.18555849\n",
      "Iteration 11, loss = 0.18154829\n",
      "Iteration 12, loss = 0.15203573\n",
      "Iteration 13, loss = 0.12825711\n",
      "Iteration 14, loss = 0.12524748\n",
      "Iteration 15, loss = 0.15196694\n",
      "Iteration 16, loss = 0.15815837\n",
      "Iteration 17, loss = 0.16935411\n",
      "Iteration 18, loss = 0.14123452\n",
      "Iteration 19, loss = 0.13762168\n",
      "Iteration 20, loss = 0.13773081\n",
      "Iteration 21, loss = 0.15900559\n",
      "Iteration 22, loss = 0.13741943\n",
      "Iteration 23, loss = 0.08047208\n",
      "Iteration 24, loss = 0.08090095\n",
      "Iteration 25, loss = 0.08599636\n",
      "Iteration 26, loss = 0.11358623\n",
      "Iteration 27, loss = 0.13251245\n",
      "Iteration 28, loss = 0.20293675\n",
      "Iteration 29, loss = 0.19933793\n",
      "Iteration 30, loss = 0.15025220\n",
      "Iteration 31, loss = 0.12103641\n",
      "Iteration 32, loss = 0.10853537\n",
      "Iteration 33, loss = 0.09163035\n",
      "Iteration 34, loss = 0.06264004\n",
      "Iteration 35, loss = 0.03589918\n",
      "Iteration 36, loss = 0.02971119\n",
      "Iteration 37, loss = 0.03265491\n",
      "Iteration 38, loss = 0.04208963\n",
      "Iteration 39, loss = 0.04896477\n",
      "Iteration 40, loss = 0.03812330\n",
      "Iteration 41, loss = 0.02694435\n",
      "Iteration 42, loss = 0.04491567\n",
      "Iteration 43, loss = 0.07861866\n",
      "Iteration 44, loss = 0.10495688\n",
      "Iteration 45, loss = 0.10298423\n",
      "Iteration 46, loss = 0.12915078\n",
      "Iteration 47, loss = 0.20398356\n",
      "Iteration 48, loss = 0.19367162\n",
      "Iteration 49, loss = 0.13441015\n",
      "Iteration 50, loss = 0.10159793\n",
      "Iteration 51, loss = 0.10035772\n",
      "Iteration 52, loss = 0.09791864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8490566037735849\n",
      "0.9\n",
      "0.8737864077669903\n",
      "Iteration 1, loss = 3.74943032\n",
      "Iteration 2, loss = 1.69588769\n",
      "Iteration 3, loss = 1.07595905\n",
      "Iteration 4, loss = 0.73480674\n",
      "Iteration 5, loss = 0.71353581\n",
      "Iteration 6, loss = 0.66403098\n",
      "Iteration 7, loss = 0.70711154\n",
      "Iteration 8, loss = 0.64980571\n",
      "Iteration 9, loss = 0.50474076\n",
      "Iteration 10, loss = 0.43974769\n",
      "Iteration 11, loss = 0.42925848\n",
      "Iteration 12, loss = 0.40265677\n",
      "Iteration 13, loss = 0.36366375\n",
      "Iteration 14, loss = 0.37389604\n",
      "Iteration 15, loss = 0.39006883\n",
      "Iteration 16, loss = 0.34456795\n",
      "Iteration 17, loss = 0.30411586\n",
      "Iteration 18, loss = 0.30939139\n",
      "Iteration 19, loss = 0.33094366\n",
      "Iteration 20, loss = 0.30486297\n",
      "Iteration 21, loss = 0.29692048\n",
      "Iteration 22, loss = 0.29544767\n",
      "Iteration 23, loss = 0.31115893\n",
      "Iteration 24, loss = 0.23911112\n",
      "Iteration 25, loss = 0.23233243\n",
      "Iteration 26, loss = 0.27328621\n",
      "Iteration 27, loss = 0.26118082\n",
      "Iteration 28, loss = 0.29184574\n",
      "Iteration 29, loss = 0.33007298\n",
      "Iteration 30, loss = 0.43417555\n",
      "Iteration 31, loss = 0.33198981\n",
      "Iteration 32, loss = 0.26882415\n",
      "Iteration 33, loss = 0.22994641\n",
      "Iteration 34, loss = 0.21827116\n",
      "Iteration 35, loss = 0.20812836\n",
      "Iteration 36, loss = 0.23682474\n",
      "Iteration 37, loss = 0.24379263\n",
      "Iteration 38, loss = 0.24879889\n",
      "Iteration 39, loss = 0.24989794\n",
      "Iteration 40, loss = 0.24940521\n",
      "Iteration 41, loss = 0.19946633\n",
      "Iteration 42, loss = 0.24334435\n",
      "Iteration 43, loss = 0.22286894\n",
      "Iteration 44, loss = 0.22290068\n",
      "Iteration 45, loss = 0.24969049\n",
      "Iteration 46, loss = 0.23381780\n",
      "Iteration 47, loss = 0.24046689\n",
      "Iteration 48, loss = 0.23406845\n",
      "Iteration 49, loss = 0.22091898\n",
      "Iteration 50, loss = 0.24093762\n",
      "Iteration 51, loss = 0.22379602\n",
      "Iteration 52, loss = 0.22996019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8518518518518519\n",
      "0.92\n",
      "0.8846153846153846\n",
      "Iteration 1, loss = 39.07313182\n",
      "Iteration 2, loss = 6.84989827\n",
      "Iteration 3, loss = 2.22354737\n",
      "Iteration 4, loss = 1.11664182\n",
      "Iteration 5, loss = 0.95374964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.90668201\n",
      "Iteration 7, loss = 0.95398497\n",
      "Iteration 8, loss = 1.07030547\n",
      "Iteration 9, loss = 0.94064259\n",
      "Iteration 10, loss = 0.83453796\n",
      "Iteration 11, loss = 0.74152148\n",
      "Iteration 12, loss = 0.75199616\n",
      "Iteration 13, loss = 0.77413046\n",
      "Iteration 14, loss = 0.74893266\n",
      "Iteration 15, loss = 0.72340662\n",
      "Iteration 16, loss = 0.80040964\n",
      "Iteration 17, loss = 0.79446924\n",
      "Iteration 18, loss = 0.71624115\n",
      "Iteration 19, loss = 0.72535628\n",
      "Iteration 20, loss = 0.71893356\n",
      "Iteration 21, loss = 0.70949800\n",
      "Iteration 22, loss = 0.71722435\n",
      "Iteration 23, loss = 0.74408627\n",
      "Iteration 24, loss = 0.70766326\n",
      "Iteration 25, loss = 0.73501941\n",
      "Iteration 26, loss = 0.72373391\n",
      "Iteration 27, loss = 0.69915894\n",
      "Iteration 28, loss = 0.70369103\n",
      "Iteration 29, loss = 0.76813527\n",
      "Iteration 30, loss = 0.76452777\n",
      "Iteration 31, loss = 0.70307174\n",
      "Iteration 32, loss = 0.67792465\n",
      "Iteration 33, loss = 0.67654284\n",
      "Iteration 34, loss = 0.68928619\n",
      "Iteration 35, loss = 0.68979888\n",
      "Iteration 36, loss = 0.71475177\n",
      "Iteration 37, loss = 0.69668381\n",
      "Iteration 38, loss = 0.69971800\n",
      "Iteration 39, loss = 0.73306869\n",
      "Iteration 40, loss = 0.70670620\n",
      "Iteration 41, loss = 0.67421363\n",
      "Iteration 42, loss = 0.72237758\n",
      "Iteration 43, loss = 0.71289235\n",
      "Iteration 44, loss = 0.70725984\n",
      "Iteration 45, loss = 0.69093466\n",
      "Iteration 46, loss = 0.68784789\n",
      "Iteration 47, loss = 0.67434104\n",
      "Iteration 48, loss = 0.67442946\n",
      "Iteration 49, loss = 0.66534479\n",
      "Iteration 50, loss = 0.64669662\n",
      "Iteration 51, loss = 0.64212715\n",
      "Iteration 52, loss = 0.65263744\n",
      "Iteration 53, loss = 0.65416201\n",
      "Iteration 54, loss = 0.68559511\n",
      "Iteration 55, loss = 0.66519329\n",
      "Iteration 56, loss = 0.64801753\n",
      "Iteration 57, loss = 0.66257233\n",
      "Iteration 58, loss = 0.70348700\n",
      "Iteration 59, loss = 0.68235447\n",
      "Iteration 60, loss = 0.65080455\n",
      "Iteration 61, loss = 0.63915178\n",
      "Iteration 62, loss = 0.64844945\n",
      "Iteration 63, loss = 0.63705369\n",
      "Iteration 64, loss = 0.63460296\n",
      "Iteration 65, loss = 0.63023428\n",
      "Iteration 66, loss = 0.64954736\n",
      "Iteration 67, loss = 0.64492244\n",
      "Iteration 68, loss = 0.64518249\n",
      "Iteration 69, loss = 0.65225362\n",
      "Iteration 70, loss = 0.63883524\n",
      "Iteration 71, loss = 0.62585786\n",
      "Iteration 72, loss = 0.65939120\n",
      "Iteration 73, loss = 0.65245757\n",
      "Iteration 74, loss = 0.67677819\n",
      "Iteration 75, loss = 0.67489373\n",
      "Iteration 76, loss = 0.66186316\n",
      "Iteration 77, loss = 0.63922143\n",
      "Iteration 78, loss = 0.64783085\n",
      "Iteration 79, loss = 0.63888747\n",
      "Iteration 80, loss = 0.63346119\n",
      "Iteration 81, loss = 0.62909778\n",
      "Iteration 82, loss = 0.62894551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8723404255319149\n",
      "0.82\n",
      "0.8453608247422681\n",
      "0.9019607843137255\n",
      "0.92\n",
      "0.9108910891089109\n",
      "MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=100, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=10, warm_start=False)\n",
      "Iteration 1, loss = 0.64914262\n",
      "Iteration 2, loss = 0.57880381\n",
      "Iteration 3, loss = 0.46835045\n",
      "Iteration 4, loss = 0.33324788\n",
      "Iteration 5, loss = 0.22961332\n",
      "Iteration 6, loss = 0.16901454\n",
      "Iteration 7, loss = 0.13203234\n",
      "Iteration 8, loss = 0.10648177\n",
      "Iteration 9, loss = 0.08807138\n",
      "Iteration 10, loss = 0.07446773\n",
      "Iteration 11, loss = 0.06310193\n",
      "Iteration 12, loss = 0.05513173\n",
      "Iteration 13, loss = 0.04780408\n",
      "Iteration 14, loss = 0.04162781\n",
      "Iteration 15, loss = 0.03684286\n",
      "Iteration 16, loss = 0.03299014\n",
      "Iteration 17, loss = 0.02964879\n",
      "Iteration 18, loss = 0.02694449\n",
      "Iteration 19, loss = 0.02428382\n",
      "Iteration 20, loss = 0.02207882\n",
      "Iteration 21, loss = 0.02026603\n",
      "Iteration 22, loss = 0.01851880\n",
      "Iteration 23, loss = 0.01715021\n",
      "Iteration 24, loss = 0.01608571\n",
      "Iteration 25, loss = 0.01480275\n",
      "Iteration 26, loss = 0.01381174\n",
      "Iteration 27, loss = 0.01295392\n",
      "Iteration 28, loss = 0.01218257\n",
      "Iteration 29, loss = 0.01148986\n",
      "Iteration 30, loss = 0.01085886\n",
      "Iteration 31, loss = 0.01026524\n",
      "Iteration 32, loss = 0.00975498\n",
      "Iteration 33, loss = 0.00922724\n",
      "Iteration 34, loss = 0.00884403\n",
      "Iteration 35, loss = 0.00840101\n",
      "Iteration 36, loss = 0.00802059\n",
      "Iteration 37, loss = 0.00766572\n",
      "Iteration 38, loss = 0.00736244\n",
      "Iteration 39, loss = 0.00706961\n",
      "Iteration 40, loss = 0.00680984\n",
      "Iteration 41, loss = 0.00654807\n",
      "Iteration 42, loss = 0.00630701\n",
      "Iteration 43, loss = 0.00607202\n",
      "Iteration 44, loss = 0.00585911\n",
      "Iteration 45, loss = 0.00565220\n",
      "Iteration 46, loss = 0.00547558\n",
      "Iteration 47, loss = 0.00530469\n",
      "Iteration 48, loss = 0.00513113\n",
      "Iteration 49, loss = 0.00497256\n",
      "Iteration 50, loss = 0.00482065\n",
      "Iteration 51, loss = 0.00468976\n",
      "Iteration 52, loss = 0.00455519\n",
      "Iteration 53, loss = 0.00443678\n",
      "Iteration 54, loss = 0.00430656\n",
      "Iteration 55, loss = 0.00419815\n",
      "Iteration 56, loss = 0.00408553\n",
      "Iteration 57, loss = 0.00398304\n",
      "Iteration 58, loss = 0.00387941\n",
      "Iteration 59, loss = 0.00378940\n",
      "Iteration 60, loss = 0.00370093\n",
      "Iteration 61, loss = 0.00361007\n",
      "Iteration 62, loss = 0.00353621\n",
      "Iteration 63, loss = 0.00345337\n",
      "Iteration 64, loss = 0.00337412\n",
      "Iteration 65, loss = 0.00331496\n",
      "Iteration 66, loss = 0.00323956\n",
      "Iteration 67, loss = 0.00316662\n",
      "Iteration 68, loss = 0.00310619\n",
      "Iteration 69, loss = 0.00303811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "pred_mlp_tfidf = tune_mlp(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_output(pred_mlp_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred_mlp_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2046467,
     "status": "error",
     "timestamp": 1588839484124,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "h6TZaYcpF0QD",
    "outputId": "1eeb7c4f-348c-4406-a43b-4b1dbe66c269"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-62fb26b6e97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_tfidf_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tfidfvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_tfidfvector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tfidfvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#to be submitted runned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c9e0cb62ddc6>\u001b[0m in \u001b[0;36mtune_xgb\u001b[0;34m(train_feature, train_label, dev_feature, dev_label, test_feature)\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearn_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimatores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_estimate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearningrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_tfidf_xgb = tune_xgb(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector) #to be submitted runned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3253,
     "status": "error",
     "timestamp": 1588835089787,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "afdZcMmcaocX",
    "outputId": "fb59d843-617f-4848-cad9-4a935ee14675"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (4, 4), input_shape=(1, 256, 1...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1653\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for '{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_1_input, conv2d_1/convolution/ReadVariableOp)' with input shapes: [?,1,256,128], [4,4,128,16].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-82e358134a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model.add(Convolution2D(n_filter,filter_length,filter_length,\n\u001b[0;32m---> 15\u001b[0;31m                         input_shape=(1, 256, 128)))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3717\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   3718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3719\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NHWC -> NCHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution)\u001b[0m\n\u001b[1;32m    982\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    985\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    967\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m    970\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3325\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3328\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 1817\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   1818\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for '{{node conv2d_1/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_1_input, conv2d_1/convolution/ReadVariableOp)' with input shapes: [?,1,256,128], [4,4,128,16]."
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 32\n",
    "n_filter = 16\n",
    "filter_length = 4\n",
    "nb_epoch = 5\n",
    "n_pool = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(n_filter,filter_length,filter_length,\n",
    "                        input_shape=(1, 256, 128)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(n_filter,filter_length,filter_length))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_tfidfvector, train_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=0)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cQX6ytkEkTOs"
   },
   "outputs": [],
   "source": [
    "######################################### LDA ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 818,
     "status": "error",
     "timestamp": 1588954201545,
     "user": {
      "displayName": "Yi LI",
      "photoUrl": "",
      "userId": "10608047051067755049"
     },
     "user_tz": -600
    },
    "id": "rkyYE2wokYZd",
    "outputId": "54e70995-c6c5-402d-8ae5-ee96c7f73f39"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2a1917c9ed46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_topic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_train_events\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_topics'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topic = 1\n",
    "lda = LatentDirichletAllocation(n_topics=n_topic,max_iter=2000,learning_method='batch')\n",
    "one = tfidfvector.fit_transform(preprocessed_train_events[0])\n",
    "lda.fit(one)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOHdLbEzfQhsh+ug+2b/Qxw",
   "collapsed_sections": [],
   "name": "BinaryClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
