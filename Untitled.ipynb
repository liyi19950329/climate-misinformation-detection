{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'articles1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d91488182029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0marticle_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"articles1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mcorinfo_train_events\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorinfo_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tweet_text_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_negs_merge.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d91488182029>\u001b[0m in \u001b[0;36mget_csv\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcsvFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'articles1.csv'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pdb\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def get_tweet_text_from_json(file_path):\n",
    "    events = []\n",
    "    labels = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "            labels.append(value[\"label\"])\n",
    "        return events,labels\n",
    "    \n",
    "def get_tweet_text_from_json_unlabel(file_path):\n",
    "    events = []\n",
    "    with open(file_path) as json_file:\n",
    "#         pdb.set_trace()\n",
    "        data = json.load(json_file)\n",
    "        for key, value in data.items():\n",
    "            events.append(value[\"text\"])\n",
    "        return events\n",
    "    \n",
    "def get_csv(file_path):\n",
    "\n",
    "    csvFile = open(file_path, \"r\")\n",
    "    reader = csv.reader(csvFile)\n",
    "    \n",
    "    result = []\n",
    "    i = 500\n",
    "    for item in reader:\n",
    "        if i == 0:\n",
    "            break\n",
    "        if reader.line_num == 1:\n",
    "            continue\n",
    "#         pdb.set_trace()\n",
    "        result.append(item[9])\n",
    "        i -= 1\n",
    "    csvFile.close()\n",
    "    return result\n",
    "\n",
    "article_events = get_csv(\"articles1.csv\")\n",
    "\n",
    "corinfo_train_events,corinfo_train_labels = get_tweet_text_from_json(\"train_negs_merge.json\")\n",
    "misinfo_train_events,misinfo_train_labels = get_tweet_text_from_json(\"train.json\")\n",
    "dev_events,dev_labels = get_tweet_text_from_json(\"dev.json\")\n",
    "test_unlabel = get_tweet_text_from_json_unlabel(\"test-unlabelled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corinfo_train_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punc = punctuation + u'.,;《》？！“”‘’@#￥%…&×（）——+【】{};；●，。&～、|\\s:：'\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if (treebank_tag.startswith('J'))|(treebank_tag =='ADJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif (treebank_tag.startswith('R'))|(treebank_tag=='ADV'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def preprocess_drop_punc_evnets(events):\n",
    "#     preprocess_drop_punc_evnets = events\n",
    "    preprocess_drop_punc_evnets = []\n",
    "#     pdb.set_trace()\n",
    "    for event in events:\n",
    "        event_re = re.sub(r\"[{}]+\".format(punc),\" \",event)\n",
    "        preprocess_drop_punc_evnets.append(event_re)\n",
    "    return preprocess_drop_punc_evnets\n",
    "\n",
    "def preprocess_stopwords_lemma_evnets(events):\n",
    "    preprocess_BOW_events = []\n",
    "    preprocess_events = []\n",
    "    preprocess_events_tokens = []\n",
    "#     pdb.set_trace()\n",
    "    after_drop_punc_evnets = preprocess_drop_punc_evnets(events)\n",
    "    for event in after_drop_punc_evnets:\n",
    "        dic_bow = defaultdict(int)\n",
    "        tokenize_words = tt.tokenize(event)\n",
    "#         rep = [abbr_dict[x] if x in abbr_dict else x for x in tokenize_words]\n",
    "        words_pos = nltk.pos_tag(tokenize_words, tagset=\"universal\")\n",
    "#         pdb.set_trace()\n",
    "        filter_event = [(w,pos) for (w,pos) in words_pos if w.lower() not in stopwords]\n",
    "        lemma_event = []\n",
    "        for (word,pos) in filter_event:\n",
    "#             pdb.set_trace()\n",
    "            word_lemma = wnl.lemmatize(word.lower(),get_wordnet_pos(pos))\n",
    "            lemma_event.append(word_lemma)\n",
    "            dic_bow[word_lemma] += 1\n",
    "        preprocess_events_tokens.append(lemma_event,)\n",
    "        preprocess_events.append(' '.join(lemma_event))\n",
    "        preprocess_BOW_events.append(dic_bow)\n",
    "    return preprocess_events_tokens,preprocess_events,preprocess_BOW_events\n",
    "preprocess_neg_events_tokens,preprocessed_neg_events,preprocessed_neg_BOW_events = preprocess_stopwords_lemma_evnets(article_events)\n",
    "preprocess_train_events_tokens,preprocessed_train_events,preprocessed_train_BOW_events = preprocess_stopwords_lemma_evnets(misinfo_train_events)\n",
    "preprocess_dev_events_tokens,preprocessed_dev_events,preprocessed_dev_BOW_events = preprocess_stopwords_lemma_evnets(dev_events)\n",
    "preprocess_test_events_tokens,preprocessed_test,preprocessed_test_BOW = preprocess_stopwords_lemma_evnets(test_unlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = [i for i in range(len(dev_labels)) if dev_labels[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 8, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 27, 28, 30, 33, 34, 36, 41, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 67, 68, 69, 70, 71, 74, 78, 80, 83, 85, 87, 92, 93, 95, 96, 98]\n"
     ]
    }
   ],
   "source": [
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "train_labels = misinfo_train_labels + [0 for i in range(len(preprocessed_neg_events))]\n",
    "tfidfvector = TfidfVectorizer(min_df=3,  \n",
    "                              max_df=0.5,\n",
    "                              max_features=None,                 \n",
    "                              ngram_range=(1, 2), \n",
    "                              use_idf=True,\n",
    "                              smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44494\n"
     ]
    }
   ],
   "source": [
    "train_tfidfvector = tfidfvector.fit_transform(preprocessed_train_events + preprocessed_neg_events)\n",
    "dev_tfidfvector = tfidfvector.transform(preprocessed_dev_events)\n",
    "test_tfidfvector = tfidfvector.transform(preprocessed_test)\n",
    "print(len(tfidfvector.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def tune_log(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "    solvers = [\"liblinear\",\"lbfgs\",\"newton-cg\",\"sag\"]\n",
    "    penalties = [\"l1\",\"l2\"]\n",
    "    cs = [0.001,0.01,0.1,0.5,0.53,1,100, 1000]\n",
    "    f_score = []\n",
    "    parameter_save = []\n",
    "    for solver in solvers:\n",
    "        for penalty in penalties:\n",
    "            for c in cs:\n",
    "                if ((solver == \"lbfgs\")|(solver == \"newton-cg\")|(solver == \"sag\")) & (penalty == \"l1\"):\n",
    "                    continue\n",
    "                clf = LogisticRegression(C=c, penalty = penalty, solver = solver)\n",
    "                clf.fit(train_feature,train_label)\n",
    "                preds = clf.predict(dev_feature)\n",
    "                _,_,f,_ = evaluate_result(dev_label, preds)\n",
    "                f_score.append(f)\n",
    "                parameter_save.append((c,penalty,solver))\n",
    "    i = f_score.index(max(f_score))\n",
    "    (c,penalty,solver) = parameter_save[i]\n",
    "    clf = LogisticRegression(C=c, penalty = penalty, solver = solver)\n",
    "    clf.fit(train_feature,train_label)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def tune_nb(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "    alphas = [0.001,0.01,0.1,0.5,0.53,0.55,0.6,1,1.2,1.3,2,10,50,100]\n",
    "    f_score = []\n",
    "    parameter_save = []\n",
    "    for alpha in alphas:\n",
    "        clf = MultinomialNB(alpha = alpha)\n",
    "        clf.fit(train_feature,train_label)\n",
    "        preds = clf.predict(dev_feature)\n",
    "        _,_,f,_ = evaluate_result(dev_label, preds)\n",
    "        f_score.append(f)\n",
    "        parameter_save.append(alpha)\n",
    "    i = f_score.index(max(f_score))\n",
    "    alpha = parameter_save[i]\n",
    "    clf = MultinomialNB(alpha = alpha)\n",
    "    clf.fit(train_feature,train_label)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "preds = tune_log(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_nb = tune_nb(train_tfidfvector,train_labels,dev_tfidfvector,dev_labels,test_tfidfvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_output(preds_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_result(groundtruth, prediction):\n",
    "    prediction = prediction.tolist()\n",
    "    for i,item in enumerate(prediction):\n",
    "        if item == -1:\n",
    "            prediction[i] = 0\n",
    "    p, r, f, _ = precision_recall_fscore_support(groundtruth, prediction, pos_label=1, average=\"binary\")\n",
    "#     pdb.set_trace()\n",
    "    # print(\"Performance on the positive class (documents with misinformation):\")\n",
    "    # print(\"Precision =\", p)\n",
    "    # print(\"Recall    =\", r)\n",
    "    # print(\"F1        =\", f)\n",
    "    # print(accuracy_score(groundtruth,prediction))\n",
    "    return p,r,f,accuracy_score(groundtruth,prediction)\n",
    "    \n",
    "def transfer_output(preds):\n",
    "    test_dic = {}\n",
    "    for i,test_pre in enumerate(preds):\n",
    "\n",
    "        if test_pre == -1:\n",
    "            dic = {}\n",
    "            dic['label'] = 0\n",
    "    #         pdb.set_trace()\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "        elif test_pre == 0:\n",
    "            dic = {}\n",
    "    #         pdb.set_trace()\n",
    "            dic['label'] = 0\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "        else:\n",
    "            dic = {}\n",
    "    #         pdb.set_trace()\n",
    "            dic['label'] = 1\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "    jstr = json.dumps(test_dic,ensure_ascii = False)\n",
    "    # print(jstr)\n",
    "    with open('test-output.json','w') as f:\n",
    "        f.write(jstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def tune_log(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "    solvers = [\"liblinear\",\"lbfgs\",\"newton-cg\",\"sag\"]\n",
    "    penalties = [\"l1\",\"l2\"]\n",
    "    cs = [0.001,0.01,0.1,0.5,0.53,1,100, 1000]\n",
    "    f_score = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    parameter_save = []\n",
    "    for solver in solvers:\n",
    "        for penalty in penalties:\n",
    "            for c in cs:\n",
    "                if ((solver == \"lbfgs\")|(solver == \"newton-cg\")|(solver == \"sag\")) & (penalty == \"l1\"):\n",
    "                    continue\n",
    "                clf = LogisticRegression(C=c, penalty = penalty, solver = solver)\n",
    "                clf.fit(train_feature,train_label)\n",
    "                preds = clf.predict(dev_feature)\n",
    "                p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "                f_score.append(f)\n",
    "                precision.append(p)\n",
    "                recall.append(r)\n",
    "                parameter_save.append((c,penalty,solver))\n",
    "    i = f_score.index(max(f_score))\n",
    "    print(precision[i])\n",
    "    print(recall[i])\n",
    "    print(f_score[i])\n",
    "    (c,penalty,solver) = parameter_save[i]\n",
    "    clf = LogisticRegression(C=c, penalty = penalty, solver = solver)\n",
    "    clf.fit(train_feature,train_label)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def tune_nb(train_feature,train_label,dev_feature,dev_label,test_feature):\n",
    "    alphas = [0.001,0.01,0.1,0.5,0.53,0.55,0.6,1,1.2,1.3,2,10,50,100]\n",
    "    f_score = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    parameter_save = []\n",
    "    for alpha in alphas:\n",
    "        clf = MultinomialNB(alpha = alpha)\n",
    "        clf.fit(train_feature,train_label)\n",
    "        preds = clf.predict(dev_feature)\n",
    "        p,r,f,_ = evaluate_result(dev_label, preds)\n",
    "        f_score.append(f)\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        parameter_save.append(alpha)\n",
    "    i = f_score.index(max(f_score))\n",
    "    print(precision[i])\n",
    "    print(recall[i])\n",
    "    print(f_score[i])\n",
    "    alpha = parameter_save[i]\n",
    "    clf = MultinomialNB(alpha = alpha)\n",
    "    clf.fit(train_feature,train_label)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "def tune_classifer_lof(train_feature, dev_feature, dev_label,test_feature):\n",
    "    n_neighborss = [10,20,30,40,100]\n",
    "    contaminations = [0.1,0.3,0.5]\n",
    "    f_score = []\n",
    "    parameter_save = []\n",
    "    for n_neighbor in n_neighborss:\n",
    "        for contamination in contaminations:\n",
    "            clf = LocalOutlierFactor(n_neighbors = n_neighbor, novelty=True, contamination=contamination)\n",
    "            clf.fit(train_feature)\n",
    "            y_pre = clf.predict(dev_feature)\n",
    "            # print(clf)\n",
    "            p,r,f,a = evaluate_result(dev_label,y_pre)\n",
    "            f_score.append(f)\n",
    "            parameter_save.append((n_neighbor,contamination))\n",
    "    i = f_score.index(max(f_score))\n",
    "    (n_neighbor,contamination) = parameter_save[i]\n",
    "    clf = LocalOutlierFactor(n_neighbors = n_neighbor, novelty=True, contamination=contamination)\n",
    "    clf.fit(train_feature)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def tune_classifer_ocs(train_feature, dev_feature, dev_label,test_feature):\n",
    "    nus = [0.001, 0.01, 0.1, 1]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    kernels = ['rbf','linear']\n",
    "    f_score = []\n",
    "    parameter_save = []\n",
    "    for kernel in kernels:\n",
    "        for nu in nus:\n",
    "            for gamma in gammas:\n",
    "                clf = svm.OneClassSVM(gamma=gamma,kernel = kernel, nu = nu)\n",
    "                clf.fit(train_feature)\n",
    "                y_pre = clf.predict(dev_feature)\n",
    "                # print(clf)\n",
    "                p,r,f,a = evaluate_result(dev_label,y_pre)\n",
    "                f_score.append(f)\n",
    "                parameter_save.append((kernel,nu,gamma))\n",
    "    i = f_score.index(max(f_score))\n",
    "    (kernel,nu,gamma) = parameter_save[i]\n",
    "    clf = svm.OneClassSVM(gamma=gamma,kernel = kernel, nu = nu)\n",
    "    clf.fit(train_feature)\n",
    "    test_pres = clf.predict(test_feature)\n",
    "    return test_pres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a1fee787bf22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrian_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_train_events\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpreprocessed_neg_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdev_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_dev_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36marg_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCVTIMEO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 t_e = TimeoutError(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, texts, blocking, is_tokenized, show_tokens)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_info_available\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshow_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36m_recv_ndarray\u001b[0;34m(self, wait_for_req_id)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_req_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_for_req_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0marr_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsonapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/bert_serving/client/__init__.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, wait_for_req_id)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;31m# receive a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceiver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0many\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCVMORE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trian_set = bc.encode(preprocessed_train_events + preprocessed_neg_events)\n",
    "dev_set = bc.encode(preprocessed_dev_events)\n",
    "test_set = bc.encode(preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds_ocm = tune_classifer_ocs(trian_set,dev_set,dev_labels, test_set)\n",
    "preds_lof = tune_classifer_lof(trian_set,dev_set,dev_labels, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n",
      "-122\n"
     ]
    }
   ],
   "source": [
    "print(sum(preds_ocm))\n",
    "print(sum(preds_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5052\n",
    "transfer_output(preds_lof)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d7777d857cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfidfvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_tfidfvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_train_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdev_tfidfvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_dev_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_tfidfvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \"string object received.\")\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m\"\"\"Check validity of ngram_range parameter\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mmin_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_m\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidfvector = TfidfVectorizer(ngram_range = 3)\n",
    "train_tfidfvector = tfidfvector.fit_transform(preprocessed_train_events)\n",
    "dev_tfidfvector = tfidfvector.transform(preprocessed_dev_events)\n",
    "test_tfidfvector = tfidfvector.transform(preprocessed_test)\n",
    "print(len(tfidfvector.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidfvector = TfidfVectorizer()\n",
    "train_tfidfvector = tfidfvector.fit_transform(preprocessed_train_events)\n",
    "dev_tfidfvector = tfidfvector.transform(preprocessed_dev_events)\n",
    "test_tfidfvector = tfidfvector.transform(preprocessed_test)\n",
    "print(len(tfidfvector.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_result(groundtruth, prediction):\n",
    "    prediction = prediction.tolist()\n",
    "    for i,item in enumerate(prediction):\n",
    "        if item == -1:\n",
    "            prediction[i] = 0\n",
    "    p, r, f, _ = precision_recall_fscore_support(groundtruth, prediction, pos_label=1, average=\"binary\")\n",
    "#     pdb.set_trace()\n",
    "    print(\"Performance on the positive class (documents with misinformation):\")\n",
    "    print(\"Precision =\", p)\n",
    "    print(\"Recall    =\", r)\n",
    "    print(\"F1        =\", f)\n",
    "    print(accuracy_score(groundtruth,prediction))\n",
    "    \n",
    "def transfer_output(preds):\n",
    "    test_dic = {}\n",
    "    for i,test_pre in enumerate(preds):\n",
    "        if test_pre == -1:\n",
    "            dic = {}\n",
    "            dic['label'] = 0\n",
    "    #         pdb.set_trace()\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "        else:\n",
    "            dic = {}\n",
    "    #         pdb.set_trace()\n",
    "            dic['label'] = 1\n",
    "            test_dic['test-%s'%(i)] = dic\n",
    "    jstr = json.dumps(test_dic,ensure_ascii = False)\n",
    "    # print(jstr)\n",
    "    with open('test-output.json','w') as f:\n",
    "        f.write(jstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.8\n",
      "Recall    = 0.08\n",
      "F1        = 0.14545454545454545\n",
      "0.53\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6103896103896104\n",
      "Recall    = 0.94\n",
      "F1        = 0.7401574803149605\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6176470588235294\n",
      "Recall    = 0.84\n",
      "F1        = 0.711864406779661\n",
      "0.66\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 1.0\n",
      "Recall    = 0.08\n",
      "F1        = 0.14814814814814814\n",
      "0.54\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6025641025641025\n",
      "Recall    = 0.94\n",
      "F1        = 0.7343749999999999\n",
      "0.66\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6025641025641025\n",
      "Recall    = 0.94\n",
      "F1        = 0.7343749999999999\n",
      "0.66\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6231884057971014\n",
      "Recall    = 0.86\n",
      "F1        = 0.7226890756302521\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6025641025641025\n",
      "Recall    = 0.94\n",
      "F1        = 0.7343749999999999\n",
      "0.66\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6025641025641025\n",
      "Recall    = 0.94\n",
      "F1        = 0.7343749999999999\n",
      "0.66\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6231884057971014\n",
      "Recall    = 0.86\n",
      "F1        = 0.7226890756302521\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6075949367088608\n",
      "Recall    = 0.96\n",
      "F1        = 0.7441860465116279\n",
      "0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.covariance import EllipticEnvelope\n",
    "nus = [0.001, 0.01, 0.1, 1]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "kernels = ['rbf','linear']\n",
    "for kernel in kernels:\n",
    "    for nu in nus:\n",
    "        for gamma in gammas:\n",
    "            clf = svm.OneClassSVM(gamma=gamma,kernel = kernel, nu = nu).fit(train_tfidfvector)\n",
    "            y_pre = clf.predict(dev_tfidfvector)\n",
    "            print(clf)\n",
    "            evaluate_result(dev_labels,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "ocs = svm.OneClassSVM(gamma=0.01,kernel = 'rbf', nu = 0.1).fit(train_tfidfvector)\n",
    "test_pres = ocs.predict(test_tfidfvector)\n",
    "\n",
    "print(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-250\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_output(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1  1 ...  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "ocs = svm.OneClassSVM(gamma=0.001,kernel = 'linear', nu = 0.001).fit(train_tfidfvector)\n",
    "test_pres = ocs.predict(test_tfidfvector)\n",
    "\n",
    "print(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-244\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_output(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5921052631578947\n",
      "Recall    = 0.9\n",
      "F1        = 0.7142857142857143\n",
      "0.64\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.7755102040816326\n",
      "Recall    = 0.76\n",
      "F1        = 0.7676767676767676\n",
      "0.77\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.75\n",
      "Recall    = 0.48\n",
      "F1        = 0.5853658536585366\n",
      "0.66\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.625\n",
      "Recall    = 0.9\n",
      "F1        = 0.7377049180327869\n",
      "0.68\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.68\n",
      "Recall    = 0.68\n",
      "F1        = 0.68\n",
      "0.68\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.75\n",
      "Recall    = 0.48\n",
      "F1        = 0.5853658536585366\n",
      "0.66\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.625\n",
      "Recall    = 0.9\n",
      "F1        = 0.7377049180327869\n",
      "0.68\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6470588235294118\n",
      "Recall    = 0.66\n",
      "F1        = 0.6534653465346535\n",
      "0.65\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.7428571428571429\n",
      "Recall    = 0.52\n",
      "F1        = 0.6117647058823529\n",
      "0.67\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5945945945945946\n",
      "Recall    = 0.88\n",
      "F1        = 0.7096774193548386\n",
      "0.64\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6037735849056604\n",
      "Recall    = 0.64\n",
      "F1        = 0.6213592233009708\n",
      "0.61\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6923076923076923\n",
      "Recall    = 0.54\n",
      "F1        = 0.6067415730337079\n",
      "0.65\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6103896103896104\n",
      "Recall    = 0.94\n",
      "F1        = 0.7401574803149605\n",
      "0.67\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.609375\n",
      "Recall    = 0.78\n",
      "F1        = 0.6842105263157895\n",
      "0.64\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6326530612244898\n",
      "Recall    = 0.62\n",
      "F1        = 0.6262626262626263\n",
      "0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "n_neighborss = [10,20,30,40,100]\n",
    "contaminations = [0.1,0.3,0.5]\n",
    "for n_neighbor in n_neighborss:\n",
    "    for contamination in contaminations:\n",
    "        clf = LocalOutlierFactor(n_neighbors = n_neighbor, novelty=True, contamination=contamination)\n",
    "        clf.fit(train_tfidfvector)\n",
    "        y_pre = clf.predict(dev_tfidfvector)\n",
    "        print(clf)\n",
    "        evaluate_result(dev_labels,y_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ... -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors = 10, novelty=True, contamination=0.3)\n",
    "clf.fit(train_tfidfvector)\n",
    "test_pres = clf.predict(test_tfidfvector)\n",
    "print(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-948\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_word_average = word_averaging_list(wv,preprocess_train_events_tokens)\n",
    "dev_word_average = word_averaging_list(wv,preprocess_dev_events_tokens)\n",
    "test_word_average = word_averaging_list(wv,preprocess_test_events_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5051546391752577\n",
      "Recall    = 0.98\n",
      "F1        = 0.6666666666666666\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5157894736842106\n",
      "Recall    = 0.98\n",
      "F1        = 0.6758620689655174\n",
      "0.53\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5050505050505051\n",
      "Recall    = 1.0\n",
      "F1        = 0.6711409395973155\n",
      "0.51\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5157894736842106\n",
      "Recall    = 0.98\n",
      "F1        = 0.6758620689655174\n",
      "0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='rbf',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.001, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.01, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5\n",
      "Recall    = 0.98\n",
      "F1        = 0.6621621621621622\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=0.1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5903614457831325\n",
      "Recall    = 0.98\n",
      "F1        = 0.7368421052631579\n",
      "0.65\n",
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.001, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.01, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=1, kernel='linear',\n",
      "            max_iter=-1, nu=1, shrinking=True, tol=0.001, verbose=False)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.0\n",
      "Recall    = 0.0\n",
      "F1        = 0.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nus = [0.001, 0.01, 0.1, 1]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "kernels = ['rbf','linear']\n",
    "for kernel in kernels:\n",
    "    for nu in nus:\n",
    "        for gamma in gammas:\n",
    "            clf = svm.OneClassSVM(gamma=gamma,kernel = kernel, nu = nu).fit(train_word_average)\n",
    "            y_pre = clf.predict(dev_word_average)\n",
    "            print(clf)\n",
    "            evaluate_result(dev_labels,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "ocs = svm.OneClassSVM(gamma=0.001,kernel = 'rbf', nu = 0.1).fit(train_word_average)\n",
    "test_pres = ocs.predict(test_word_average)\n",
    "\n",
    "print(test_pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_pres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5208333333333334\n",
      "Recall    = 1.0\n",
      "F1        = 0.684931506849315\n",
      "0.54\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5897435897435898\n",
      "Recall    = 0.92\n",
      "F1        = 0.71875\n",
      "0.64\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=10, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6461538461538462\n",
      "Recall    = 0.84\n",
      "F1        = 0.7304347826086957\n",
      "0.69\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5154639175257731\n",
      "Recall    = 1.0\n",
      "F1        = 0.6802721088435374\n",
      "0.53\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5822784810126582\n",
      "Recall    = 0.92\n",
      "F1        = 0.7131782945736436\n",
      "0.63\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=20, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6349206349206349\n",
      "Recall    = 0.8\n",
      "F1        = 0.7079646017699115\n",
      "0.67\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5208333333333334\n",
      "Recall    = 1.0\n",
      "F1        = 0.684931506849315\n",
      "0.54\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.575\n",
      "Recall    = 0.92\n",
      "F1        = 0.7076923076923077\n",
      "0.62\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=30, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6461538461538462\n",
      "Recall    = 0.84\n",
      "F1        = 0.7304347826086957\n",
      "0.69\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5263157894736842\n",
      "Recall    = 1.0\n",
      "F1        = 0.6896551724137931\n",
      "0.55\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.575\n",
      "Recall    = 0.92\n",
      "F1        = 0.7076923076923077\n",
      "0.62\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=40, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6307692307692307\n",
      "Recall    = 0.82\n",
      "F1        = 0.7130434782608696\n",
      "0.67\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.1, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.5568181818181818\n",
      "Recall    = 0.98\n",
      "F1        = 0.7101449275362319\n",
      "0.6\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.3, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.618421052631579\n",
      "Recall    = 0.94\n",
      "F1        = 0.746031746031746\n",
      "0.68\n",
      "LocalOutlierFactor(algorithm='auto', contamination=0.5, leaf_size=30,\n",
      "                   metric='minkowski', metric_params=None, n_jobs=None,\n",
      "                   n_neighbors=100, novelty=True, p=2)\n",
      "Performance on the positive class (documents with misinformation):\n",
      "Precision = 0.6212121212121212\n",
      "Recall    = 0.82\n",
      "F1        = 0.706896551724138\n",
      "0.66\n"
     ]
    }
   ],
   "source": [
    "n_neighborss = [10,20,30,40,100]\n",
    "contaminations = [0.1,0.3,0.5]\n",
    "for n_neighbor in n_neighborss:\n",
    "    for contamination in contaminations:\n",
    "        clf = LocalOutlierFactor(n_neighbors = n_neighbor, novelty=True, contamination=contamination)\n",
    "        clf.fit(train_word_average)\n",
    "        y_pre = clf.predict(dev_word_average)\n",
    "        print(clf)\n",
    "        evaluate_result(dev_labels,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "all_texts = preprocess_train_events_tokens + preprocess_dev_events_tokens + preprocess_test_events_tokens\n",
    "word_model_skip_gram = Word2Vec(all_texts, min_count=1, size=60, window=8, iter=50, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "        self.word_model = word_model\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self):  # comply with scikit-learn transformer requirement\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "\n",
    "\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "            mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "            array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "\n",
    "    def __init__(self, word_model):\n",
    "\n",
    "        self.word_model = word_model\n",
    "        self.word_idf_weight = None\n",
    "        self.vector_size = word_model.wv.vector_size\n",
    "\n",
    "    def fit(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        \"\"\"\n",
    "        Fit in a list of docs, which had been preprocessed and tokenized,\n",
    "        such as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
    "        Then build up a tfidf model to compute each word's idf as its weight.\n",
    "        Noted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
    "        :param\n",
    "            pre_processed_docs: list of docs, which are tokenized\n",
    "        :return:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        text_docs = []\n",
    "        for doc in docs:\n",
    "            text_docs.append(\" \".join(doc))\n",
    "\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf.fit(text_docs)  # must be list of text string\n",
    "\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
    "        self.word_idf_weight = defaultdict(lambda: max_idf,[(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "        doc_word_vector = self.word_average_list(docs)\n",
    "        return doc_word_vector\n",
    "\n",
    "\n",
    "    def word_average(self, sent):\n",
    "        \"\"\"\n",
    "        Compute average word vector for a single doc/sentence.\n",
    "        :param sent: list of sentence tokens\n",
    "        :return:\n",
    "            mean: float of averaging word vectors\n",
    "        \"\"\"\n",
    "\n",
    "        mean = []\n",
    "        for word in sent:\n",
    "            if word in self.word_model.wv.vocab:\n",
    "                mean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
    "\n",
    "        if not mean:  # empty words\n",
    "            # If a text is empty, return a vector of zeros.\n",
    "            logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "            return np.zeros(self.vector_size)\n",
    "        else:\n",
    "            mean = np.array(mean).mean(axis=0)\n",
    "            return mean\n",
    "\n",
    "\n",
    "    def word_average_list(self, docs):\n",
    "        \"\"\"\n",
    "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
    "        :param docs: list of sentence in list of separated tokens\n",
    "        :return:\n",
    "            array of average word vector in shape (len(docs),)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.word_average(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
    "train_vec = mean_vec_tr.transform(train_tokenized)\n",
    "dev_vec = mean_vec_tr.transform(dev_tokenized)\n",
    "test_vec = mean_vec_tr.transform(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pdb\n",
    "from pyquery import PyQuery as pq\n",
    "header={'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\"}\n",
    "def get_data_path(href):\n",
    "    ret=urllib.request.Request(url=href,headers=header)\n",
    "    res=urllib.request.urlopen(ret)\n",
    "    response=BeautifulSoup(res,'html.parser')\n",
    "    title = response.find_all(lambda tag: tag.name == 'h1')\n",
    "    datas=response.find_all(lambda tag: tag.name == 'p')\n",
    "#     pdb.set_trace()\n",
    "    del datas[0]\n",
    "    del datas[0]\n",
    "    del datas[0]\n",
    "    del datas[0]\n",
    "    data = title + datas\n",
    "    dr = re.compile(r'<[^>]+>',re.S)\n",
    "    data_clean = dr.sub('',str(data))\n",
    "    return data_clean.replace('[','').replace(']','') + href\n",
    "\n",
    "# desktop user-agent\n",
    "train_negs = {}\n",
    "i = 0\n",
    "try:\n",
    "    \n",
    "    url=\"https://www.bbc.com/sport\"\n",
    "    ret=urllib.request.Request(url=url,headers=header)\n",
    "    res=urllib.request.urlopen(ret)\n",
    "    response=BeautifulSoup(res,'html.parser')\n",
    "    css_class = response.find(attrs={'class':'gel-layout__item gel-2/3@l gel-3/4@xxl'})\n",
    "    css_class = css_class.find_all(lambda tag: tag.name == 'a')\n",
    "    hrefs = set([tag['href'] for tag in css_class])\n",
    "    hrefs_texts = [href for href in hrefs if href.split('/')[-1].isdigit()]\n",
    "    hrefs_urls = [href for href in hrefs if href not in hrefs_texts]\n",
    "    hrefs_texts = map(lambda tag: \"https://www.bbc.com\" + tag if 'https' not in tag else tag, hrefs_texts)\n",
    "    \n",
    "    for href in hrefs_texts:\n",
    "        content = get_data_path(href)\n",
    "        dic_train = {}\n",
    "        dic_train['text'] = content\n",
    "        dic_train['label'] = 0\n",
    "        train_negs['train-%s'%i] = dic_train\n",
    "        i += 1\n",
    "    pdb.set_trace()\n",
    "except Exception as inst:\n",
    "    print(inst)\n",
    "jstr = json.dumps(train_negs,ensure_ascii = False)\n",
    "with open('train_negs_bbc_sport.json','w') as f:\n",
    "    f.write(jstr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
